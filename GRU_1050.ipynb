{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score , precision_score , recall_score , accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# data = pd.read_csv('data_test/trainNotNormalizedM1_15.csv', delimiter = ',').reset_index(drop=True)\n",
    "# data = pd.read_csv('data_test/trainM1_15.csv', delimiter = ',').reset_index(drop=True)\n",
    "data = pd.read_csv('data_test/M1-15+DemographicFeatureEng.csv', delimiter = ',').reset_index(drop=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_ans_data = pd.read_csv('data_test/M16All.csv', delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ans_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[['1','2','3','4','5','6','7','8','9','10','11','12','13','14','Age','AccAge','AgeWhenOpenAcc','mrch_cd_Indicator',\n",
    " 'Age_Indicator',\n",
    " 'AccAge_Indicator',\n",
    " 'AgeWhenOpenAcc_Indicator',\n",
    " '15_Indicator',\n",
    " 'New_cat_cd',\n",
    " 'New_gnd_cd',\n",
    " 'New_idv_incm_seg_cd',\n",
    " 'New_ip_tp_cd',\n",
    " 'New_ocp_cd',\n",
    " 'New_mar_st_cd',\n",
    " 'New_Province']]\n",
    "y = data[['15']].values\n",
    "x_ans = data[['2','3','4','5','6','7','8','9','10','11','12','13','14','15','Age','AccAge','AgeWhenOpenAcc','mrch_cd_Indicator',\n",
    " 'Age_Indicator',\n",
    " 'AccAge_Indicator',\n",
    " 'AgeWhenOpenAcc_Indicator',\n",
    " '15_Indicator',\n",
    " 'New_cat_cd',\n",
    " 'New_gnd_cd',\n",
    " 'New_idv_incm_seg_cd',\n",
    " 'New_ip_tp_cd',\n",
    " 'New_ocp_cd',\n",
    " 'New_mar_st_cd',\n",
    " 'New_Province']]\n",
    "y_ans = y_ans_data[['Target']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score1(y,y_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "####### for transform index category #######\n",
    "############################################\n",
    "import Encoder as en\n",
    "import numpy as np\n",
    "listcate = ['New_cat_cd',\n",
    " 'New_gnd_cd',\n",
    " 'New_idv_incm_seg_cd',\n",
    " 'New_ip_tp_cd',\n",
    " 'New_ocp_cd',\n",
    " 'New_mar_st_cd',\n",
    " 'New_Province']\n",
    "mcle = en.MultiColumnLabelEncoder(columns=np.array(listcate))\n",
    "mcle.fit(x)\n",
    "mcle.transform(x_ans)\n",
    "mcle.transform(x)\n",
    "x_ans = x_ans.values\n",
    "x = x.values\n",
    "\n",
    "print(x.shape,y.shape,x_ans.shape,y_ans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape,y.shape,x_ans.shape,y_ans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x, x_stack, y, y_stack = train_test_split(x, y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "x1 = pd.concat([x, x_ans])\n",
    "x = x.to_dict('records')\n",
    "x_ans = x_ans.to_dict('records')\n",
    "x1 = x1.to_dict('records')\n",
    "vec = DictVectorizer()\n",
    "\n",
    "vec.fit(x1)\n",
    "x = vec.transform(x).toarray()\n",
    "x_ans = vec.transform(x_ans).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA , NMF\n",
    "n_components = 15\n",
    "# pca = PCA(n_components=n_components, svd_solver='full',random_state=42)\n",
    "pca = NMF(n_components=n_components, init='random',random_state=42)\n",
    "pca.fit(x)\n",
    "x = pca.transform(x)\n",
    "x_ans = pca.transform(x_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "# x = preprocessing.normalize(x, norm='l2')MinMaxScaler\n",
    "# x_ans = preprocessing.normalize(x_ans, norm='l2')\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "# scaler.fit(x[:,:22])\n",
    "# x[:,:22] = scaler.transform(x[:,:22])\n",
    "# x_ans[:,:22] = scaler.transform(x_ans[:,:22])\n",
    "\n",
    "scaler.fit(x)\n",
    "x = scaler.transform(x)\n",
    "x_ans = scaler.transform(x_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Reshape,Bidirectional,Input, Dense,Embedding,GRU,Dropout,Conv1D,MaxPooling1D,Flatten,concatenate,Add,TimeDistributed,BatchNormalization\n",
    "from keras.optimizers import Adam , RMSprop\n",
    "from keras import regularizers\n",
    "import keras.utils\n",
    "from keras import optimizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "def gru_model():\n",
    "  input = Input(shape=(29,1))\n",
    "  x = Dropout(0.10)(input)\n",
    "#   x = input\n",
    "  x = Conv1D(30,4, padding='same')(x)\n",
    "  x = MaxPooling1D(pool_size=4, padding='same')(x)\n",
    "#   x = BatchNormalization()(x)\n",
    "  x = Conv1D(30,4, padding='same')(x)\n",
    "  x = MaxPooling1D(pool_size=4, padding='same')(x)\n",
    "#   x = BatchNormalization()(x)\n",
    "#   x = Flatten()(x)\n",
    "  x= Bidirectional(GRU(30))(x)\n",
    "#   x = BatchNormalization()(x)\n",
    "#   x = Reshape(14)(x)\n",
    "\n",
    "  x = Dropout(0.15)(x)\n",
    "#   x = Dense(5, activation='relu',kernel_regularizer=regularizers.l2(0.05))(x)\n",
    "#   x = Bidirectional(GRU(20))(x)\n",
    "#   x = Dropout(0.20)(x)\n",
    "  out = Dense(1, activation='linear')(x)\n",
    "  \n",
    "#   sgd = optimizers.SGD(lr=1000)\n",
    "  model = Model(inputs=input, outputs=[out])\n",
    "  model.compile(optimizer='adam',\n",
    "                loss='mean_absolute_error',\n",
    "                metrics=['mae'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "weight_path_feedforward_nn='recomment.h5'\n",
    "callbacks_list_feedforward_nn = [\n",
    "#         TensorBoard(log_dir='/data/Graph/ff', histogram_freq=1, write_graph=True, write_grads=False),\n",
    "        ModelCheckpoint(\n",
    "            weight_path_feedforward_nn,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            monitor='mae',\n",
    "            mode='min',\n",
    "            verbose=1\n",
    "        )\n",
    "  ]\n",
    "print('start training GRU')\n",
    "model_ = gru_model()\n",
    "verbose = 1\n",
    "n = 10000000\n",
    "# n = 1\n",
    "train_params = [(10 ,64)]\n",
    "for (epochs, batch_size) in train_params:\n",
    "  print(\"train with {} epochs and {} batch size\".format(epochs, batch_size))\n",
    "  model_.fit(np.reshape(x, (x.shape[0],x.shape[1],1)), y/n, epochs=epochs, batch_size=batch_size, verbose=verbose,\n",
    "                           callbacks=callbacks_list_feedforward_nn,\n",
    "                           validation_data=(np.reshape(x_ans, (x_ans.shape[0],x_ans.shape[1],1)), y_ans/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre = model_.predict(np.reshape(x_ans, (x_ans.shape[0],x_ans.shape[1],1)))\n",
    "y_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre[y_pre < 0] = 0\n",
    "score1(y_ans,y_pre*n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ycx_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(20 ,256)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.reshape(x, (x.shape[0],x.shape[1],1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error , precision_score , recall_score , accuracy_score , roc_curve , auc , classification_report,fbeta_score\n",
    "def smape(A, F):\n",
    "#     F[F <= 0] = 0.01\n",
    "#     F = np.exp(F)\n",
    "    b = np.abs(A) + np.abs(F)\n",
    "    b[b == 0] = 0.00001\n",
    "    smape_s = 100/len(A) * np.sum(2 * np.abs(F - A) / (b))\n",
    "    smape_s = 1-(smape_s/200)\n",
    "    return smape_s\n",
    "\n",
    "def score1(y_true,y_pre,thresholds = 0.5):\n",
    "    mae = mean_absolute_error(y_true, y_pre)\n",
    "#     mae = smape(y_true, y_pre)\n",
    "#     print(mae,'smape')\n",
    "    print(mae)\n",
    "    \n",
    "\n",
    "#     y_pre[y_pre >= threshold] = 1\n",
    "#     y_pre[y_pre < threshold] = 0\n",
    "#     print(fbeta_score(y_test, y_pre, pos_label=1 , average='binary', beta=0.5),'f2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = [(20 ,256),(20 ,256)]\n",
    "for (epochs, batch_size) in train_params:\n",
    "  print(\"train with {} epochs and {} batch size\".format(epochs, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Reshape,Bidirectional,Input, Dense,Embedding,GRU,Dropout,Conv1D,MaxPooling1D,Flatten,concatenate,Add,TimeDistributed,BatchNormalization\n",
    "from keras.optimizers import Adam , RMSprop\n",
    "from keras import regularizers\n",
    "import keras.utils\n",
    "from keras import optimizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "\n",
    "\n",
    "def gru_model():\n",
    "  input = Input(shape=(14,1))\n",
    "  x = input\n",
    "  x1 = Conv1D(30,4, padding='same')(x)\n",
    "  x1 = MaxPooling1D(pool_size=4, padding='same')(x1)\n",
    "  x1 = Conv1D(30,4, padding='same')(x1)\n",
    "  x1 = MaxPooling1D(pool_size=4, padding='same')(x1)\n",
    "  x1 = Flatten()(x1)\n",
    "  x2 = Bidirectional(GRU(15))(x)\n",
    "  x3 = Conv1D(30,4, padding='same')(x)\n",
    "  x3 = MaxPooling1D(pool_size=4, padding='same')(x3)\n",
    "  x3 = Conv1D(30,4, padding='same')(x3)\n",
    "  x3 = MaxPooling1D(pool_size=4, padding='same')(x3)\n",
    "  x3 = Bidirectional(GRU(15))(x3)\n",
    "  x1 = BatchNormalization()(x1)\n",
    "  x2 = BatchNormalization()(x2)\n",
    "  x3 = BatchNormalization()(x3)\n",
    "    \n",
    "  x = Add()([x1,x2,x3])\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Dropout(0.10)(x)\n",
    "  x_ = x\n",
    "  x = Reshape((30,1))(x)\n",
    "  x1 = Conv1D(15,4, padding='same')(x)\n",
    "  x1 = MaxPooling1D(pool_size=4, padding='same')(x1)\n",
    "  x1 = Conv1D(15,4, padding='same')(x1)\n",
    "  x1 = MaxPooling1D(pool_size=4, padding='same')(x1)\n",
    "  x1 = Flatten()(x1)\n",
    "\n",
    "  x2 = Bidirectional(GRU(15))(x)\n",
    "  x3 = Conv1D(20,4, padding='same')(x)\n",
    "  x3 = MaxPooling1D(pool_size=4, padding='same')(x3)\n",
    "  x3 = Conv1D(20,4, padding='same')(x3)\n",
    "  x3 = MaxPooling1D(pool_size=4, padding='same')(x3)\n",
    "#   x1 = Flatten()(x1)\n",
    "  x3 = Bidirectional(GRU(15))(x3)\n",
    "  x1 = BatchNormalization()(x1)\n",
    "  x2 = BatchNormalization()(x2)\n",
    "  x3 = BatchNormalization()(x3)\n",
    "    \n",
    "  x = Add()([x1,x2,x3,x_])\n",
    "  x = BatchNormalization()(x)\n",
    "\n",
    "  x = Dropout(0.10)(x)\n",
    "  x_2 = x\n",
    "  x = Reshape((30,1))(x)\n",
    "  x1 = Conv1D(15,4, padding='same')(x)\n",
    "  x1 = MaxPooling1D(pool_size=4, padding='same')(x1)\n",
    "  x1 = Conv1D(15,4, padding='same')(x1)\n",
    "  x1 = MaxPooling1D(pool_size=4, padding='same')(x1)\n",
    "  x1 = Flatten()(x1)\n",
    "\n",
    "  x2 = Bidirectional(GRU(15))(x)\n",
    "  x3 = Conv1D(20,4, padding='same')(x)\n",
    "  x3 = MaxPooling1D(pool_size=4, padding='same')(x3)\n",
    "  x3 = Conv1D(20,4, padding='same')(x3)\n",
    "  x3 = MaxPooling1D(pool_size=4, padding='same')(x3)\n",
    "  x3 = Bidirectional(GRU(15))(x3)\n",
    "  x1 = BatchNormalization()(x1)\n",
    "  x2 = BatchNormalization()(x2)\n",
    "  x3 = BatchNormalization()(x3)\n",
    "\n",
    "  x = Add()([x1,x2,x3,x_,x_2])\n",
    "#   x = BatchNormalization()(x)\n",
    "#   x = Dropout(0.15)(x)\n",
    "  out = Dense(1, activation='linear')(x)\n",
    "  \n",
    "#   sgd = optimizers.SGD(lr=1000)\n",
    "  model = Model(inputs=input, outputs=out)\n",
    "  model.compile(optimizer='adam',\n",
    "                loss='mean_absolute_error',\n",
    "                metrics=['mae'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yc = np.copy(y)\n",
    "yc[yc != 0] = 1\n",
    "yc_ans = np.copy(y_ans)\n",
    "yc_ans[yc_ans != 0] = 1\n",
    "# yc_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, ADASYN  \n",
    "from imblearn.combine import SMOTEENN ,SMOTETomek\n",
    "\n",
    "sm = SMOTEENN(random_state=42)\n",
    "# sm = SMOTETomek(random_state=42)\n",
    "# sm = ADASYN(random_state=42)\n",
    "# sm.fit(x)\n",
    "# x_sm, y_sm = sm.fit_sample(x_train, y_train)\n",
    "x, yc = sm.fit_sample(x, yc)\n",
    "\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "############### Xgboost ####################\n",
    "############################################\n",
    "import xgboost as xgb\n",
    "# dsm = xgb.DMatrix(x, label=y)\n",
    "dt = xgb.DMatrix(x_ans, label=yc_ans)\n",
    "dx = xgb.DMatrix(x, label=yc)\n",
    "                  \n",
    "# dt = xgb.DMatrix(x_ans_, label=yc_ans)\n",
    "# dx = xgb.DMatrix(x_, label=yc)\n",
    "\n",
    "# dsm = xgb.DMatrix(x_sm, label=y_sm)                  \n",
    "evallist = [(dt, 'eval'), (dx, 'train')]\n",
    "\n",
    "num_round = 10000\n",
    "# binary:logistic\n",
    "# param = {\n",
    "#  'objective': 'reg:linear',\n",
    "#  'colsample_bytree': 0.9683760122352089,\n",
    "#  'gamma':0.8835260600913024,\n",
    "#  'learning_rate': 0.199426498504554,\n",
    "#  'max_depth': 20,\n",
    "#  'min_child_weight': 10.95324500379702,\n",
    "#  'n_estimators': 25,\n",
    "# #  'objective': 'binary:logistic',\n",
    "#  'scale_pos_weight': 1,\n",
    "#  'seed': 42,\n",
    "#  'eval_metric': 'mae',\n",
    "#  'lambda': 150,\n",
    "#  'alpha': 100,\n",
    "# #  'rate_drop':0.950292864879127905,\n",
    "# #  'tree_method':'exact',\n",
    "#  'normalize_type':'forest',\n",
    "#  'subsample': 0.9035691355661921}gblinear\n",
    "\n",
    "param = {\n",
    "#     'booster': 'dart',\n",
    "#     'booster': 'gblinear',\n",
    "  'objective': 'binary:logistic',\n",
    " 'colsample_bytree': 0.9683760122352089,\n",
    " 'gamma':0.8835260600913024,\n",
    "#  'learning_rate': 0.1249426498504554,\n",
    "#  'max_depth': 20,\n",
    " 'min_child_weight': 110.95324500379702,\n",
    " 'n_estimators': 19,\n",
    " 'scale_pos_weight': 1,\n",
    " 'seed': 42,\n",
    " 'eval_metric': 'auc',\n",
    " 'lambda': 20,\n",
    " 'alpha': 120,\n",
    "# 'updater':'coord_descent',\n",
    "#     'feature_selector':'greedy',\n",
    "#     'top_k':12,\n",
    " 'rate_drop':0.950292864879127905,\n",
    " 'tree_method':'exact',\n",
    " 'normalize_type':'forest',\n",
    " 'subsample': 0.9035691355661921,\n",
    " 'skip_drop' : 0.6,\n",
    " 'one_drop': False,\n",
    "}\n",
    " \n",
    "evals_result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.train(param, dx, num_round, evallist, evals_result=evals_result,early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "treee = bst.best_ntree_limit\n",
    "# treee = 20\n",
    "# yc_pre = np.array(bst.predict(dt, ntree_limit=treee))\n",
    "yc_pre = bst.predict(dt)\n",
    "yc_pre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5910990907732\n",
    "threshold = 0.3972261\n",
    "ycx_pre  = yc_pre > threshold\n",
    "ycx_pre = ycx_pre.astype(int)\n",
    "f1_score(yc_ans, ycx_pre, average='binary',pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score,precision_recall_fscore_support\n",
    "fpr, tpr, thresholds = roc_curve(yc_ans, yc_pre, pos_label=1)\n",
    "for i in thresholds:\n",
    "#     print(i)\n",
    "    ycx_pre  = yc_pre > i\n",
    "    ycx_pre = ycx_pre.astype(int)\n",
    "#     print(f1_score(yc_ans, ycx_pre, average='binary',pos_label=1),i)\n",
    "    print(precision_score(yc_ans, ycx_pre, average='micro',pos_label=1),i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(yc_ans, yc_pre, pos_label=1)\n",
    "auc(fpr, tpr)\n",
    "# mean_absolute_error(yc_ans, yc_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(bst,max_num_features =20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yc_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(x, yc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba(x_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yc_pre = clf.predict_proba(x_ans)[:,0]\n",
    "yc_pre = clf.predict(x_ans)\n",
    "# >>> fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n",
    "# >>> metrics.auc(fpr, tpr)\n",
    "\n",
    "ycx_pre  = yc_pre > threshold\n",
    "ycx_pre = ycx_pre.astype(int)\n",
    "f1_score(yc_ans, ycx_pre, average='binary',pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yc_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(n_estimators=80, max_depth=18,\n",
    "                                    learning_rate=0.4, loss='lad',\n",
    "                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "from sklearn.ensemble.partial_dependence import partial_dependence\n",
    "from sklearn import linear_model\n",
    "from sklearn import grid_search\n",
    "import scipy.stats as st\n",
    "from sklearn.model_selection import RandomizedSearchCV , GridSearchCV\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import scipy.stats as st\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor, VotingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV , GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.externals import joblib\n",
    "import catboost as cb\n",
    "from lightgbm.sklearn import LGBMRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "\n",
    "gbr = GradientBoostingRegressor(n_estimators=80, max_depth=18,\n",
    "                                    learning_rate=0.4, loss='lad',\n",
    "                                    random_state=42)\n",
    "elas = linear_model.ElasticNetCV(cv=5, random_state=0,l1_ratio=[0.05,.1,0.15,.2, .5, .9,0.95, 1],n_jobs=-1\n",
    "                                 ,normalize=False, positive=False)\n",
    "\n",
    "n_jobs = 3\n",
    "# n_iter = 20\n",
    "# n_iter_nt = 1\n",
    "# n_components = 25\n",
    "cv = 5\n",
    "seed=42\n",
    "# n_features= 25\n",
    "n_features= 10\n",
    "scoring = 'neg_mean_absolute_error'\n",
    "scoring = 'neg_mean_squared_log_error'\n",
    "# scoring = 'r2'\n",
    "estimator = RandomForestRegressor()\n",
    "params = {\n",
    "    \"max_depth\": [1, 2, 3,5,10,20,30,40,50,100, None],\n",
    "    \"max_features\": st.randint(1, 300),\n",
    "#     \"max_features\": [\"log2\"],\n",
    "    \"min_samples_split\": st.randint(2, 10),\n",
    "#     \"min_samples_leaf\": st.randint(1, n_features),\n",
    "#     \"bootstrap\": [True, False],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    'oob_score': [True, False],\n",
    "    'random_state': [seed],\n",
    "    'warm_start' : [True],\n",
    "}\n",
    "rnf = RandomizedSearchCV(estimator, params, cv=cv,n_jobs=n_jobs, n_iter=10, scoring = scoring) \n",
    "clf = gbr\n",
    "\n",
    "bag = BaggingRegressor(elas,n_jobs = n_jobs, n_estimators=30,warm_start =True,bootstrap_features =True)\n",
    "clf = bag\n",
    "# clf = elas\n",
    "clf = rnf\n",
    "# estimator = SVR()\n",
    "# params = {  \n",
    "#     'C':[0.001, 0.01, 0.1, 1, 10], \n",
    "#     'degree': st.randint(1, 10),\n",
    "#     'shrinking': [True, False],\n",
    "# #     'probability': [True],\n",
    "#     'tol': [1e-3],\n",
    "# #     'random_state': [seed],\n",
    "# }\n",
    "# clf = RandomizedSearchCV(estimator, params, cv=cv,n_jobs=n_jobs, n_iter=10, scoring = 'neg_mean_absolute_error')\n",
    "\n",
    "\n",
    "clf.fit(x, y)\n",
    "# y_a = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_clf = clf.predict(x_ans)\n",
    "y_clf[y_clf < 0] = 0\n",
    "y_clf[y_clf > 2952646748] = 2952646748\n",
    "# y_ans = y_ans[:,0]\n",
    "score1(y_ans,y_clf)\n",
    "score1(y_ans,y_clf*yc_pre)\n",
    "score1(y_ans,y_clf*ycx_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(x_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    " 'objective': 'reg:linear',\n",
    " 'colsample_bytree': 0.9683760122352089,\n",
    " 'gamma':0.8835260600913024,\n",
    " 'learning_rate': 0.199426498504554,\n",
    " 'max_depth': 20,\n",
    " 'min_child_weight': 10.95324500379702,\n",
    " 'n_estimators': 25,\n",
    " 'scale_pos_weight': 12,\n",
    " 'seed': 42,\n",
    " 'eval_metric': 'mae',\n",
    " 'lambda': 150,\n",
    " 'alpha': 100,\n",
    " 'rate_drop':0.50292864879127905,\n",
    " 'tree_method':'exact',\n",
    " 'normalize_type':'forest',\n",
    " 'subsample': 0.9035691355661921\n",
    "}\n",
    "\n",
    "clf = XGBRegressor(**param)\n",
    "clf.fit(x,y)\n",
    "y_clf = clf.predict(x_ans)\n",
    "y_clf[y_clf < 0] = 0\n",
    "y_clf[y_clf > 2952646748] = 2952646748\n",
    "# y_ans = y_ans[:,0]\n",
    "score1(y_ans,y_clf)\n",
    "score1(y_ans,y_clf*yc_pre)\n",
    "score1(y_ans,y_clf*ycx_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb2 = XGBRegressor(**param).fit(x,y)\n",
    "rnf2 = rnf.fit(x,y)\n",
    "a = xgb2.predict(x_stack)\n",
    "b = rnf2.predict(x_stack)\n",
    "x_2 = np.column_stack((a,b))\n",
    "a = xgb2.predict(x_ans)\n",
    "b = rnf2.predict(x_ans)\n",
    "x_ans_2 = np.column_stack((a,b))\n",
    "\n",
    "x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [(\"xbg\", XGBRegressor(**param)),\n",
    "              (\"rnf\", rnf),\n",
    "              (\"elas\", elas),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros((x_stack.shape[0], len(estimators))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = np.zeros((x_stack.shape[0], len(estimators)))\n",
    "x_ans_2 = np.zeros((x_ans.shape[0], len(estimators)))\n",
    "for n, (name, estimator) in enumerate(estimators):\n",
    "#     print(n, name, estimator)\n",
    "    y_estimator = estimator.fit(x,y).predict(x_stack)\n",
    "    x_2[:,n] = y_estimator\n",
    "    x_ans_2[:,n] = estimator.predict(x_ans)\n",
    "    score1(y_stack,y_estimator)\n",
    "    print(name)\n",
    "x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(x_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "# a = clf.coef_\n",
    "# a = clf.best_estimator_.feature_importances_\n",
    "a = clf.feature_importances_\n",
    "# a = a[:15]\n",
    "pyplot.bar(range(len(a)), a)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(range(1,15))\n",
    "features = [(14,15),15]\n",
    "fig, axs = plot_partial_dependence(clf, x_ans, features,\n",
    "#                                        feature_names=list(data1)[:10],\n",
    "                                       n_jobs=-1, grid_resolution=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "############### Xgboost ####################\n",
    "############################################\n",
    "import xgboost as xgb\n",
    "# dsm = xgb.DMatrix(x, label=y)\n",
    "dt = xgb.DMatrix(x_ans_2, label=y_ans)\n",
    "dx = xgb.DMatrix(x_2, label=y_stack)\n",
    "                  \n",
    "# dt = xgb.DMatrix(x_ans_, label=yc_ans)\n",
    "# dx = xgb.DMatrix(x_, label=yc)\n",
    "\n",
    "# dsm = xgb.DMatrix(x_sm, label=y_sm)                  \n",
    "evallist = [(dt, 'eval'), (dx, 'train')]\n",
    "\n",
    "num_round = 100\n",
    "\n",
    "param = {\n",
    "    'booster':'gblinear',\n",
    " 'objective': 'reg:linear',\n",
    " 'colsample_bytree': 0.9683760122352089,\n",
    " 'gamma':0.8835260600913024,\n",
    "#  'learning_rate': 0.199426498504554,\n",
    " 'max_depth': 20,\n",
    " 'min_child_weight': 10.95324500379702,\n",
    " 'n_estimators': 25,\n",
    "#  'objective': 'binary:logistic',\n",
    " 'scale_pos_weight': 1,\n",
    " 'seed': 42,\n",
    " 'eval_metric': 'mae',\n",
    " 'lambda': 150,\n",
    " 'alpha': 100,\n",
    "#  'rate_drop':0.50292864879127905,\n",
    " 'tree_method':'exact',\n",
    " 'normalize_type':'forest',\n",
    " 'subsample': 0.9035691355661921\n",
    "}\n",
    "# gblinear\n",
    "\n",
    " \n",
    "evals_result = {}\n",
    "bst = xgb.train(param, dx, num_round, evallist, evals_result=evals_result,early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treee = bst.best_ntree_limit\n",
    "# treee = 20\n",
    "# y_clf = np.array(bst.predict(dt, ntree_limit=treee))\n",
    "y_clf = bst.predict(dt)\n",
    "y_clf[y_clf < 0] = 0\n",
    "y_clf[y_clf > 2952646748] = 2952646748\n",
    "# y_ans = y_ans[:,0]\n",
    "score1(y_ans,y_clf)\n",
    "score1(y_ans,y_clf*yc_pre)\n",
    "score1(y_ans,y_clf*ycx_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = bag\n",
    "clf.fit(x_2,y_stack)\n",
    "y_clf = clf.predict(x_ans_2)\n",
    "y_clf[y_clf < 0] = 0\n",
    "y_clf[y_clf > 2952646748] = 2952646748\n",
    "# y_ans = y_ans[:,0]\n",
    "score1(y_ans,y_clf)\n",
    "score1(y_ans,y_clf*yc_pre)\n",
    "score1(y_ans,y_clf*ycx_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([])\n",
    "# a[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = clf.coef_\n",
    "a = clf.best_estimator_.feature_importances_\n",
    "n = 0.01\n",
    "a[a > n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(a != 0)[:,0]\n",
    "x_ = x[:,np.argwhere(a > n)[:,0]]\n",
    "x_ans_ = x_ans[:,np.argwhere(a > n)[:,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = x[:,:14]\n",
    "x_ans_ = x_ans[:,:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ = linear_model.ElasticNetCV(cv=5, random_state=0,l1_ratio=[0.05,.1,0.15,.2, .5, .9,0.95, 1],n_jobs=-1\n",
    "                                 ,normalize=False, positive=False)\n",
    "n_features= x_.shape[1]\n",
    "scoring = 'neg_mean_absolute_error'\n",
    "# scoring = 'neg_mean_squared_error'\n",
    "\n",
    "estimator = RandomForestRegressor()\n",
    "\n",
    "# Parameter for RandomForestClassifier\n",
    "params = {\n",
    "    \"max_depth\": [3, None],\n",
    "    \"max_features\": st.randint(1, n_features),\n",
    "    \"min_samples_split\": st.randint(2, 10),\n",
    "    \"min_samples_leaf\": st.randint(1, n_features),\n",
    "#     \"bootstrap\": [True, False],\n",
    "    \"criterion\": [\"mse\"],\n",
    "    'oob_score': [True, False],\n",
    "    'random_state': [seed],\n",
    "}\n",
    "rnf = RandomizedSearchCV(estimator, params, cv=cv,n_jobs=n_jobs, n_iter=50, scoring = scoring) \n",
    "\n",
    "clf_ = gbr\n",
    "# clf_ = rnf\n",
    "clf_.fit(x_, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_clf = clf_.predict(x_ans_)\n",
    "y_clf[y_clf < 0] = 0\n",
    "y_clf[y_clf > 2952646748] = 2952646748\n",
    "# y_ans = y_ans[:,0]\n",
    "# y_clf = y_clf[:,0]\n",
    "score1(y_ans,y_clf)\n",
    "score1(y_ans,y_clf*yc_pre)\n",
    "score1(y_ans,y_clf*ycx_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(range(0,10))\n",
    "fig, axs = plot_partial_dependence(clf_, x_ans_, features,\n",
    "#                                        feature_names=list(data1)[:10],\n",
    "                                       n_jobs=-1, grid_resolution=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### models ################ param\n",
    "############################################\n",
    "######## scikit-learn multi-models #########\n",
    "############################################\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import scipy.stats as st\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor, VotingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV , GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.externals import joblib\n",
    "import catboost as cb\n",
    "from lightgbm.sklearn import LGBMRegressor\n",
    "\n",
    "n_jobs = 3\n",
    "n_iter = 30\n",
    "n_iter_nt = 3\n",
    "n_components = 25\n",
    "cv = 5\n",
    "seed=42\n",
    "n_features= 14\n",
    "# n_features= x_sm.shape[1]\n",
    "# n_features= n_components\n",
    "is_pca = False\n",
    "scoring = 'neg_mean_absolute_error'\n",
    "# is_pca = True\n",
    "\n",
    "# estimator = XGBClassifier(nthreads=-1,tree_method='exact')\n",
    "param = {'objective': 'reg:linear',\n",
    " 'colsample_bytree': 0.9683760122352089,\n",
    " 'gamma': 0.7790711924812199,\n",
    " 'learning_rate': 0.2249426498504554,\n",
    " 'max_depth': 20,\n",
    " 'min_child_weight': 10.95324500379702,\n",
    " 'n_estimators': 150,\n",
    " 'scale_pos_weight': 1,\n",
    " 'seed': 42,\n",
    " 'eval_metric': ['mae'],\n",
    " 'lambda': 2,\n",
    " 'alpha': 15,\n",
    " 'rate_drop':0.1,\n",
    " 'tree_method':'exact',\n",
    " 'normalize_type':'forest',\n",
    " 'subsample': 0.9035691355661921}\n",
    "\n",
    "estimator = XGBRegressor(**param)\n",
    "# objective = 'binary:logistic'\n",
    "\n",
    "# Parameter for XGBoost\n",
    "params = {  \n",
    "    \"n_estimators\": st.randint(3, 40),\n",
    "    \"max_depth\": st.randint(3, 30),\n",
    "    \"learning_rate\": st.uniform(0.05, 0.4),\n",
    "    \"colsample_bytree\": st.beta(10, 1),\n",
    "    \"subsample\": st.beta(10, 1),\n",
    "    \"gamma\": st.uniform(0, 10),\n",
    "#     'objective': [objective],\n",
    "    'scale_pos_weight': st.randint(0, 2),\n",
    "    \"min_child_weight\": st.expon(0, 50),\n",
    "    'lambda': st.randint(1, 20),\n",
    "    'alpha': st.randint(0, 20),\n",
    "    'rate_drop':st.uniform(0, 1),\n",
    "    \"seed\": [seed],\n",
    "}\n",
    "\n",
    "xgb = RandomizedSearchCV(estimator, params, cv=cv,n_jobs=n_jobs, n_iter=n_iter, scoring = scoring) \n",
    "if(is_pca):\n",
    "    estimators = [('reduce_dim', PCA(n_components=n_components, svd_solver='full')), ('clf', xgb)]\n",
    "    xgb = Pipeline(estimators)\n",
    "    xgb = GridSearchCV(xgb, cv=5, n_jobs=-1, param_grid=param_grid)\n",
    "    \n",
    "#################################################################\n",
    "#################################################################\n",
    "\n",
    "estimator = LogisticRegression()\n",
    "# Parameter for LogisticRegression\n",
    "params = {\n",
    "    \"penalty\": ['l2'],\n",
    "    \"C\": [0.001, 0.01, 0.1, 1, 10],\n",
    "    \"tol\": [1e-4, 1e-3, 1e-2, 1e-1],\n",
    "    \"solver\": ['newton-cg', 'lbfgs', 'liblinear', 'sag'],\n",
    "    \"max_iter\": st.randint(50, 100),\n",
    "    'random_state': [seed],\n",
    "} \n",
    "log = RandomizedSearchCV(estimator, params, cv=cv,n_jobs=n_jobs, n_iter=n_iter , scoring = scoring) \n",
    "if(is_pca):\n",
    "    estimators = [('reduce_dim', PCA(n_components=n_components, svd_solver='full')), ('clf', log)]\n",
    "    log = Pipeline(estimators)\n",
    "    log = GridSearchCV(log, cv=5, n_jobs=-1, param_grid=param_grid)\n",
    "\n",
    "#################################################################\n",
    "#################################################################\n",
    "\n",
    "estimator = KNeighborsRegressor()\n",
    "# Parameter for KNeighborsClassifier\n",
    "params = {\n",
    "    \"n_neighbors\": st.randint(2, 50),\n",
    "    \"weights\": ['uniform', 'distance'],\n",
    "    \"algorithm\": ['ball_tree', 'kd_tree', 'brute'],\n",
    "    \"leaf_size\": st.randint(10, 30),\n",
    "    \"p\": st.randint(1, 2),\n",
    "}\n",
    "knn = RandomizedSearchCV(estimator, params, cv=cv,n_jobs=n_jobs, n_iter=n_iter_nt , scoring = scoring)\n",
    "if(is_pca):\n",
    "    estimators = [('reduce_dim', PCA(n_components=n_components, svd_solver='full')), ('clf', knn)]\n",
    "    knn = Pipeline(estimators)\n",
    "    \n",
    "#################################################################\n",
    "#################################################################\n",
    "\n",
    "estimator = RandomForestRegressor()\n",
    "\n",
    "# Parameter for RandomForestClassifier\n",
    "params = {\n",
    "    \"max_depth\": [3, None],\n",
    "    \"max_features\": st.randint(1, n_features),\n",
    "    \"min_samples_split\": st.randint(2, 10),\n",
    "    \"min_samples_leaf\": st.randint(1, n_features),\n",
    "    \"bootstrap\": [True, False],\n",
    "    \"criterion\": [\"mse\"],\n",
    "    'random_state': [seed],\n",
    "}\n",
    "rnf = RandomizedSearchCV(estimator, params, cv=cv,n_jobs=n_jobs, n_iter=n_iter, scoring = scoring) \n",
    "if(is_pca):\n",
    "    estimators = [('reduce_dim', PCA(n_components=n_components, svd_solver='full')), ('clf', rnf)]\n",
    "    rnf = Pipeline(estimators)\n",
    "    rnf = GridSearchCV(rnf, cv=5, n_jobs=-1, param_grid=param_grid)\n",
    "    \n",
    "#################################################################\n",
    "#################################################################\n",
    "\n",
    "estimator = ExtraTreesRegressor()\n",
    "# \n",
    "# Parameter for ExtraTreesClassifier\n",
    "params = {\n",
    "    \"n_estimators\": st.randint(5, 50),\n",
    "    \"max_depth\": [3, None],\n",
    "    \"max_features\": st.randint(1, n_features),\n",
    "    \"min_samples_split\": st.randint(2, 10),\n",
    "    \"min_samples_leaf\": st.randint(1, n_features),\n",
    "    \"bootstrap\": [True],\n",
    "    \"oob_score\": [False,True],\n",
    "#     \"criterion\": [\"gini\", \"entropy\"],\n",
    "    'random_state': [seed],\n",
    "    \n",
    "}\n",
    "ext = RandomizedSearchCV(estimator, params, cv=cv,n_jobs=n_jobs, n_iter=n_iter, scoring = scoring) \n",
    "if(is_pca):\n",
    "    estimators = [('reduce_dim', PCA(n_components=n_components, svd_solver='full')), ('clf', ext)]\n",
    "    ext = Pipeline(estimators)\n",
    "    ext = GridSearchCV(ext, cv=5, n_jobs=-1, param_grid=param_grid)\n",
    "\n",
    "#################################################################\n",
    "#################################################################\n",
    "\n",
    "estimator = AdaBoostRegressor()\n",
    "# Parameter for AdaBoost\n",
    "params = { \n",
    "    'n_estimators':st.randint(10, 100), \n",
    "    'learning_rate':st.beta(10, 1), \n",
    "#     'algorithm':['SAMME', 'SAMME.R'],\n",
    "    'random_state': [seed],\n",
    "}\n",
    "ada = RandomizedSearchCV(estimator, params, cv=cv,n_jobs=n_jobs, n_iter=n_iter, scoring = scoring) \n",
    "if(is_pca):\n",
    "    estimators = [('reduce_dim', PCA(n_components=n_components, svd_solver='full')), ('clf', ada)]\n",
    "    ada = Pipeline(estimators)\n",
    "    ada = GridSearchCV(ada, cv=5, n_jobs=-1, param_grid=param_grid)\n",
    "    \n",
    "\n",
    "\n",
    "#################################################################\n",
    "#################################################################\n",
    "estimator = SVR()\n",
    "# Parameter for SVC\n",
    "params = {  \n",
    "    'C':[0.001, 0.01, 0.1, 1, 10], \n",
    "    'degree': st.randint(1, 10),\n",
    "    'shrinking': [True, False],\n",
    "#     'probability': [True],\n",
    "    'tol': [1e-3],\n",
    "#     'random_state': [seed],\n",
    "}\n",
    "svc = RandomizedSearchCV(estimator, params, cv=cv,n_jobs=n_jobs, n_iter=n_iter_nt, scoring = scoring)\n",
    "if(is_pca):\n",
    "    estimators = [('reduce_dim', PCA(n_components=n_components, svd_solver='full')), ('clf', svc)]\n",
    "    svc = Pipeline(estimators)\n",
    "#     svc = GridSearchCV(svc, cv=5, n_jobs=-1, param_grid=param_grid)\n",
    "\n",
    "#################################################################\n",
    "#################################################################\n",
    "\n",
    "# Parameter for LGBMClassifier\n",
    "params = {'boosting_type': 'gbdt',\n",
    "          'max_depth' : -1,\n",
    "#           'objective': 'binary',\n",
    "          'nthread': 3, # Updated from nthread\n",
    "          'num_leaves': 64,\n",
    "          'learning_rate': 0.05,\n",
    "          'max_bin': 512,\n",
    "          'subsample_for_bin': 200,\n",
    "          'subsample': 1,\n",
    "          'subsample_freq': 1,\n",
    "          'colsample_bytree': 0.8,\n",
    "          'reg_alpha': 5,\n",
    "          'reg_lambda': 10,\n",
    "          'min_split_gain': 0.5,\n",
    "          'min_child_weight': 1,\n",
    "          'min_child_samples': 5,\n",
    "          'scale_pos_weight': 1,\n",
    "          'num_class' : 1,\n",
    "          'metric' : 'mape'}\n",
    "\n",
    "gridParams = {\n",
    "    'learning_rate': [0.005],\n",
    "    'n_estimators': [40],\n",
    "    'num_leaves': [6,8,12,16],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    'objective' : ['mape'],\n",
    "    'random_state' : [42,502], # Updated from 'seed'\n",
    "    'colsample_bytree' : [0.65, 0.66],\n",
    "    'subsample' : [0.7,0.75],\n",
    "    'reg_alpha' : [1,1.2],\n",
    "    'reg_lambda' : [1,1.2,1.4],\n",
    "    }\n",
    "\n",
    "estimator = LGBMRegressor(boosting_type= 'gbdt',\n",
    "          objective = 'mape',\n",
    "          n_jobs = 3, # Updated from 'nthread'\n",
    "          silent = True,\n",
    "          max_depth = params['max_depth'],\n",
    "          max_bin = params['max_bin'],\n",
    "          subsample_for_bin = params['subsample_for_bin'],\n",
    "          subsample = params['subsample'],\n",
    "          subsample_freq = params['subsample_freq'],\n",
    "          min_split_gain = params['min_split_gain'],\n",
    "          min_child_weight = params['min_child_weight'],\n",
    "          min_child_samples = params['min_child_samples'],\n",
    "          scale_pos_weight = params['scale_pos_weight'])\n",
    "\n",
    "lgb = GridSearchCV(estimator, gridParams, verbose=0, cv=cv,n_jobs=n_jobs , scoring = scoring)\n",
    "if(is_pca):\n",
    "    print('asas')\n",
    "    estimators = [('reduce_dim', PCA(n_components=n_components, svd_solver='full')), ('clf', lgb)]\n",
    "    lgb = Pipeline(estimators)\n",
    "    lgb = GridSearchCV(lgb, cv=5, n_jobs=-1, param_grid=param_grid)\n",
    "# lgb = RandomizedSearchCV(estimator, params, cv=cv,n_jobs=n_jobs, n_iter=n_iter_nt)\n",
    "\n",
    "#################################################################\n",
    "#################################################################\n",
    "\n",
    "estimator = cb.CatBoostRegressor()\n",
    "params = {'depth': st.randint(3, 16),\n",
    "          'learning_rate' : st.uniform(0.05, 0.4),\n",
    "         'l2_leaf_reg': st.randint(0, 10),\n",
    "         'iterations': [1]}\n",
    "\n",
    "cat = RandomizedSearchCV(estimator, params, cv=cv,n_jobs=n_jobs, n_iter=2, scoring = scoring) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA ,NMF\n",
    "from sklearn.pipeline import Pipeline\n",
    "nmf = PCA(n_components=13, random_state=42)\n",
    "clf = svc\n",
    "estimators = [('reduce_dim',nmf ), ('model', clf)]\n",
    "# clf = Pipeline(estimators)\n",
    "\n",
    "clf.fit(x, y)\n",
    "y_clf = clf.predict(x_ans)\n",
    "y_clf[y_clf < 0] = 0\n",
    "y_clf[y_clf > 2952646748] = 2952646748\n",
    "score1(y_ans,y_clf)\n",
    "score1(y_ans,y_clf*yc_pre)\n",
    "score1(y_ans,y_clf*ycx_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yc_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score1(y_ans,y_clf)\n",
    "score1(y_ans,y_clf[:,0]*yc_pre)\n",
    "score1(y_ans,y_clf[:,0]*ycx_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
