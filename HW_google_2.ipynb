{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EBM_IT_01\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(521976, 29)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score , precision_score , recall_score , accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# data = pd.read_csv('data_test/M1-15+DemographicFeatureEng.csv', delimiter = ',').reset_index(drop=True)\n",
    "# y_ans_data = pd.read_csv('data_test/M16All.csv', delimiter = ',') .drop(['B', 'C'], axis=1) \n",
    "# data = pd.read_csv('google_data/GGtrainnnnnnnnnnnnnnnnnnn.csv', delimiter = ',').reset_index(drop=True)\n",
    "data = pd.read_csv('google_data/trainFeiFeiFin.csv', delimiter = ',').reset_index(drop=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_list = [\n",
    "    'Mode_channelGrouping',\n",
    "'Mode_device.deviceCategory',\n",
    " 'Mode_device.isMobile',\n",
    " 'Mode_geoNetwork.continent',\n",
    " 'Mode_geoNetwork.subContinent',\n",
    "    'Mode_Month',\n",
    " 'Mode_browser2',\n",
    " 'Mode_OperatSys2',\n",
    " 'Mode_city2',\n",
    " 'Mode_Country2',\n",
    " 'Mode_metro2',\n",
    " 'Mode_Domain2',\n",
    " 'Mode_region2',\n",
    " 'Mode_campaign',\n",
    " 'Mode_TrueFalse',\n",
    " 'Mode_Keyword',\n",
    " 'Mode_SurceMed',\n",
    " 'Mode_SourceSource',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in category_list:\n",
    "#     print(len(data[i].unique()),i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EBM_IT_01\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(617242, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ans = pd.read_csv('google_data/ForYourSubmissionIsus.csv', delimiter = ',')\n",
    "data_ans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[category_list] = data[category_list].fillna('none')\n",
    "data_ans[category_list] = data_ans[category_list].fillna('none')\n",
    "data = data.fillna(0)\n",
    "data_ans = data_ans.fillna(0)\n",
    "data[['Mode_device.isMobile','Mode_TrueFalse']] = data[['Mode_device.isMobile','Mode_TrueFalse']].astype('str')\n",
    "data_ans[['Mode_device.isMobile','Mode_TrueFalse']] = data_ans[['Mode_device.isMobile','Mode_TrueFalse']].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = data.drop(['Target','fullVisitorId'], axis=1)\n",
    "x = data_x\n",
    "feature_name = list(data_x)\n",
    "y = data['Target'].values\n",
    "# y = np.log(y+1)\n",
    "# y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "def target_encode(trn_series=None, \n",
    "                  tst_series=None, \n",
    "                  target=None, \n",
    "                  min_samples_leaf=0, \n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior  \n",
    "    \"\"\" \n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean \n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index \n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ans = data_ans[feature_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1139218, 27)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Mode_channelGrouping',\n",
       " 'Mode_device.deviceCategory',\n",
       " 'Mode_device.isMobile',\n",
       " 'Mode_geoNetwork.continent',\n",
       " 'Mode_OperatSys2',\n",
       " 'Mode_campaign',\n",
       " 'Mode_TrueFalse',\n",
       " 'Mode_SurceMed']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combine=pd.concat([x,x_ans],ignore_index=True)\n",
    "print(df_combine.shape)\n",
    "#Find One_hot features that unique count <15\n",
    "one_hot = df_combine[list(category_list)].nunique().reset_index()\n",
    "one_hot.columns = ['features','unique_count']\n",
    "one_hot = one_hot.loc[one_hot['unique_count'] < 10,\"features\"]\n",
    "one_hot = list(one_hot)\n",
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in one_hot:\n",
    "    category_list.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process feature =====>Mode_channelGrouping\n",
      "(1139218, 9)\n",
      "(1139218, 35)\n",
      "Process feature =====>Mode_device.deviceCategory\n",
      "(1139218, 4)\n",
      "(1139218, 38)\n",
      "Process feature =====>Mode_device.isMobile\n",
      "(1139218, 3)\n",
      "(1139218, 40)\n",
      "Process feature =====>Mode_geoNetwork.continent\n",
      "(1139218, 7)\n",
      "(1139218, 46)\n",
      "Process feature =====>Mode_OperatSys2\n",
      "(1139218, 9)\n",
      "(1139218, 54)\n",
      "Process feature =====>Mode_campaign\n",
      "(1139218, 8)\n",
      "(1139218, 61)\n",
      "Process feature =====>Mode_TrueFalse\n",
      "(1139218, 2)\n",
      "(1139218, 62)\n",
      "Process feature =====>Mode_SurceMed\n",
      "(1139218, 8)\n",
      "(1139218, 69)\n"
     ]
    }
   ],
   "source": [
    "for i in one_hot:\n",
    "    print(\"Process feature =====>\"+str(i))\n",
    "    df_combine[\"one_hot_feature\"] = df_combine[i]\n",
    "    df_combine[\"one_hot_feature\"] =  str(i) + \".\" + df_combine[\"one_hot_feature\"].astype('str')\n",
    "    one_hot_combine = pd.get_dummies(df_combine[\"one_hot_feature\"])\n",
    "    print(one_hot_combine.shape)\n",
    "    df_combine = df_combine.join(one_hot_combine)\n",
    "    del df_combine[\"one_hot_feature\"]\n",
    "    del df_combine[i]\n",
    "    del one_hot_combine\n",
    "    print(df_combine.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(521976, 69)\n",
      "(617242, 69)\n"
     ]
    }
   ],
   "source": [
    "x = df_combine[:len(x)]\n",
    "print(x.shape)\n",
    "x_ans = df_combine[len(x):]\n",
    "print(x_ans.shape)\n",
    "del df_combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode_category = category_list\n",
    "for i in category_list:\n",
    "    x[i], x_ans[i] = target_encode(x[i], \n",
    "                             x_ans[i], \n",
    "                             target=data['Target'], \n",
    "                             min_samples_leaf=10,\n",
    "                             smoothing=5,\n",
    "                             noise_level=0,\n",
    "                                  )\n",
    "feature_name = list(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = list(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "DataError",
     "evalue": "No numeric types to aggregate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDataError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36maggregate\u001b[1;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[0;32m   4656\u001b[0m         axis=''))\n\u001b[0;32m   4657\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0maggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4658\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataFrameGroupBy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4659\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4660\u001b[0m     \u001b[0magg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maggregate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36maggregate\u001b[1;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[0;32m   4087\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4088\u001b[0m         \u001b[0m_level\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_level'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4089\u001b[1;33m         \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_aggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_level\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_level\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4090\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhow\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4091\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_aggregate\u001b[1;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_agg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_agg_1dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mSpecificationError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_agg\u001b[1;34m(arg, func)\u001b[0m\n\u001b[0;32m    439\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magg_how\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m                     \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magg_how\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_agg_1dim\u001b[1;34m(name, how, subset)\u001b[0m\n\u001b[0;32m    422\u001b[0m                     raise SpecificationError(\"nested dictionary is ambiguous \"\n\u001b[0;32m    423\u001b[0m                                              \"in aggregation\")\n\u001b[1;32m--> 424\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mcolg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_level\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_level\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0m_agg_2dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36maggregate\u001b[1;34m(self, func_or_funcs, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3483\u001b[0m             \u001b[1;31m# but not the class list / tuple itself.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3484\u001b[0m             ret = self._aggregate_multiple_funcs(func_or_funcs,\n\u001b[1;32m-> 3485\u001b[1;33m                                                  (_level or 0) + 1)\n\u001b[0m\u001b[0;32m   3486\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3487\u001b[0m             \u001b[0mcyfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_cython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_or_funcs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m_aggregate_multiple_funcs\u001b[1;34m(self, arg, _level)\u001b[0m\n\u001b[0;32m   3556\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3557\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_selection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3558\u001b[1;33m             \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3560\u001b[0m         if isinstance(list(compat.itervalues(results))[0],\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36maggregate\u001b[1;34m(self, func_or_funcs, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3477\u001b[0m         \u001b[0m_level\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_level'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_or_funcs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3479\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc_or_funcs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3481\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_or_funcs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36mmean\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1304\u001b[0m         \u001b[0mnv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_groupby_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'numeric_only'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1305\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cython_agg_general\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1307\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mGroupByError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m_cython_agg_general\u001b[1;34m(self, how, alt, numeric_only, min_count)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mDataError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No numeric types to aggregate'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_aggregated_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDataError\u001b[0m: No numeric types to aggregate"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "agg_func={}\n",
    "agg_col=['fullVisitorId']\n",
    "for col in [i for i in x.columns]:\n",
    "    if col=='totals_transactionRevenue':\n",
    "        agg_func[col]=['sum']\n",
    "        agg_col.append(str(col)+'_sum')\n",
    "    elif col=='revenue_status':\n",
    "        agg_func[col]=['sum']\n",
    "        agg_col.append(str(col)+'_sum')\n",
    "    else:\n",
    "        agg_func[col]=['sum','max','min','mean','var','std']\n",
    "        agg_col.append(str(col)+'_sum')\n",
    "        agg_col.append(str(col)+'_max')\n",
    "        agg_col.append(str(col)+'_min')\n",
    "        agg_col.append(str(col)+'_mean')\n",
    "        agg_col.append(str(col)+'_var')\n",
    "        agg_col.append(str(col)+'_std')\n",
    "    \n",
    "x=x.groupby(x.fullVisitorId).aggregate(agg_func).reset_index()\n",
    "x.columns=agg_col\n",
    "\n",
    "x_ans=x_ans.groupby(x_ans.fullVisitorId).aggregate(agg_func).reset_index()\n",
    "x_ans.columns=agg_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(521976, 69) (521976,) (617242, 69)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape,y.shape,x_ans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.values\n",
    "x_ans = x_ans.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.01787473, 0.        , ..., 0.        , 1.        ,\n",
       "        0.        ],\n",
       "       [1.        , 0.03911253, 0.        , ..., 0.        , 1.        ,\n",
       "        0.        ],\n",
       "       [1.        , 0.61897598, 0.        , ..., 0.        , 1.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [3.        , 0.61897598, 1.        , ..., 0.        , 1.        ,\n",
       "        0.        ],\n",
       "       [1.        , 0.61897598, 1.        , ..., 0.        , 1.        ,\n",
       "        0.        ],\n",
       "       [1.        , 0.00420803, 0.        , ..., 0.        , 1.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(521976, 28) (521976,) (617242, 28)\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "####### for transform index category #######\n",
    "############################################\n",
    "import Encoder as en\n",
    "import numpy as np\n",
    "listcate = category_list\n",
    "mcle = en.MultiColumnLabelEncoder(columns=np.array(listcate))\n",
    "x1 = pd.concat([x, x_ans])\n",
    "mcle.fit(x1)\n",
    "mcle.transform(x_ans)\n",
    "mcle.transform(x)\n",
    "x_ans = x_ans.values\n",
    "x = x.values\n",
    "\n",
    "print(x.shape,y.shape,x_ans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-6524cebba10e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mx_ans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_ans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_ans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    945\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "x1 = pd.concat([x, x_ans])\n",
    "x = x.to_dict('records')\n",
    "x_ans = x_ans.to_dict('records')\n",
    "x1 = x1.to_dict('records')\n",
    "vec = DictVectorizer()\n",
    "\n",
    "vec.fit(x1)\n",
    "x = vec.transform(x).toarray()\n",
    "x_ans = vec.transform(x_ans).toarray()\n",
    "print(x.shape,y.shape,x_ans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "# x = preprocessing.normalize(x, norm='l2')MinMaxScaler\n",
    "# x_ans = preprocessing.normalize(x_ans, norm='l2')\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaler.fit(x)\n",
    "x = scaler.transform(x)\n",
    "x_ans = scaler.transform(x_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(521976, 20) (617242, 20)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape,x_ans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA , NMF\n",
    "n_components = 20\n",
    "pca = PCA(n_components=n_components, svd_solver='full',random_state=42)\n",
    "# pca = NMF(n_components=n_components, init='random',random_state=42)\n",
    "pca.fit(x)\n",
    "x = pca.transform(x)\n",
    "x_ans = pca.transform(x_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# x, x_stack, y, y_stack = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = y_test.shape[0]\n",
    "ww =np.argwhere(y_test == 0).shape[0]\n",
    "ww/w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EBM_IT_01\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\EBM_IT_01\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_stack' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-06ae2c6311fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx_2i\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_ans_2i\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mx_2\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mx_ans_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlevel_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_stack\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_stack\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_ans\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;31m# x_2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_stack' is not defined"
     ]
    }
   ],
   "source": [
    "import regressor as reg_model\n",
    "params1 = {\n",
    "    'n_jobs' : 3,\n",
    "    'n_iter' : 30,\n",
    "    'n_iter_nt' : 3,\n",
    "    'n_components' : 25,\n",
    "    'cv' : 5,\n",
    "    'seed':42,\n",
    "    'n_features': 14,\n",
    "    'is_pca' : False,\n",
    "    'scoring' : 'neg_mean_absolute_error',\n",
    "}\n",
    "\n",
    "estimators = reg_model.get_regressor(**params1)\n",
    "def level_1(x,y,x_stack,y_stack,x_ans,estimators):\n",
    "    x_2i = np.zeros((x_stack.shape[0], len(estimators)))\n",
    "    x_ans_2i = np.zeros((x_ans.shape[0], len(estimators)))\n",
    "    for n, (name, estimator) in enumerate(estimators):\n",
    "        y_predict = estimator.fit(x,y).predict(x_stack)\n",
    "        y_predict = y_predict.reshape((y_predict.shape[0],))\n",
    "        x_2i[:,n] = y_predict\n",
    "        y_predict_ans = estimator.predict(x_ans)\n",
    "        y_predict_ans = y_predict_ans.reshape((y_predict_ans.shape[0],))\n",
    "        x_ans_2i[:,n] = y_predict_ans\n",
    "#         print(score1(y_stack,y_predict),name)\n",
    "        print(score1(y_ans,y_predict_ans),name)\n",
    "    return x_2i,x_ans_2i\n",
    "\n",
    "x_2 , x_ans_2 = level_1(x,y,x_stack,y_stack,x_ans,estimators)\n",
    "# x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "679746.4124593252 xbg\n",
      "728841.477104563 knn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:739: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "641554.6066446591 rnf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:739: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "647002.6588113708 ext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "884031.1677741491 ada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1442725.939299192 svc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1094: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455082.75424669473 elas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361464.71858865954 bag\n"
     ]
    }
   ],
   "source": [
    "params2 = {\n",
    "    'n_jobs' : 3,\n",
    "    'n_iter' : 30,\n",
    "    'n_iter_nt' : 3,\n",
    "    'n_components' : 25,\n",
    "    'cv' : 5,\n",
    "    'seed':42,\n",
    "    'n_features': x_2.shape[1],\n",
    "    'is_pca' : False,\n",
    "    'scoring' : 'neg_mean_absolute_error',\n",
    "}\n",
    "\n",
    "estimators2 = reg_model.get_regressor(**params2)\n",
    "x_3 , x_ans_3 = level_1(x_2,y_stack,x_ans_2,y_ans,x_ans_2,estimators2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error , mean_squared_error , recall_score , accuracy_score , roc_curve , auc , classification_report,fbeta_score\n",
    "def score1(y_true,y_pre,thresholds = 0.5):\n",
    "#     mae = mean_absolute_error(y_true, y_pre)\n",
    "    mae = mean_squared_error(y_true, y_pre)\n",
    "    return mae**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2[1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingRegressor(base_estimator=ElasticNetCV(alphas=None, copy_X=True, cv=5, eps=0.001, fit_intercept=True,\n",
       "       l1_ratio=[0.05, 0.1, 0.15, 0.2, 0.5, 0.9, 0.95, 1], max_iter=1000,\n",
       "       n_alphas=100, n_jobs=-1, normalize=False, positive=False,\n",
       "       precompute='auto', random_state=0, selection='cyclic', tol=0.0001,\n",
       "       verbose=0),\n",
       "         bootstrap=True, bootstrap_features=True, max_features=1.0,\n",
       "         max_samples=1.0, n_estimators=30, n_jobs=3, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=True)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators2[7][1].predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343744.8873469687"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score1(y_ans,x_3[:,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "317441.0751886432"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.mean(x_ans_2[:,(2,7,6)], axis=1)\n",
    "score1(y_ans,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8149364.38513082, 8252011.79852651],\n",
       "       [  33830.63076103,   31516.24626187],\n",
       "       [ 735364.35664436,  751050.01596143],\n",
       "       ...,\n",
       "       [  35639.0009043 ,   34077.87844652],\n",
       "       [  49648.18740921,   43545.79869038],\n",
       "       [  33823.76523288,   31516.24626187]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ans_2[:,(7,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (17193) into shape (10000)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-212-83f520ada9bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx_ans_2i\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[0mx_ans_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlevel_1_xall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[0mx_ans_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-212-83f520ada9bf>\u001b[0m in \u001b[0;36mlevel_1_xall\u001b[1;34m(x, y, x_test, y_test, estimators)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0my_predict_ans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0my_predict_ans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_predict_ans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_predict_ans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mx_ans_2i\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_predict_ans\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;31m#         print(score1(y_stack,y_predict),name)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_predict_ans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (17193) into shape (10000)"
     ]
    }
   ],
   "source": [
    "import regressor as reg_model\n",
    "params1 = {\n",
    "    'n_jobs' : -1,\n",
    "    'n_iter' : 20,\n",
    "    'n_iter_nt' : 2,\n",
    "    'n_components' : 25,\n",
    "    'cv' : 5,\n",
    "    'seed':42,\n",
    "    'n_features': x.shape[1],\n",
    "    'is_pca' : False,\n",
    "    'scoring' : 'neg_mean_absolute_error',\n",
    "#     'scoring' : 'neg_mean_squared_log_error',\n",
    "}\n",
    "# params1 = {\n",
    "#     'n_jobs' : 3,\n",
    "#     'n_iter' : 30,\n",
    "#     'n_iter_nt' : 3,\n",
    "#     'n_components' : 25,\n",
    "#     'cv' : 5,\n",
    "#     'seed':42,\n",
    "#     'n_features': 14,\n",
    "#     'is_pca' : False,\n",
    "#     'scoring' : 'neg_mean_absolute_error',\n",
    "# }\n",
    " \n",
    "estimators = reg_model.get_regressor(**params1)\n",
    "def level_1_xall(x,y,x_test,y_test,estimators):\n",
    "#     x_2i = np.zeros((x_stack.shape[0], len(estimators)))\n",
    "    x_ans_2i = np.zeros((x_ans.shape[0], len(estimators)))\n",
    "    for n, (name, estimator) in enumerate(estimators):\n",
    "        estimator.fit(x,y)\n",
    "#         y_predict = y_predict.reshape((y_predict.shape[0],))\n",
    "#         x_2i[:,n] = y_predict\n",
    "        y_predict_ans = estimator.predict(x_test)\n",
    "        y_predict_ans = y_predict_ans.reshape((y_predict_ans.shape[0],))\n",
    "        x_ans_2i[:,n] = y_predict_ans\n",
    "#         print(score1(y_stack,y_predict),name)\n",
    "        print(score1(y_test,y_predict_ans),name,n)\n",
    "    return x_ans_2i\n",
    "\n",
    "x_ans_2 = level_1_xall(x,y,x_test,y_test,estimators)\n",
    "x_ans_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235115.30072818775"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.mean(x_ans_2[:,(6,1)], axis=1)\n",
    "a = x_ans_2[:,1]*0.9+x_ans_2[:,6]*0.1\n",
    "# a = x_ans_2[:,2]*0.3+x_ans_2[:,3]*0.4+x_ans_2[:,7]*0.3\n",
    "# a = np.mean(x_ans_2, axis=1)\n",
    "a[a < 0] = 0\n",
    "score1(y_ans,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68769, 20)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:30:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 2 pruned nodes, max_depth=7\n",
      "[16:30:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[0]\teval-rmse:2.0119\ttrain-rmse:1.96768\n",
      "Multiple eval metrics have been passed: 'train-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until train-rmse hasn't improved in 10 rounds.\n",
      "[16:30:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 78 extra nodes, 2 pruned nodes, max_depth=8\n",
      "[16:30:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[1]\teval-rmse:1.91914\ttrain-rmse:1.87486\n",
      "[16:30:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 70 extra nodes, 4 pruned nodes, max_depth=8\n",
      "[16:30:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[2]\teval-rmse:1.85291\ttrain-rmse:1.80894\n",
      "[16:30:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 86 extra nodes, 6 pruned nodes, max_depth=9\n",
      "[16:30:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[3]\teval-rmse:1.80443\ttrain-rmse:1.75932\n",
      "[16:30:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 88 extra nodes, 6 pruned nodes, max_depth=9\n",
      "[16:30:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[4]\teval-rmse:1.76859\ttrain-rmse:1.72216\n",
      "[16:30:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 94 extra nodes, 6 pruned nodes, max_depth=9\n",
      "[16:30:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[5]\teval-rmse:1.74128\ttrain-rmse:1.69407\n",
      "[16:30:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 100 extra nodes, 2 pruned nodes, max_depth=11\n",
      "[16:30:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[6]\teval-rmse:1.7212\ttrain-rmse:1.67359\n",
      "[16:30:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 106 extra nodes, 8 pruned nodes, max_depth=10\n",
      "[16:30:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[7]\teval-rmse:1.70544\ttrain-rmse:1.6562\n",
      "[16:30:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 104 extra nodes, 2 pruned nodes, max_depth=10\n",
      "[16:30:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[8]\teval-rmse:1.695\ttrain-rmse:1.64342\n",
      "[16:30:46] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 98 extra nodes, 4 pruned nodes, max_depth=9\n",
      "[16:30:46] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[9]\teval-rmse:1.68599\ttrain-rmse:1.63249\n",
      "[16:30:46] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 108 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[16:30:46] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[10]\teval-rmse:1.67891\ttrain-rmse:1.62385\n",
      "[16:30:46] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 118 extra nodes, 6 pruned nodes, max_depth=10\n",
      "[16:30:46] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[11]\teval-rmse:1.67325\ttrain-rmse:1.61596\n",
      "[16:30:46] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 120 extra nodes, 10 pruned nodes, max_depth=11\n",
      "[16:30:46] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[12]\teval-rmse:1.66759\ttrain-rmse:1.60925\n",
      "[16:30:47] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 114 extra nodes, 2 pruned nodes, max_depth=13\n",
      "[16:30:47] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[13]\teval-rmse:1.66505\ttrain-rmse:1.60328\n",
      "[16:30:47] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 96 extra nodes, 6 pruned nodes, max_depth=12\n",
      "[16:30:47] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[14]\teval-rmse:1.66162\ttrain-rmse:1.59889\n",
      "[16:30:48] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 102 extra nodes, 8 pruned nodes, max_depth=9\n",
      "[16:30:48] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[15]\teval-rmse:1.65909\ttrain-rmse:1.59465\n",
      "[16:30:48] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 116 extra nodes, 14 pruned nodes, max_depth=12\n",
      "[16:30:48] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[16]\teval-rmse:1.65649\ttrain-rmse:1.59123\n",
      "[16:30:48] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 124 extra nodes, 4 pruned nodes, max_depth=13\n",
      "[16:30:48] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[17]\teval-rmse:1.65408\ttrain-rmse:1.58764\n",
      "[16:30:49] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 120 extra nodes, 8 pruned nodes, max_depth=11\n",
      "[16:30:49] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[18]\teval-rmse:1.6527\ttrain-rmse:1.58457\n",
      "[16:30:49] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 114 extra nodes, 6 pruned nodes, max_depth=11\n",
      "[16:30:49] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[19]\teval-rmse:1.65125\ttrain-rmse:1.58196\n",
      "[16:30:50] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 132 extra nodes, 8 pruned nodes, max_depth=13\n",
      "[16:30:50] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[20]\teval-rmse:1.65065\ttrain-rmse:1.57914\n",
      "[16:30:50] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 122 extra nodes, 8 pruned nodes, max_depth=12\n",
      "[16:30:50] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[21]\teval-rmse:1.64934\ttrain-rmse:1.57608\n",
      "[16:30:51] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 132 extra nodes, 2 pruned nodes, max_depth=12\n",
      "[16:30:51] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[22]\teval-rmse:1.64849\ttrain-rmse:1.57368\n",
      "[16:30:51] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 132 extra nodes, 16 pruned nodes, max_depth=13\n",
      "[16:30:51] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[23]\teval-rmse:1.64769\ttrain-rmse:1.57163\n",
      "[16:30:52] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 106 extra nodes, 6 pruned nodes, max_depth=11\n",
      "[16:30:52] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[24]\teval-rmse:1.64718\ttrain-rmse:1.56984\n",
      "[16:30:52] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 102 extra nodes, 6 pruned nodes, max_depth=12\n",
      "[16:30:52] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[25]\teval-rmse:1.64635\ttrain-rmse:1.56817\n",
      "[16:30:53] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 152 extra nodes, 6 pruned nodes, max_depth=14\n",
      "[16:30:53] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[26]\teval-rmse:1.64559\ttrain-rmse:1.56584\n",
      "[16:30:54] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 108 extra nodes, 2 pruned nodes, max_depth=14\n",
      "[16:30:54] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[27]\teval-rmse:1.64509\ttrain-rmse:1.56416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:30:54] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 140 extra nodes, 6 pruned nodes, max_depth=14\n",
      "[16:30:54] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[28]\teval-rmse:1.645\ttrain-rmse:1.56221\n",
      "[16:30:55] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 100 extra nodes, 2 pruned nodes, max_depth=11\n",
      "[16:30:55] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[29]\teval-rmse:1.64447\ttrain-rmse:1.56044\n",
      "[16:30:56] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 122 extra nodes, 2 pruned nodes, max_depth=12\n",
      "[16:30:56] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[30]\teval-rmse:1.64353\ttrain-rmse:1.55857\n",
      "[16:30:56] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 84 extra nodes, 12 pruned nodes, max_depth=8\n",
      "[16:30:56] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[31]\teval-rmse:1.64269\ttrain-rmse:1.55705\n",
      "[16:30:57] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 114 extra nodes, 6 pruned nodes, max_depth=10\n",
      "[16:30:57] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[32]\teval-rmse:1.64209\ttrain-rmse:1.5557\n",
      "[16:30:58] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 120 extra nodes, 14 pruned nodes, max_depth=12\n",
      "[16:30:58] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[33]\teval-rmse:1.64218\ttrain-rmse:1.55429\n",
      "[16:30:58] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 126 extra nodes, 4 pruned nodes, max_depth=12\n",
      "[16:30:58] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[34]\teval-rmse:1.64163\ttrain-rmse:1.55246\n",
      "[16:30:59] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 122 extra nodes, 10 pruned nodes, max_depth=11\n",
      "[16:30:59] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[35]\teval-rmse:1.64133\ttrain-rmse:1.55112\n",
      "[16:31:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 116 extra nodes, 2 pruned nodes, max_depth=11\n",
      "[16:31:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[36]\teval-rmse:1.64078\ttrain-rmse:1.54965\n",
      "[16:31:01] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 88 extra nodes, 12 pruned nodes, max_depth=17\n",
      "[16:31:01] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[37]\teval-rmse:1.64\ttrain-rmse:1.54849\n",
      "[16:31:01] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 100 extra nodes, 0 pruned nodes, max_depth=11\n",
      "[16:31:01] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[38]\teval-rmse:1.63997\ttrain-rmse:1.54777\n",
      "[16:31:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 118 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[16:31:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[39]\teval-rmse:1.63972\ttrain-rmse:1.5465\n",
      "[16:31:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 116 extra nodes, 4 pruned nodes, max_depth=10\n",
      "[16:31:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[40]\teval-rmse:1.63975\ttrain-rmse:1.54543\n",
      "[16:31:04] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 108 extra nodes, 4 pruned nodes, max_depth=14\n",
      "[16:31:04] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[41]\teval-rmse:1.63963\ttrain-rmse:1.5445\n",
      "[16:31:04] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 96 extra nodes, 8 pruned nodes, max_depth=10\n",
      "[16:31:04] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[42]\teval-rmse:1.63929\ttrain-rmse:1.54336\n",
      "[16:31:05] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 98 extra nodes, 14 pruned nodes, max_depth=12\n",
      "[16:31:05] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[43]\teval-rmse:1.63929\ttrain-rmse:1.5423\n",
      "[16:31:06] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 106 extra nodes, 4 pruned nodes, max_depth=13\n",
      "[16:31:06] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[44]\teval-rmse:1.63931\ttrain-rmse:1.5412\n",
      "[16:31:07] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 100 extra nodes, 12 pruned nodes, max_depth=13\n",
      "[16:31:07] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[45]\teval-rmse:1.63882\ttrain-rmse:1.54024\n",
      "[16:31:07] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 114 extra nodes, 8 pruned nodes, max_depth=10\n",
      "[16:31:07] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[46]\teval-rmse:1.63874\ttrain-rmse:1.53909\n",
      "[16:31:08] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 102 extra nodes, 14 pruned nodes, max_depth=12\n",
      "[16:31:08] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[47]\teval-rmse:1.63858\ttrain-rmse:1.53802\n",
      "[16:31:09] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 128 extra nodes, 12 pruned nodes, max_depth=14\n",
      "[16:31:09] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[48]\teval-rmse:1.63834\ttrain-rmse:1.53687\n",
      "[16:31:10] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 122 extra nodes, 4 pruned nodes, max_depth=10\n",
      "[16:31:10] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[49]\teval-rmse:1.63814\ttrain-rmse:1.53551\n",
      "[16:31:11] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 102 extra nodes, 0 pruned nodes, max_depth=13\n",
      "[16:31:11] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[50]\teval-rmse:1.63801\ttrain-rmse:1.53459\n",
      "[16:31:11] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 114 extra nodes, 12 pruned nodes, max_depth=10\n",
      "[16:31:11] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[51]\teval-rmse:1.6378\ttrain-rmse:1.53359\n",
      "[16:31:12] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 126 extra nodes, 8 pruned nodes, max_depth=17\n",
      "[16:31:12] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[52]\teval-rmse:1.63777\ttrain-rmse:1.53277\n",
      "[16:31:13] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 90 extra nodes, 2 pruned nodes, max_depth=11\n",
      "[16:31:13] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[53]\teval-rmse:1.63756\ttrain-rmse:1.53173\n",
      "[16:31:14] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 102 extra nodes, 6 pruned nodes, max_depth=12\n",
      "[16:31:14] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[54]\teval-rmse:1.63748\ttrain-rmse:1.53094\n",
      "[16:31:15] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 124 extra nodes, 12 pruned nodes, max_depth=11\n",
      "[16:31:15] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[55]\teval-rmse:1.63742\ttrain-rmse:1.52976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:31:16] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 110 extra nodes, 8 pruned nodes, max_depth=14\n",
      "[16:31:16] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[56]\teval-rmse:1.6373\ttrain-rmse:1.52863\n",
      "[16:31:17] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 104 extra nodes, 2 pruned nodes, max_depth=12\n",
      "[16:31:17] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[57]\teval-rmse:1.63696\ttrain-rmse:1.52787\n",
      "[16:31:18] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 114 extra nodes, 8 pruned nodes, max_depth=12\n",
      "[16:31:18] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[58]\teval-rmse:1.63673\ttrain-rmse:1.52689\n",
      "[16:31:19] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 100 extra nodes, 12 pruned nodes, max_depth=11\n",
      "[16:31:19] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[59]\teval-rmse:1.63674\ttrain-rmse:1.52619\n",
      "[16:31:19] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 104 extra nodes, 14 pruned nodes, max_depth=11\n",
      "[16:31:19] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[60]\teval-rmse:1.63655\ttrain-rmse:1.52539\n",
      "[16:31:21] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 108 extra nodes, 8 pruned nodes, max_depth=10\n",
      "[16:31:21] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[61]\teval-rmse:1.63643\ttrain-rmse:1.52478\n",
      "[16:31:22] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 104 extra nodes, 6 pruned nodes, max_depth=12\n",
      "[16:31:22] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[62]\teval-rmse:1.63632\ttrain-rmse:1.52399\n",
      "[16:31:23] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 80 extra nodes, 18 pruned nodes, max_depth=12\n",
      "[16:31:23] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[63]\teval-rmse:1.63606\ttrain-rmse:1.52331\n",
      "[16:31:24] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 98 extra nodes, 6 pruned nodes, max_depth=9\n",
      "[16:31:24] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[64]\teval-rmse:1.63625\ttrain-rmse:1.52257\n",
      "[16:31:25] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 82 extra nodes, 4 pruned nodes, max_depth=8\n",
      "[16:31:25] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[65]\teval-rmse:1.63624\ttrain-rmse:1.52209\n",
      "[16:31:26] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 108 extra nodes, 8 pruned nodes, max_depth=11\n",
      "[16:31:26] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[66]\teval-rmse:1.63599\ttrain-rmse:1.52105\n",
      "[16:31:27] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 102 extra nodes, 6 pruned nodes, max_depth=13\n",
      "[16:31:27] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[67]\teval-rmse:1.63604\ttrain-rmse:1.52039\n",
      "[16:31:28] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 106 extra nodes, 8 pruned nodes, max_depth=10\n",
      "[16:31:28] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[68]\teval-rmse:1.63617\ttrain-rmse:1.51947\n",
      "[16:31:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 102 extra nodes, 2 pruned nodes, max_depth=15\n",
      "[16:31:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[69]\teval-rmse:1.63592\ttrain-rmse:1.51865\n",
      "[16:31:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 122 extra nodes, 4 pruned nodes, max_depth=9\n",
      "[16:31:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[70]\teval-rmse:1.63577\ttrain-rmse:1.51771\n",
      "[16:31:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 106 extra nodes, 18 pruned nodes, max_depth=11\n",
      "[16:31:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[71]\teval-rmse:1.63563\ttrain-rmse:1.51698\n",
      "[16:31:32] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 122 extra nodes, 8 pruned nodes, max_depth=10\n",
      "[16:31:32] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[72]\teval-rmse:1.63566\ttrain-rmse:1.51598\n",
      "[16:31:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 96 extra nodes, 16 pruned nodes, max_depth=11\n",
      "[16:31:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[73]\teval-rmse:1.63544\ttrain-rmse:1.51527\n",
      "[16:31:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 106 extra nodes, 10 pruned nodes, max_depth=12\n",
      "[16:31:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[74]\teval-rmse:1.63529\ttrain-rmse:1.51458\n",
      "[16:31:36] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 102 extra nodes, 6 pruned nodes, max_depth=9\n",
      "[16:31:36] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[75]\teval-rmse:1.63505\ttrain-rmse:1.51385\n",
      "[16:31:37] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 98 extra nodes, 2 pruned nodes, max_depth=11\n",
      "[16:31:37] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[76]\teval-rmse:1.63521\ttrain-rmse:1.51303\n",
      "[16:31:38] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 100 extra nodes, 10 pruned nodes, max_depth=10\n",
      "[16:31:38] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[77]\teval-rmse:1.63511\ttrain-rmse:1.5124\n",
      "[16:31:39] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 80 extra nodes, 6 pruned nodes, max_depth=11\n",
      "[16:31:39] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[78]\teval-rmse:1.63507\ttrain-rmse:1.51195\n",
      "[16:31:40] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 104 extra nodes, 10 pruned nodes, max_depth=11\n",
      "[16:31:40] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[79]\teval-rmse:1.63517\ttrain-rmse:1.51135\n",
      "[16:31:41] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 92 extra nodes, 2 pruned nodes, max_depth=13\n",
      "[16:31:41] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[80]\teval-rmse:1.63497\ttrain-rmse:1.5108\n",
      "[16:31:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 116 extra nodes, 12 pruned nodes, max_depth=10\n",
      "[16:31:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[81]\teval-rmse:1.63494\ttrain-rmse:1.50997\n",
      "[16:31:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 96 extra nodes, 12 pruned nodes, max_depth=14\n",
      "[16:31:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[82]\teval-rmse:1.63497\ttrain-rmse:1.5094\n",
      "[16:31:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 110 extra nodes, 8 pruned nodes, max_depth=13\n",
      "[16:31:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[83]\teval-rmse:1.63515\ttrain-rmse:1.50868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:31:46] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 118 extra nodes, 8 pruned nodes, max_depth=13\n",
      "[16:31:46] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[84]\teval-rmse:1.63493\ttrain-rmse:1.50794\n",
      "[16:31:47] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 94 extra nodes, 14 pruned nodes, max_depth=9\n",
      "[16:31:47] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[85]\teval-rmse:1.63483\ttrain-rmse:1.50729\n",
      "[16:31:49] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 94 extra nodes, 6 pruned nodes, max_depth=11\n",
      "[16:31:49] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[86]\teval-rmse:1.63503\ttrain-rmse:1.50673\n",
      "[16:31:50] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 96 extra nodes, 0 pruned nodes, max_depth=11\n",
      "[16:31:50] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[87]\teval-rmse:1.63527\ttrain-rmse:1.50615\n",
      "[16:31:51] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 108 extra nodes, 14 pruned nodes, max_depth=14\n",
      "[16:31:51] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[88]\teval-rmse:1.63518\ttrain-rmse:1.5055\n",
      "[16:31:52] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 100 extra nodes, 6 pruned nodes, max_depth=15\n",
      "[16:31:52] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[89]\teval-rmse:1.63491\ttrain-rmse:1.50476\n",
      "[16:31:53] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 66 extra nodes, 0 pruned nodes, max_depth=11\n",
      "[16:31:53] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[90]\teval-rmse:1.63481\ttrain-rmse:1.50428\n",
      "[16:31:55] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 104 extra nodes, 12 pruned nodes, max_depth=11\n",
      "[16:31:55] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[91]\teval-rmse:1.63439\ttrain-rmse:1.5035\n",
      "[16:31:56] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 100 extra nodes, 10 pruned nodes, max_depth=11\n",
      "[16:31:56] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[92]\teval-rmse:1.63459\ttrain-rmse:1.50286\n",
      "[16:31:57] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 100 extra nodes, 12 pruned nodes, max_depth=12\n",
      "[16:31:57] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[93]\teval-rmse:1.63468\ttrain-rmse:1.50204\n",
      "[16:31:58] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 114 extra nodes, 2 pruned nodes, max_depth=14\n",
      "[16:31:58] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[94]\teval-rmse:1.63466\ttrain-rmse:1.50143\n",
      "[16:32:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 104 extra nodes, 2 pruned nodes, max_depth=11\n",
      "[16:32:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[95]\teval-rmse:1.63472\ttrain-rmse:1.50069\n",
      "[16:32:01] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 82 extra nodes, 14 pruned nodes, max_depth=11\n",
      "[16:32:01] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[96]\teval-rmse:1.63458\ttrain-rmse:1.50011\n",
      "[16:32:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 74 extra nodes, 16 pruned nodes, max_depth=12\n",
      "[16:32:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[97]\teval-rmse:1.63424\ttrain-rmse:1.49967\n",
      "[16:32:04] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 106 extra nodes, 6 pruned nodes, max_depth=12\n",
      "[16:32:04] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[98]\teval-rmse:1.63434\ttrain-rmse:1.49909\n",
      "[16:32:05] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 74 extra nodes, 14 pruned nodes, max_depth=12\n",
      "[16:32:05] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[99]\teval-rmse:1.63436\ttrain-rmse:1.4986\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "# dsm = xgb.DMatrix(x, label=y)\n",
    "dt = xgb.DMatrix(x_test, label=y_test,feature_names=feature_name)\n",
    "dx = xgb.DMatrix(x, label=y,feature_names=feature_name)\n",
    "dx = xgb.DMatrix(x_train, label=y_train,feature_names=feature_name)\n",
    "# dt = xgb.DMatrix(x_test, label=y_test)\n",
    "# dx = xgb.DMatrix(x, label=y)\n",
    "# evallist = [(dx, 'eval'), (dx, 'train')]\n",
    "evallist = [(dt, 'eval'), (dx, 'train')]\n",
    "num_round = 100\n",
    "param = {\n",
    "'booster':'dart',\n",
    "#  'booster':'gblinear',\n",
    " 'objective': 'reg:linear',\n",
    " 'colsample_bytree': 0.9683760122352089,\n",
    " 'gamma':0.8835260600913024,\n",
    " 'learning_rate': 0.199426498504554,\n",
    " 'max_depth': 20,\n",
    " 'min_child_weight': 10.95324500379702,\n",
    " 'n_estimators': 25,\n",
    " 'scale_pos_weight': 1,\n",
    " 'seed': 42,\n",
    " 'eval_metric': 'rmse',\n",
    " 'lambda': 150,\n",
    " 'alpha': 200,\n",
    "#  'rate_drop':0.50292864879127905,\n",
    " 'tree_method':'exact',\n",
    " 'normalize_type':'forest',\n",
    " 'subsample': 0.9035691355661921,\n",
    "    'nthread':-1,\n",
    "}\n",
    "evals_result = {}\n",
    "bst = xgb.train(param, dx, num_round, evallist, evals_result=evals_result,early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EBM_IT_01\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:704: UserWarning: silent keyword has been found in `params` and will be ignored. Please use silent argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[20]\tvalid_0's rmse: 1.80591\n",
      "[40]\tvalid_0's rmse: 1.7206\n",
      "[60]\tvalid_0's rmse: 1.68474\n",
      "[80]\tvalid_0's rmse: 1.66809\n",
      "[100]\tvalid_0's rmse: 1.66114\n",
      "[120]\tvalid_0's rmse: 1.65679\n",
      "[140]\tvalid_0's rmse: 1.65216\n",
      "[160]\tvalid_0's rmse: 1.64953\n",
      "[180]\tvalid_0's rmse: 1.64849\n",
      "[200]\tvalid_0's rmse: 1.64763\n",
      "[220]\tvalid_0's rmse: 1.64692\n",
      "[240]\tvalid_0's rmse: 1.64666\n",
      "[260]\tvalid_0's rmse: 1.64644\n",
      "[280]\tvalid_0's rmse: 1.64529\n",
      "[300]\tvalid_0's rmse: 1.6452\n",
      "[320]\tvalid_0's rmse: 1.64472\n",
      "[340]\tvalid_0's rmse: 1.64472\n",
      "[360]\tvalid_0's rmse: 1.64507\n",
      "[380]\tvalid_0's rmse: 1.64466\n",
      "[400]\tvalid_0's rmse: 1.64495\n",
      "[420]\tvalid_0's rmse: 1.64591\n",
      "Early stopping, best iteration is:\n",
      "[333]\tvalid_0's rmse: 1.64446\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb \n",
    "lgb_params = {\"objective\" : \"regression\", \"metric\" : \"rmse\",\n",
    "              \"num_leaves\" : 50, \"learning_rate\" : 0.032, \n",
    "              \"bagging_fraction\" : 0.75, \"feature_fraction\" : 0.8, \"bagging_frequency\" : 9}\n",
    "    \n",
    "lgb_train = lgb.Dataset(x_train, label=y_train[:,0])\n",
    "lgb_val = lgb.Dataset(x_test, label=y_test[:,0])\n",
    "\n",
    "model = lgb.train(lgb_params, lgb_train, 1000, valid_sets=[lgb_val], early_stopping_rounds=100, verbose_eval=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_x = lgb.Dataset(x, label=y[:,0])\n",
    "model = lgb.train(lgb_params, lgb_x, 1000, verbose_eval=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_clf = model.predict(x_ans, num_iteration=model.best_iteration)\n",
    "y_clf[y_clf < 0] = 0\n",
    "sv = y_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6298336535773157\n",
      "1.7726936238622255\n",
      "1.8775413066230322\n"
     ]
    }
   ],
   "source": [
    "treee = bst.best_ntree_limit\n",
    "# treee = 20\n",
    "dt = xgb.DMatrix(x_test, label=y_test,feature_names=feature_name)\n",
    "y_pre = np.array(bst.predict(dt, ntree_limit=treee))\n",
    "y_pre[y_pre<=0] = 0\n",
    "# yc_pre = bst.predict(dt)\n",
    "# yc_pre = np.log(yc_pre+1)\n",
    "print(score1(y_test,y_pre))\n",
    "print(score1(y_test,y_pre*yc_pre))\n",
    "print(score1(y_test,y_pre*ycx_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(726591,)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dans = xgb.DMatrix(x_ans,feature_names=feature_name)\n",
    "treee = bst.best_ntree_limit\n",
    "sv = np.array(bst.predict(dans, ntree_limit=treee))\n",
    "sv[sv < 0] = 0\n",
    "# sv.shape\n",
    "# np.savetxt(\"google_data/sub1.csv\", sv, delimiter=\",\")\n",
    "sv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_clf = sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv[sv <  0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "svs = sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "svs = 0.5*sv+0.5*y_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('google_data/sample_submission.csv', delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EBM_IT_01\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "test = data_ans[['fullVisitorId']]\n",
    "test['PredictedLogRevenue'] = svs\n",
    "sub_1 = submission[['fullVisitorId']].join(test.set_index('fullVisitorId'),on='fullVisitorId').groupby(\"fullVisitorId\").agg({\"PredictedLogRevenue\" : \"max\"})\n",
    "sub_1.fillna(0).to_csv(\"google_submit/testsetsetsetsetsetsetsetsetset222222.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0.014663\n",
       "1          0.014663\n",
       "2          0.036762\n",
       "3          0.014663\n",
       "4          0.014663\n",
       "5          0.014663\n",
       "6          4.683664\n",
       "7          0.014663\n",
       "8          0.014663\n",
       "9          0.014663\n",
       "10         0.014663\n",
       "11         0.014663\n",
       "12         0.036762\n",
       "13         0.014663\n",
       "14         0.014663\n",
       "15         0.014663\n",
       "16         0.036762\n",
       "17         0.036762\n",
       "18         0.014663\n",
       "19         0.014663\n",
       "20         0.014663\n",
       "21         0.014663\n",
       "22         0.014663\n",
       "23         4.359842\n",
       "24         0.014663\n",
       "25         0.014663\n",
       "26         1.424487\n",
       "27         0.014663\n",
       "28         0.036762\n",
       "29         0.014663\n",
       "            ...    \n",
       "617212     0.020422\n",
       "617213     0.069303\n",
       "617214     0.014663\n",
       "617215     0.014663\n",
       "617216     0.014663\n",
       "617217     0.014663\n",
       "617218     0.014663\n",
       "617219     0.036762\n",
       "617220     0.014663\n",
       "617221     0.036762\n",
       "617222     0.036762\n",
       "617223     0.014663\n",
       "617224     0.036762\n",
       "617225     0.014663\n",
       "617226     0.014663\n",
       "617227     0.036762\n",
       "617228     0.036762\n",
       "617229     0.014663\n",
       "617230     0.257967\n",
       "617231     0.014663\n",
       "617232     3.864808\n",
       "617233     0.014663\n",
       "617234     0.014663\n",
       "617235     0.036762\n",
       "617236    10.530762\n",
       "617237     0.036762\n",
       "617238     0.014663\n",
       "617239     0.036762\n",
       "617240     0.014663\n",
       "617241     0.014663\n",
       "Length: 617242, dtype: float64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv\n",
    "# y_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting KFold iterations...\n",
      "Iteration: 1   rmse: 1.6252156931921136\n",
      "Iteration: 2   rmse: 1.6145917570310366\n",
      "Iteration: 3   rmse: 1.6102511493439358\n",
      "Iteration: 4   rmse: 1.6159927022207048\n",
      "Iteration: 5   rmse: 1.6239875026594537\n",
      "\n",
      " Best score: 1.6102511493439358  Avg Score: 1.618007760889449\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm.sklearn import LGBMRegressor\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import scipy.stats as st\n",
    "from sklearn.model_selection import RandomizedSearchCV , GridSearchCV\n",
    "\n",
    "k=1\n",
    "splits=5\n",
    "avg_score=0\n",
    "\n",
    "lgb_params = {\"objective\" : \"regression\", \"metric\" : \"rmse\",\n",
    "              \"num_leaves\" : 50, \"learning_rate\" : 0.032, \n",
    "              \"bagging_fraction\" : 0.75, \"feature_fraction\" : 0.8, \"bagging_frequency\" : 9, 'use_best_model':True}\n",
    "# lgb_params = {\"objective\" : \"regression\", \"metric\" : \"rmse\",\n",
    "#               \"num_leaves\" : 10, \"learning_rate\" : 0.032, \n",
    "#               \"bagging_fraction\" : 0.75, \"feature_fraction\" : 0.8, \"bagging_frequency\" : 9, 'use_best_model':True}\n",
    "\n",
    "y_kf = np.copy(y)\n",
    "y_kf[y_kf != 0] = 1\n",
    "\n",
    "param = {\n",
    "'booster':'dart',\n",
    " 'objective': 'reg:linear',\n",
    " 'colsample_bytree': 0.9683760122352089,\n",
    " 'gamma':0.8835260600913024,\n",
    " 'learning_rate': 0.199426498504554,\n",
    " 'max_depth': 20,\n",
    " 'min_child_weight': 10.95324500379702,\n",
    " 'n_estimators': 25,\n",
    " 'scale_pos_weight': 1,\n",
    " 'seed': 42,\n",
    " 'eval_metric': 'rmse',\n",
    " 'lambda': 150,\n",
    " 'alpha': 200,\n",
    " 'tree_method':'exact',\n",
    " 'normalize_type':'forest',\n",
    " 'subsample': 0.9035691355661921,\n",
    "    'nthread':-1,\n",
    "}\n",
    "# select_feature x_stack  pd.concat([df1, df4], axis=1, sort=False)\n",
    "\n",
    "\n",
    "# x_for_train = x_stack.copy()\n",
    "# x_ans_train = x_ans_stack.copy()\n",
    "\n",
    "x_for_train = pd.concat([x, x_stack], axis=1, sort=False)\n",
    "x_ans_train = x_ans.reset_index(drop=True).join(x_ans_stack)\n",
    "# x_ans_train = pd.concat([x_ans, x_ans_stack], axis=1, sort=False)\n",
    "\n",
    "# x_for_train = x.copy() \n",
    "# x_ans_train = x_ans.copy()\n",
    "# select_feature = list(x_for_train)\n",
    "# select_feature = feature_name\n",
    "x_for_train = x_for_train[select_feature]\n",
    "x_ans_train = x_ans_train[select_feature]\n",
    "x_f = x_for_train.values\n",
    "\n",
    "skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=200)\n",
    "skf.get_n_splits(x_f)\n",
    "print('\\nStarting KFold iterations...')\n",
    "importances = pd.DataFrame()\n",
    "# x_stack = x_for_train.copy()\n",
    "# x_stack['y_lgb'] = 0\n",
    "\n",
    "for train_index,test_index in skf.split(x_f,y_kf):\n",
    "\n",
    "    model = LGBMRegressor(**lgb_params)\n",
    "#     model = LGBMRegressor(n_estimators=1500, objective=\"regression\", metric=\"rmse\", num_leaves=31, min_child_samples=100,\n",
    "#                       learning_rate=0.03, bagging_fraction=0.75, feature_fraction=0.55, bagging_frequency=6, \n",
    "#                       bagging_seed=2019, subsample=.9, colsample_bytree=.9, use_best_model=True)\n",
    "#     model = KNeighborsRegressor()\n",
    "#     model = XGBRegressor(nthread=-1)\n",
    "    \n",
    "#     model = ElasticNetCV(cv=5, random_state=0,l1_ratio=[0.05,.1,0.15,.2, .5, .9,0.95, 1],n_jobs=-1,normalize=False, positive=False)\n",
    "    estimator = RandomForestRegressor()\n",
    "    params = {\n",
    "        \"max_depth\": [1, 2, 3,5,10, None],\n",
    "        \"max_features\": st.randint(1, 27),\n",
    "    #     \"max_features\": [\"log2\"],\n",
    "        \"min_samples_split\": st.randint(2, 10),\n",
    "    #     \"min_samples_leaf\": st.randint(1, n_features),\n",
    "    #     \"bootstrap\": [True, False],\n",
    "        \"criterion\": [\"mse\"],\n",
    "        'oob_score': [True, False],\n",
    "        'random_state': [42],\n",
    "        'warm_start' : [True],\n",
    "    }\n",
    "#     model = RandomizedSearchCV(estimator, params, cv=5,n_jobs=-1, n_iter=10, scoring = 'neg_mean_squared_error')\n",
    "\n",
    "    df_X=x_f[train_index,:]\n",
    "    df_y=y[train_index]\n",
    "    val_X=x_f[test_index,:]\n",
    "    val_y=y[test_index]\n",
    "    \n",
    "    model.fit(df_X,df_y)\n",
    "    preds_x=pd.Series(model.predict(val_X))\n",
    "    acc= score1(val_y,preds_x)\n",
    "    print('Iteration:',k,'  rmse:',acc)\n",
    "#     thresholds = np.arange(0, 0.03, 0.005)\n",
    "#     for i in thresholds:\n",
    "#         test = preds_x.copy()\n",
    "#         test[test <  i] = 0\n",
    "#         print('test:',score1(val_y,test),\", i = \",i)\n",
    "\n",
    "    \n",
    "    if k==1:\n",
    "        score=acc\n",
    "        model1=model\n",
    "        preds1=pd.Series(model.predict(x_ans_train.values))\n",
    "        preds = preds1\n",
    "        sv = preds.copy()\n",
    "        \n",
    "    else:\n",
    "        preds1=pd.Series(model.predict(x_ans_train.values))\n",
    "        preds=preds+preds1\n",
    "        if score>acc:\n",
    "            score=acc\n",
    "            model1=model\n",
    "            sv = preds1.copy()\n",
    "#     x_stack['y_lgb'][test_index] = preds_x\n",
    "    \n",
    "    \n",
    "            \n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df['feature'] = np.asarray(select_feature)\n",
    "    imp_df['gain'] = model.feature_importances_\n",
    "#     imp_df['gain'] = model.coef_\n",
    "    imp_df['fold'] = k\n",
    "    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n",
    "    \n",
    "    avg_score=avg_score+acc        \n",
    "    k=k+1\n",
    "print('\\n Best score:',score,' Avg Score:',avg_score/splits)\n",
    "preds=preds/splits\n",
    "# x_ans_stack = x_ans.copy()\n",
    "# x_ans_stack['y_lgb'] = 0\n",
    "# x_ans_stack['y_lgb'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ada8136e80>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x864 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "\n",
    "importances['gain_log'] = np.log1p(importances['gain'])\n",
    "mean_gain = importances[['gain', 'feature']].groupby('feature').mean()\n",
    "importances['mean_gain'] = importances['feature'].map(mean_gain['gain'])\n",
    "\n",
    "plt.figure(figsize=(8, 12))\n",
    "sns.barplot(x='mean_gain', y='feature', data=importances.sort_values('mean_gain', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75,)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model1.feature_importances_\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mode_Domain2',\n",
       " 'Mode_Month',\n",
       " 'Mode_SourceSource',\n",
       " 'Mode_city2',\n",
       " 'Mode_metro2',\n",
       " 'Mode_region2',\n",
       " 'Sum_totals.hits',\n",
       " 'Sum_totals.newVisits',\n",
       " 'Sum_totals.pageviews',\n",
       " 'Sum_totals.visits',\n",
       " 'Sum_visitNumber',\n",
       " 'ratio1',\n",
       " 'ratio2',\n",
       " 'ratio3',\n",
       " 'y_ada',\n",
       " 'y_elas',\n",
       " 'y_ext',\n",
       " 'y_lgb',\n",
       " 'y_rnf',\n",
       " 'y_xbg',\n",
       " 'y_lgb']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_feature = importances.sort_values('mean_gain', ascending=False).groupby('feature').mean()\n",
    "select_feature = select_feature[select_feature['mean_gain'] > 50]\n",
    "select_feature = select_feature.index.values.tolist() + ['y_lgb']\n",
    "# select_feature = np.hstack(select_feature.index.values,np.array(['y_lgb']))\n",
    "select_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Mode_Country2', 'Mode_Domain2', 'Mode_Month',\n",
       "       'Mode_OperatSys2.Macintosh', 'Mode_SourceSource', 'Mode_city2',\n",
       "       'Mode_device.deviceCategory.desktop', 'Mode_metro2', 'Mode_region2',\n",
       "       'Sum_totals.hits', 'Sum_totals.newVisits', 'Sum_totals.pageviews',\n",
       "       'Sum_totals.visits', 'Sum_visitNumber', 'ratio1', 'ratio2', 'ratio3'],\n",
       "      dtype='object', name='feature')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select_feature.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>gain</th>\n",
       "      <th>fold</th>\n",
       "      <th>gain_log</th>\n",
       "      <th>mean_gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sum_visitNumber</td>\n",
       "      <td>210</td>\n",
       "      <td>1</td>\n",
       "      <td>5.351858</td>\n",
       "      <td>203.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mode_geoNetwork.subContinent</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>3.871201</td>\n",
       "      <td>52.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sum_totals.bounces</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>3.178054</td>\n",
       "      <td>31.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sum_totals.hits</td>\n",
       "      <td>294</td>\n",
       "      <td>1</td>\n",
       "      <td>5.686975</td>\n",
       "      <td>304.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sum_totals.newVisits</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "      <td>4.804021</td>\n",
       "      <td>126.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sum_totals.pageviews</td>\n",
       "      <td>583</td>\n",
       "      <td>1</td>\n",
       "      <td>6.369901</td>\n",
       "      <td>557.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sum_totals.visits</td>\n",
       "      <td>130</td>\n",
       "      <td>1</td>\n",
       "      <td>4.875197</td>\n",
       "      <td>140.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mode_Month</td>\n",
       "      <td>436</td>\n",
       "      <td>1</td>\n",
       "      <td>6.079933</td>\n",
       "      <td>430.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mode_browser2</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>3.465736</td>\n",
       "      <td>29.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mode_city2</td>\n",
       "      <td>301</td>\n",
       "      <td>1</td>\n",
       "      <td>5.710427</td>\n",
       "      <td>330.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mode_Country2</td>\n",
       "      <td>153</td>\n",
       "      <td>1</td>\n",
       "      <td>5.036953</td>\n",
       "      <td>160.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Mode_metro2</td>\n",
       "      <td>114</td>\n",
       "      <td>1</td>\n",
       "      <td>4.744932</td>\n",
       "      <td>137.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mode_Domain2</td>\n",
       "      <td>162</td>\n",
       "      <td>1</td>\n",
       "      <td>5.093750</td>\n",
       "      <td>157.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Mode_region2</td>\n",
       "      <td>175</td>\n",
       "      <td>1</td>\n",
       "      <td>5.170484</td>\n",
       "      <td>156.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Mode_Keyword</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>3.663562</td>\n",
       "      <td>37.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Mode_SourceSource</td>\n",
       "      <td>248</td>\n",
       "      <td>1</td>\n",
       "      <td>5.517453</td>\n",
       "      <td>241.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ratio1</td>\n",
       "      <td>749</td>\n",
       "      <td>1</td>\n",
       "      <td>6.620073</td>\n",
       "      <td>717.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ratio2</td>\n",
       "      <td>525</td>\n",
       "      <td>1</td>\n",
       "      <td>6.265301</td>\n",
       "      <td>521.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ratio3</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>4.795791</td>\n",
       "      <td>114.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Mode_channelGrouping.(Other)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Mode_channelGrouping.Affiliates</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Mode_channelGrouping.Direct</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>28.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mode_channelGrouping.Display</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Mode_channelGrouping.Organic Search</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Mode_channelGrouping.Paid Search</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Mode_channelGrouping.Referral</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>9.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Mode_channelGrouping.Social</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Mode_channelGrouping.none</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Mode_device.deviceCategory.desktop</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>4.615121</td>\n",
       "      <td>86.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Mode_device.deviceCategory.mobile</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>2.890372</td>\n",
       "      <td>8.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Mode_geoNetwork.continent.Europe</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Mode_geoNetwork.continent.Oceania</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Mode_geoNetwork.continent.none</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Mode_OperatSys2.Android</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>10.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Mode_OperatSys2.Chrome OS</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>3.367296</td>\n",
       "      <td>29.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Mode_OperatSys2.Linux</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>25.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Mode_OperatSys2.Macintosh</td>\n",
       "      <td>122</td>\n",
       "      <td>4</td>\n",
       "      <td>4.812184</td>\n",
       "      <td>99.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Mode_OperatSys2.OtherOper</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Mode_OperatSys2.Windows</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>2.772589</td>\n",
       "      <td>16.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Mode_OperatSys2.Windows Phone</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Mode_OperatSys2.iOS</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Mode_OperatSys2.none</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Mode_campaign.(not set)</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Mode_campaign.AW - Accessories</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Mode_campaign.AW - Dynamic Search Ads Whole Site</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Mode_campaign.Data Share Promo</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Mode_campaign.OtherCamp</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Mode_campaign.Retail (DO NOT EDIT owners nopha...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Mode_campaign.none</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Mode_campaign.test-liyuhz</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Mode_TrueFalse.True</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>3.401197</td>\n",
       "      <td>27.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Mode_TrueFalse.none</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Mode_SurceMed.(none)</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Mode_SurceMed.(not set)</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Mode_SurceMed.affiliate</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Mode_SurceMed.cpc</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Mode_SurceMed.cpm</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Mode_SurceMed.none</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Mode_SurceMed.organic</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Mode_SurceMed.referral</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>9.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>276 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              feature  gain  fold  gain_log  \\\n",
       "0                                     Sum_visitNumber   210     1  5.351858   \n",
       "1                        Mode_geoNetwork.subContinent    47     1  3.871201   \n",
       "2                                  Sum_totals.bounces    23     1  3.178054   \n",
       "3                                     Sum_totals.hits   294     1  5.686975   \n",
       "4                                Sum_totals.newVisits   121     1  4.804021   \n",
       "5                                Sum_totals.pageviews   583     1  6.369901   \n",
       "6                                   Sum_totals.visits   130     1  4.875197   \n",
       "7                                          Mode_Month   436     1  6.079933   \n",
       "8                                       Mode_browser2    31     1  3.465736   \n",
       "9                                          Mode_city2   301     1  5.710427   \n",
       "10                                      Mode_Country2   153     1  5.036953   \n",
       "11                                        Mode_metro2   114     1  4.744932   \n",
       "12                                       Mode_Domain2   162     1  5.093750   \n",
       "13                                       Mode_region2   175     1  5.170484   \n",
       "14                                       Mode_Keyword    38     1  3.663562   \n",
       "15                                  Mode_SourceSource   248     1  5.517453   \n",
       "16                                             ratio1   749     1  6.620073   \n",
       "17                                             ratio2   525     1  6.265301   \n",
       "18                                             ratio3   120     1  4.795791   \n",
       "19                       Mode_channelGrouping.(Other)     0     1  0.000000   \n",
       "20                    Mode_channelGrouping.Affiliates     0     1  0.000000   \n",
       "21                        Mode_channelGrouping.Direct    16     1  2.833213   \n",
       "22                       Mode_channelGrouping.Display    13     1  2.639057   \n",
       "23                Mode_channelGrouping.Organic Search     1     1  0.693147   \n",
       "24                   Mode_channelGrouping.Paid Search     1     1  0.693147   \n",
       "25                      Mode_channelGrouping.Referral     5     1  1.791759   \n",
       "26                        Mode_channelGrouping.Social     1     1  0.693147   \n",
       "27                          Mode_channelGrouping.none     0     1  0.000000   \n",
       "28                 Mode_device.deviceCategory.desktop   100     1  4.615121   \n",
       "29                  Mode_device.deviceCategory.mobile    17     1  2.890372   \n",
       "..                                                ...   ...   ...       ...   \n",
       "39                   Mode_geoNetwork.continent.Europe     1     4  0.693147   \n",
       "40                  Mode_geoNetwork.continent.Oceania     0     4  0.000000   \n",
       "41                     Mode_geoNetwork.continent.none     0     4  0.000000   \n",
       "42                            Mode_OperatSys2.Android    13     4  2.639057   \n",
       "43                          Mode_OperatSys2.Chrome OS    28     4  3.367296   \n",
       "44                              Mode_OperatSys2.Linux    24     4  3.218876   \n",
       "45                          Mode_OperatSys2.Macintosh   122     4  4.812184   \n",
       "46                          Mode_OperatSys2.OtherOper     0     4  0.000000   \n",
       "47                            Mode_OperatSys2.Windows    15     4  2.772589   \n",
       "48                      Mode_OperatSys2.Windows Phone     0     4  0.000000   \n",
       "49                                Mode_OperatSys2.iOS     2     4  1.098612   \n",
       "50                               Mode_OperatSys2.none     0     4  0.000000   \n",
       "51                            Mode_campaign.(not set)     1     4  0.693147   \n",
       "52                     Mode_campaign.AW - Accessories     3     4  1.386294   \n",
       "53   Mode_campaign.AW - Dynamic Search Ads Whole Site     0     4  0.000000   \n",
       "54                     Mode_campaign.Data Share Promo     0     4  0.000000   \n",
       "55                            Mode_campaign.OtherCamp     0     4  0.000000   \n",
       "56  Mode_campaign.Retail (DO NOT EDIT owners nopha...     0     4  0.000000   \n",
       "57                                 Mode_campaign.none     0     4  0.000000   \n",
       "58                          Mode_campaign.test-liyuhz     0     4  0.000000   \n",
       "59                                Mode_TrueFalse.True    29     4  3.401197   \n",
       "60                                Mode_TrueFalse.none     2     4  1.098612   \n",
       "61                               Mode_SurceMed.(none)     9     4  2.302585   \n",
       "62                            Mode_SurceMed.(not set)     0     4  0.000000   \n",
       "63                            Mode_SurceMed.affiliate     0     4  0.000000   \n",
       "64                                  Mode_SurceMed.cpc     7     4  2.079442   \n",
       "65                                  Mode_SurceMed.cpm     1     4  0.693147   \n",
       "66                                 Mode_SurceMed.none     0     4  0.000000   \n",
       "67                              Mode_SurceMed.organic    11     4  2.484907   \n",
       "68                             Mode_SurceMed.referral    12     4  2.564949   \n",
       "\n",
       "    mean_gain  \n",
       "0      203.25  \n",
       "1       52.00  \n",
       "2       31.00  \n",
       "3      304.50  \n",
       "4      126.50  \n",
       "5      557.25  \n",
       "6      140.25  \n",
       "7      430.00  \n",
       "8       29.75  \n",
       "9      330.50  \n",
       "10     160.00  \n",
       "11     137.50  \n",
       "12     157.75  \n",
       "13     156.75  \n",
       "14      37.75  \n",
       "15     241.00  \n",
       "16     717.75  \n",
       "17     521.00  \n",
       "18     114.75  \n",
       "19       0.00  \n",
       "20       0.00  \n",
       "21      28.50  \n",
       "22      10.00  \n",
       "23       5.00  \n",
       "24       1.50  \n",
       "25       9.50  \n",
       "26       0.75  \n",
       "27       0.00  \n",
       "28      86.50  \n",
       "29       8.50  \n",
       "..        ...  \n",
       "39       0.25  \n",
       "40       0.00  \n",
       "41       0.00  \n",
       "42      10.50  \n",
       "43      29.50  \n",
       "44      25.25  \n",
       "45      99.50  \n",
       "46       0.00  \n",
       "47      16.75  \n",
       "48       0.00  \n",
       "49       2.25  \n",
       "50       0.00  \n",
       "51       2.25  \n",
       "52       1.50  \n",
       "53       1.25  \n",
       "54       0.00  \n",
       "55       0.00  \n",
       "56       0.00  \n",
       "57       0.00  \n",
       "58       0.00  \n",
       "59      27.00  \n",
       "60       3.50  \n",
       "61       7.00  \n",
       "62       0.00  \n",
       "63       0.00  \n",
       "64       6.00  \n",
       "65       2.00  \n",
       "66       0.00  \n",
       "67       6.00  \n",
       "68       9.25  \n",
       "\n",
       "[276 rows x 5 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-41-a36c5852260c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-41-a36c5852260c>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    importances.sort_values('mean_gain', ascending=False))\u001b[0m\n\u001b[1;37m                                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "importances.sort_values('mean_gain', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>gain</th>\n",
       "      <th>fold</th>\n",
       "      <th>gain_log</th>\n",
       "      <th>mean_gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sum_visitNumber</td>\n",
       "      <td>210</td>\n",
       "      <td>1</td>\n",
       "      <td>5.351858</td>\n",
       "      <td>203.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mode_geoNetwork.subContinent</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>3.871201</td>\n",
       "      <td>52.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sum_totals.bounces</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>3.178054</td>\n",
       "      <td>31.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sum_totals.hits</td>\n",
       "      <td>294</td>\n",
       "      <td>1</td>\n",
       "      <td>5.686975</td>\n",
       "      <td>304.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sum_totals.newVisits</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "      <td>4.804021</td>\n",
       "      <td>126.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sum_totals.pageviews</td>\n",
       "      <td>583</td>\n",
       "      <td>1</td>\n",
       "      <td>6.369901</td>\n",
       "      <td>557.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sum_totals.visits</td>\n",
       "      <td>130</td>\n",
       "      <td>1</td>\n",
       "      <td>4.875197</td>\n",
       "      <td>140.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mode_Month</td>\n",
       "      <td>436</td>\n",
       "      <td>1</td>\n",
       "      <td>6.079933</td>\n",
       "      <td>430.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mode_city2</td>\n",
       "      <td>301</td>\n",
       "      <td>1</td>\n",
       "      <td>5.710427</td>\n",
       "      <td>330.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mode_Country2</td>\n",
       "      <td>153</td>\n",
       "      <td>1</td>\n",
       "      <td>5.036953</td>\n",
       "      <td>160.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Mode_metro2</td>\n",
       "      <td>114</td>\n",
       "      <td>1</td>\n",
       "      <td>4.744932</td>\n",
       "      <td>137.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mode_Domain2</td>\n",
       "      <td>162</td>\n",
       "      <td>1</td>\n",
       "      <td>5.093750</td>\n",
       "      <td>157.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Mode_region2</td>\n",
       "      <td>175</td>\n",
       "      <td>1</td>\n",
       "      <td>5.170484</td>\n",
       "      <td>156.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Mode_Keyword</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>3.663562</td>\n",
       "      <td>37.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Mode_SourceSource</td>\n",
       "      <td>248</td>\n",
       "      <td>1</td>\n",
       "      <td>5.517453</td>\n",
       "      <td>241.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ratio1</td>\n",
       "      <td>749</td>\n",
       "      <td>1</td>\n",
       "      <td>6.620073</td>\n",
       "      <td>717.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ratio2</td>\n",
       "      <td>525</td>\n",
       "      <td>1</td>\n",
       "      <td>6.265301</td>\n",
       "      <td>521.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ratio3</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>4.795791</td>\n",
       "      <td>114.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Mode_device.deviceCategory.desktop</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>4.615121</td>\n",
       "      <td>86.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Mode_device.isMobile.False</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>3.332205</td>\n",
       "      <td>34.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Mode_OperatSys2.Macintosh</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>4.624973</td>\n",
       "      <td>99.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sum_visitNumber</td>\n",
       "      <td>215</td>\n",
       "      <td>2</td>\n",
       "      <td>5.375278</td>\n",
       "      <td>203.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mode_geoNetwork.subContinent</td>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>3.970292</td>\n",
       "      <td>52.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sum_totals.bounces</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>3.526361</td>\n",
       "      <td>31.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sum_totals.hits</td>\n",
       "      <td>294</td>\n",
       "      <td>2</td>\n",
       "      <td>5.686975</td>\n",
       "      <td>304.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sum_totals.newVisits</td>\n",
       "      <td>125</td>\n",
       "      <td>2</td>\n",
       "      <td>4.836282</td>\n",
       "      <td>126.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sum_totals.pageviews</td>\n",
       "      <td>552</td>\n",
       "      <td>2</td>\n",
       "      <td>6.315358</td>\n",
       "      <td>557.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sum_totals.visits</td>\n",
       "      <td>168</td>\n",
       "      <td>2</td>\n",
       "      <td>5.129899</td>\n",
       "      <td>140.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mode_Month</td>\n",
       "      <td>439</td>\n",
       "      <td>2</td>\n",
       "      <td>6.086775</td>\n",
       "      <td>430.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mode_city2</td>\n",
       "      <td>342</td>\n",
       "      <td>2</td>\n",
       "      <td>5.837730</td>\n",
       "      <td>330.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Mode_region2</td>\n",
       "      <td>108</td>\n",
       "      <td>3</td>\n",
       "      <td>4.691348</td>\n",
       "      <td>156.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Mode_Keyword</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>3.737670</td>\n",
       "      <td>37.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Mode_SourceSource</td>\n",
       "      <td>231</td>\n",
       "      <td>3</td>\n",
       "      <td>5.446737</td>\n",
       "      <td>241.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ratio1</td>\n",
       "      <td>749</td>\n",
       "      <td>3</td>\n",
       "      <td>6.620073</td>\n",
       "      <td>717.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ratio2</td>\n",
       "      <td>510</td>\n",
       "      <td>3</td>\n",
       "      <td>6.236370</td>\n",
       "      <td>521.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ratio3</td>\n",
       "      <td>109</td>\n",
       "      <td>3</td>\n",
       "      <td>4.700480</td>\n",
       "      <td>114.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Mode_device.deviceCategory.desktop</td>\n",
       "      <td>78</td>\n",
       "      <td>3</td>\n",
       "      <td>4.369448</td>\n",
       "      <td>86.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Mode_device.isMobile.False</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>3.737670</td>\n",
       "      <td>34.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Mode_OperatSys2.Macintosh</td>\n",
       "      <td>78</td>\n",
       "      <td>3</td>\n",
       "      <td>4.369448</td>\n",
       "      <td>99.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sum_visitNumber</td>\n",
       "      <td>189</td>\n",
       "      <td>4</td>\n",
       "      <td>5.247024</td>\n",
       "      <td>203.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mode_geoNetwork.subContinent</td>\n",
       "      <td>63</td>\n",
       "      <td>4</td>\n",
       "      <td>4.158883</td>\n",
       "      <td>52.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sum_totals.bounces</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>3.433987</td>\n",
       "      <td>31.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sum_totals.hits</td>\n",
       "      <td>283</td>\n",
       "      <td>4</td>\n",
       "      <td>5.648974</td>\n",
       "      <td>304.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sum_totals.newVisits</td>\n",
       "      <td>127</td>\n",
       "      <td>4</td>\n",
       "      <td>4.852030</td>\n",
       "      <td>126.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sum_totals.pageviews</td>\n",
       "      <td>549</td>\n",
       "      <td>4</td>\n",
       "      <td>6.309918</td>\n",
       "      <td>557.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sum_totals.visits</td>\n",
       "      <td>133</td>\n",
       "      <td>4</td>\n",
       "      <td>4.897840</td>\n",
       "      <td>140.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mode_Month</td>\n",
       "      <td>455</td>\n",
       "      <td>4</td>\n",
       "      <td>6.122493</td>\n",
       "      <td>430.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mode_city2</td>\n",
       "      <td>313</td>\n",
       "      <td>4</td>\n",
       "      <td>5.749393</td>\n",
       "      <td>330.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mode_Country2</td>\n",
       "      <td>160</td>\n",
       "      <td>4</td>\n",
       "      <td>5.081404</td>\n",
       "      <td>160.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Mode_metro2</td>\n",
       "      <td>146</td>\n",
       "      <td>4</td>\n",
       "      <td>4.990433</td>\n",
       "      <td>137.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mode_Domain2</td>\n",
       "      <td>150</td>\n",
       "      <td>4</td>\n",
       "      <td>5.017280</td>\n",
       "      <td>157.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Mode_region2</td>\n",
       "      <td>177</td>\n",
       "      <td>4</td>\n",
       "      <td>5.181784</td>\n",
       "      <td>156.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Mode_Keyword</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>3.637586</td>\n",
       "      <td>37.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Mode_SourceSource</td>\n",
       "      <td>245</td>\n",
       "      <td>4</td>\n",
       "      <td>5.505332</td>\n",
       "      <td>241.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ratio1</td>\n",
       "      <td>663</td>\n",
       "      <td>4</td>\n",
       "      <td>6.498282</td>\n",
       "      <td>717.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ratio2</td>\n",
       "      <td>547</td>\n",
       "      <td>4</td>\n",
       "      <td>6.306275</td>\n",
       "      <td>521.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ratio3</td>\n",
       "      <td>117</td>\n",
       "      <td>4</td>\n",
       "      <td>4.770685</td>\n",
       "      <td>114.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Mode_device.deviceCategory.desktop</td>\n",
       "      <td>80</td>\n",
       "      <td>4</td>\n",
       "      <td>4.394449</td>\n",
       "      <td>86.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Mode_device.isMobile.False</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>3.610918</td>\n",
       "      <td>34.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Mode_OperatSys2.Macintosh</td>\n",
       "      <td>122</td>\n",
       "      <td>4</td>\n",
       "      <td>4.812184</td>\n",
       "      <td>99.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               feature  gain  fold  gain_log  mean_gain\n",
       "0                      Sum_visitNumber   210     1  5.351858     203.25\n",
       "1         Mode_geoNetwork.subContinent    47     1  3.871201      52.00\n",
       "2                   Sum_totals.bounces    23     1  3.178054      31.00\n",
       "3                      Sum_totals.hits   294     1  5.686975     304.50\n",
       "4                 Sum_totals.newVisits   121     1  4.804021     126.50\n",
       "5                 Sum_totals.pageviews   583     1  6.369901     557.25\n",
       "6                    Sum_totals.visits   130     1  4.875197     140.25\n",
       "7                           Mode_Month   436     1  6.079933     430.00\n",
       "9                           Mode_city2   301     1  5.710427     330.50\n",
       "10                       Mode_Country2   153     1  5.036953     160.00\n",
       "11                         Mode_metro2   114     1  4.744932     137.50\n",
       "12                        Mode_Domain2   162     1  5.093750     157.75\n",
       "13                        Mode_region2   175     1  5.170484     156.75\n",
       "14                        Mode_Keyword    38     1  3.663562      37.75\n",
       "15                   Mode_SourceSource   248     1  5.517453     241.00\n",
       "16                              ratio1   749     1  6.620073     717.75\n",
       "17                              ratio2   525     1  6.265301     521.00\n",
       "18                              ratio3   120     1  4.795791     114.75\n",
       "28  Mode_device.deviceCategory.desktop   100     1  4.615121      86.50\n",
       "32          Mode_device.isMobile.False    27     1  3.332205      34.75\n",
       "45           Mode_OperatSys2.Macintosh   101     1  4.624973      99.50\n",
       "0                      Sum_visitNumber   215     2  5.375278     203.25\n",
       "1         Mode_geoNetwork.subContinent    52     2  3.970292      52.00\n",
       "2                   Sum_totals.bounces    33     2  3.526361      31.00\n",
       "3                      Sum_totals.hits   294     2  5.686975     304.50\n",
       "4                 Sum_totals.newVisits   125     2  4.836282     126.50\n",
       "5                 Sum_totals.pageviews   552     2  6.315358     557.25\n",
       "6                    Sum_totals.visits   168     2  5.129899     140.25\n",
       "7                           Mode_Month   439     2  6.086775     430.00\n",
       "9                           Mode_city2   342     2  5.837730     330.50\n",
       "..                                 ...   ...   ...       ...        ...\n",
       "13                        Mode_region2   108     3  4.691348     156.75\n",
       "14                        Mode_Keyword    41     3  3.737670      37.75\n",
       "15                   Mode_SourceSource   231     3  5.446737     241.00\n",
       "16                              ratio1   749     3  6.620073     717.75\n",
       "17                              ratio2   510     3  6.236370     521.00\n",
       "18                              ratio3   109     3  4.700480     114.75\n",
       "28  Mode_device.deviceCategory.desktop    78     3  4.369448      86.50\n",
       "32          Mode_device.isMobile.False    41     3  3.737670      34.75\n",
       "45           Mode_OperatSys2.Macintosh    78     3  4.369448      99.50\n",
       "0                      Sum_visitNumber   189     4  5.247024     203.25\n",
       "1         Mode_geoNetwork.subContinent    63     4  4.158883      52.00\n",
       "2                   Sum_totals.bounces    30     4  3.433987      31.00\n",
       "3                      Sum_totals.hits   283     4  5.648974     304.50\n",
       "4                 Sum_totals.newVisits   127     4  4.852030     126.50\n",
       "5                 Sum_totals.pageviews   549     4  6.309918     557.25\n",
       "6                    Sum_totals.visits   133     4  4.897840     140.25\n",
       "7                           Mode_Month   455     4  6.122493     430.00\n",
       "9                           Mode_city2   313     4  5.749393     330.50\n",
       "10                       Mode_Country2   160     4  5.081404     160.00\n",
       "11                         Mode_metro2   146     4  4.990433     137.50\n",
       "12                        Mode_Domain2   150     4  5.017280     157.75\n",
       "13                        Mode_region2   177     4  5.181784     156.75\n",
       "14                        Mode_Keyword    37     4  3.637586      37.75\n",
       "15                   Mode_SourceSource   245     4  5.505332     241.00\n",
       "16                              ratio1   663     4  6.498282     717.75\n",
       "17                              ratio2   547     4  6.306275     521.00\n",
       "18                              ratio3   117     4  4.770685     114.75\n",
       "28  Mode_device.deviceCategory.desktop    80     4  4.394449      86.50\n",
       "32          Mode_device.isMobile.False    36     4  3.610918      34.75\n",
       "45           Mode_OperatSys2.Macintosh   122     4  4.812184      99.50\n",
       "\n",
       "[84 rows x 5 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances.loc[importances['mean_gain'] > 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([34, 36, 31, 30, 29, 38, 27, 26, 39, 40, 41, 46, 20, 19, 48, 50, 35,\n",
       "       53, 54, 55, 56, 57, 58, 51, 66, 62, 63, 67, 52, 64, 65, 33, 49, 24,\n",
       "       61, 60, 68, 22, 23, 37, 42, 25, 44, 47, 59,  8,  2, 21, 43, 32, 14,\n",
       "        1, 28, 45, 18,  4, 11, 12, 10, 13,  6,  0, 15,  3,  9,  7, 17,  5,\n",
       "       16], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sum_visitNumber'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(a >= 40)[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([183,  50,  20, 279, 125, 555, 145, 437,  28, 334, 148, 114, 158,\n",
       "       173,  31, 261, 740, 545, 103,   0,   0,  28,  15,   7,   1,   6,\n",
       "         2,   0,  96,  21,   0,   1,  25,   2,   0,   0,   0,  11,   0,\n",
       "         0,   0,   0,   7,  30,  33, 113,   0,  15,   0,   0,   0,   1,\n",
       "         0,   0,   0,   0,   0,   0,   0,  20,   3,   6,   0,   0,   8,\n",
       "         1,   0,   2,  17])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "# a = clf.coef_ \n",
    "# a = clf.best_estimator_.feature_importances_\n",
    "b = model1.feature_importances_\n",
    "sort = b.argsort()[-10:][::-1]\n",
    "a= b[sort]\n",
    "\n",
    "fname = np.asarray(feature_name)[sort]\n",
    "# a = a[:15]\n",
    "pyplot.bar(range(len(a)), a,tick_label =fname,width =0.8)\n",
    "pyplot.axis('tight')\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ratio1', 'Sum_totals.pageviews', 'ratio2', 'Mode_Month',\n",
       "       'Mode_city2', 'Sum_totals.hits', 'Mode_SourceSource',\n",
       "       'Sum_visitNumber', 'Mode_Domain2', 'Mode_Country2'], dtype='<U61')"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_select = np.argwhere(a >= 40)[:,0]\n",
    "x = x[:,index_select]\n",
    "x_ans = x_ans[:,index_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1dd21cb76a0>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(bst,max_num_features =20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "yc = np.copy(y)\n",
    "yc[yc != 0] = 1\n",
    "\n",
    "yc_train = np.copy(y_train)\n",
    "yc_train[yc_train != 0] = 1\n",
    "\n",
    "yc_test = np.copy(y_test)\n",
    "yc_test[yc_test != 0] = 1\n",
    "\n",
    "# yc_ans = np.copy(y_ans)\n",
    "# yc_ans[yc_ans != 0] = 1\n",
    "# yc_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:46:24] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[0]\teval-auc:0.938736\ttrain-auc:0.936661\n",
      "Multiple eval metrics have been passed: 'train-auc' will be used for early stopping.\n",
      "\n",
      "Will train until train-auc hasn't improved in 10 rounds.\n",
      "[17:46:24] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[1]\teval-auc:0.967139\ttrain-auc:0.965124\n",
      "[17:46:24] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[2]\teval-auc:0.968085\ttrain-auc:0.966112\n",
      "[17:46:24] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 2 pruned nodes, max_depth=6\n",
      "[3]\teval-auc:0.968587\ttrain-auc:0.966598\n",
      "[17:46:25] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[4]\teval-auc:0.969342\ttrain-auc:0.967348\n",
      "[17:46:25] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[5]\teval-auc:0.970022\ttrain-auc:0.96801\n",
      "[17:46:25] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[6]\teval-auc:0.97078\ttrain-auc:0.968798\n",
      "[17:46:25] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[7]\teval-auc:0.976092\ttrain-auc:0.975959\n",
      "[17:46:25] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[8]\teval-auc:0.976381\ttrain-auc:0.976257\n",
      "[17:46:26] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[9]\teval-auc:0.98029\ttrain-auc:0.981844\n",
      "[17:46:26] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[10]\teval-auc:0.980495\ttrain-auc:0.98216\n",
      "[17:46:26] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11]\teval-auc:0.985039\ttrain-auc:0.985593\n",
      "[17:46:26] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[12]\teval-auc:0.985195\ttrain-auc:0.985804\n",
      "[17:46:26] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[13]\teval-auc:0.985353\ttrain-auc:0.985947\n",
      "[17:46:27] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14]\teval-auc:0.985478\ttrain-auc:0.986085\n",
      "[17:46:27] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[15]\teval-auc:0.986727\ttrain-auc:0.98775\n",
      "[17:46:27] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 2 pruned nodes, max_depth=6\n",
      "[16]\teval-auc:0.986866\ttrain-auc:0.987915\n",
      "[17:46:27] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 2 pruned nodes, max_depth=6\n",
      "[17]\teval-auc:0.98693\ttrain-auc:0.987988\n",
      "[17:46:27] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[18]\teval-auc:0.988164\ttrain-auc:0.988799\n",
      "[17:46:27] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 6 pruned nodes, max_depth=6\n",
      "[19]\teval-auc:0.988199\ttrain-auc:0.988869\n",
      "[17:46:28] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 2 pruned nodes, max_depth=6\n",
      "[20]\teval-auc:0.988252\ttrain-auc:0.988939\n",
      "[17:46:28] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 4 pruned nodes, max_depth=6\n",
      "[21]\teval-auc:0.988314\ttrain-auc:0.989043\n",
      "[17:46:28] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 4 pruned nodes, max_depth=6\n",
      "[22]\teval-auc:0.98839\ttrain-auc:0.989105\n",
      "[17:46:28] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 2 pruned nodes, max_depth=6\n",
      "[23]\teval-auc:0.98844\ttrain-auc:0.989163\n",
      "[17:46:28] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 2 pruned nodes, max_depth=6\n",
      "[24]\teval-auc:0.988489\ttrain-auc:0.989209\n",
      "[17:46:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 2 pruned nodes, max_depth=6\n",
      "[25]\teval-auc:0.988772\ttrain-auc:0.989506\n",
      "[17:46:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 2 pruned nodes, max_depth=6\n",
      "[26]\teval-auc:0.988835\ttrain-auc:0.989575\n",
      "[17:46:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 2 pruned nodes, max_depth=6\n",
      "[27]\teval-auc:0.989119\ttrain-auc:0.989839\n",
      "[17:46:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[28]\teval-auc:0.989169\ttrain-auc:0.989929\n",
      "[17:46:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 6 pruned nodes, max_depth=6\n",
      "[29]\teval-auc:0.989206\ttrain-auc:0.989969\n",
      "[17:46:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 2 pruned nodes, max_depth=6\n",
      "[30]\teval-auc:0.989227\ttrain-auc:0.990012\n",
      "[17:46:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 2 pruned nodes, max_depth=6\n",
      "[31]\teval-auc:0.989244\ttrain-auc:0.990048\n",
      "[17:46:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\n",
      "[32]\teval-auc:0.989262\ttrain-auc:0.990071\n",
      "[17:46:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\n",
      "[33]\teval-auc:0.98929\ttrain-auc:0.990121\n",
      "[17:46:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 4 pruned nodes, max_depth=6\n",
      "[34]\teval-auc:0.989311\ttrain-auc:0.990152\n",
      "[17:46:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=6\n",
      "[35]\teval-auc:0.989338\ttrain-auc:0.990175\n",
      "[17:46:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 2 pruned nodes, max_depth=6\n",
      "[36]\teval-auc:0.989364\ttrain-auc:0.990207\n",
      "[17:46:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 8 pruned nodes, max_depth=5\n",
      "[37]\teval-auc:0.989383\ttrain-auc:0.990226\n",
      "[17:46:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 6 pruned nodes, max_depth=4\n",
      "[38]\teval-auc:0.989405\ttrain-auc:0.990243\n",
      "[17:46:32] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 2 pruned nodes, max_depth=5\n",
      "[39]\teval-auc:0.989424\ttrain-auc:0.99027\n",
      "[17:46:32] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 2 pruned nodes, max_depth=6\n",
      "[40]\teval-auc:0.989431\ttrain-auc:0.990291\n",
      "[17:46:32] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 2 pruned nodes, max_depth=5\n",
      "[41]\teval-auc:0.989454\ttrain-auc:0.990314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:46:32] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 4 pruned nodes, max_depth=5\n",
      "[42]\teval-auc:0.989467\ttrain-auc:0.990334\n",
      "[17:46:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 10 pruned nodes, max_depth=5\n",
      "[43]\teval-auc:0.989473\ttrain-auc:0.99035\n",
      "[17:46:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 4 pruned nodes, max_depth=6\n",
      "[44]\teval-auc:0.989485\ttrain-auc:0.990359\n",
      "[17:46:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 8 pruned nodes, max_depth=4\n",
      "[45]\teval-auc:0.989491\ttrain-auc:0.990371\n",
      "[17:46:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[46]\teval-auc:0.989497\ttrain-auc:0.990387\n",
      "[17:46:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 8 pruned nodes, max_depth=5\n",
      "[47]\teval-auc:0.989503\ttrain-auc:0.990396\n",
      "[17:46:34] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 8 pruned nodes, max_depth=5\n",
      "[48]\teval-auc:0.989517\ttrain-auc:0.990415\n",
      "[17:46:34] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 14 pruned nodes, max_depth=4\n",
      "[49]\teval-auc:0.989522\ttrain-auc:0.990428\n",
      "[17:46:34] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 8 pruned nodes, max_depth=5\n",
      "[50]\teval-auc:0.989532\ttrain-auc:0.990441\n",
      "[17:46:34] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=4\n",
      "[51]\teval-auc:0.989544\ttrain-auc:0.99046\n",
      "[17:46:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 8 pruned nodes, max_depth=4\n",
      "[52]\teval-auc:0.989551\ttrain-auc:0.990473\n",
      "[17:46:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 6 pruned nodes, max_depth=4\n",
      "[53]\teval-auc:0.989559\ttrain-auc:0.99049\n",
      "[17:46:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 12 pruned nodes, max_depth=4\n",
      "[54]\teval-auc:0.989569\ttrain-auc:0.990497\n",
      "[17:46:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 8 pruned nodes, max_depth=5\n",
      "[55]\teval-auc:0.989574\ttrain-auc:0.990513\n",
      "[17:46:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\n",
      "[56]\teval-auc:0.989581\ttrain-auc:0.990524\n",
      "[17:46:36] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 2 pruned nodes, max_depth=6\n",
      "[57]\teval-auc:0.98959\ttrain-auc:0.990545\n",
      "[17:46:36] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 12 pruned nodes, max_depth=2\n",
      "[58]\teval-auc:0.989592\ttrain-auc:0.990548\n",
      "[17:46:36] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 2 pruned nodes, max_depth=6\n",
      "[59]\teval-auc:0.989591\ttrain-auc:0.99056\n",
      "[17:46:36] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 10 pruned nodes, max_depth=4\n",
      "[60]\teval-auc:0.989597\ttrain-auc:0.990566\n",
      "[17:46:37] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 8 pruned nodes, max_depth=4\n",
      "[61]\teval-auc:0.9896\ttrain-auc:0.990574\n",
      "[17:46:37] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 10 pruned nodes, max_depth=2\n",
      "[62]\teval-auc:0.989596\ttrain-auc:0.990576\n",
      "[17:46:37] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 4 pruned nodes, max_depth=5\n",
      "[63]\teval-auc:0.989593\ttrain-auc:0.990583\n",
      "[17:46:37] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 4 pruned nodes, max_depth=6\n",
      "[64]\teval-auc:0.989599\ttrain-auc:0.990595\n",
      "[17:46:37] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 10 pruned nodes, max_depth=3\n",
      "[65]\teval-auc:0.989609\ttrain-auc:0.9906\n",
      "[17:46:38] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 16 pruned nodes, max_depth=2\n",
      "[66]\teval-auc:0.989611\ttrain-auc:0.990605\n",
      "[17:46:38] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 6 pruned nodes, max_depth=5\n",
      "[67]\teval-auc:0.989615\ttrain-auc:0.990612\n",
      "[17:46:38] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 6 pruned nodes, max_depth=5\n",
      "[68]\teval-auc:0.98962\ttrain-auc:0.990618\n",
      "[17:46:38] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 12 pruned nodes, max_depth=5\n",
      "[69]\teval-auc:0.989626\ttrain-auc:0.990631\n",
      "[17:46:39] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 8 pruned nodes, max_depth=5\n",
      "[70]\teval-auc:0.989625\ttrain-auc:0.990636\n",
      "[17:46:39] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=4\n",
      "[71]\teval-auc:0.989628\ttrain-auc:0.990644\n",
      "[17:46:39] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 8 pruned nodes, max_depth=4\n",
      "[72]\teval-auc:0.989628\ttrain-auc:0.99065\n",
      "[17:46:39] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 8 pruned nodes, max_depth=4\n",
      "[73]\teval-auc:0.989634\ttrain-auc:0.990661\n",
      "[17:46:39] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 8 pruned nodes, max_depth=4\n",
      "[74]\teval-auc:0.98964\ttrain-auc:0.990676\n",
      "[17:46:40] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 8 pruned nodes, max_depth=6\n",
      "[75]\teval-auc:0.989635\ttrain-auc:0.990687\n",
      "[17:46:40] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 2 pruned nodes, max_depth=6\n",
      "[76]\teval-auc:0.989639\ttrain-auc:0.990704\n",
      "[17:46:40] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 12 pruned nodes, max_depth=3\n",
      "[77]\teval-auc:0.989638\ttrain-auc:0.990705\n",
      "[17:46:40] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 6 pruned nodes, max_depth=3\n",
      "[78]\teval-auc:0.989642\ttrain-auc:0.990709\n",
      "[17:46:41] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 14 pruned nodes, max_depth=5\n",
      "[79]\teval-auc:0.989641\ttrain-auc:0.990715\n",
      "[17:46:41] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 16 pruned nodes, max_depth=0\n",
      "[80]\teval-auc:0.989641\ttrain-auc:0.990715\n",
      "[17:46:41] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 12 pruned nodes, max_depth=3\n",
      "[81]\teval-auc:0.989637\ttrain-auc:0.990721\n",
      "[17:46:41] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 10 pruned nodes, max_depth=3\n",
      "[82]\teval-auc:0.989643\ttrain-auc:0.990724\n",
      "[17:46:42] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[83]\teval-auc:0.98965\ttrain-auc:0.990736\n",
      "[17:46:42] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 14 pruned nodes, max_depth=3\n",
      "[84]\teval-auc:0.989648\ttrain-auc:0.990744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:46:42] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 6 pruned nodes, max_depth=5\n",
      "[85]\teval-auc:0.989647\ttrain-auc:0.99075\n",
      "[17:46:42] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 16 pruned nodes, max_depth=0\n",
      "[86]\teval-auc:0.989647\ttrain-auc:0.99075\n",
      "[17:46:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 6 pruned nodes, max_depth=6\n",
      "[87]\teval-auc:0.989649\ttrain-auc:0.990757\n",
      "[17:46:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 6 pruned nodes, max_depth=5\n",
      "[88]\teval-auc:0.989652\ttrain-auc:0.990762\n",
      "[17:46:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 10 pruned nodes, max_depth=3\n",
      "[89]\teval-auc:0.989653\ttrain-auc:0.990765\n",
      "[17:46:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 20 pruned nodes, max_depth=3\n",
      "[90]\teval-auc:0.989658\ttrain-auc:0.990768\n",
      "[17:46:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 16 pruned nodes, max_depth=3\n",
      "[91]\teval-auc:0.98966\ttrain-auc:0.990771\n",
      "[17:46:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 10 pruned nodes, max_depth=3\n",
      "[92]\teval-auc:0.989658\ttrain-auc:0.990775\n",
      "[17:46:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 14 pruned nodes, max_depth=4\n",
      "[93]\teval-auc:0.989659\ttrain-auc:0.99078\n",
      "[17:46:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 12 pruned nodes, max_depth=4\n",
      "[94]\teval-auc:0.989658\ttrain-auc:0.990782\n",
      "[17:46:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 14 pruned nodes, max_depth=2\n",
      "[95]\teval-auc:0.989661\ttrain-auc:0.990785\n",
      "[17:46:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 14 pruned nodes, max_depth=2\n",
      "[96]\teval-auc:0.98966\ttrain-auc:0.990786\n",
      "[17:46:46] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 16 pruned nodes, max_depth=5\n",
      "[97]\teval-auc:0.989663\ttrain-auc:0.990793\n",
      "[17:46:46] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 16 pruned nodes, max_depth=3\n",
      "[98]\teval-auc:0.989666\ttrain-auc:0.990797\n",
      "[17:46:46] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 18 pruned nodes, max_depth=2\n",
      "[99]\teval-auc:0.989671\ttrain-auc:0.990799\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "############### Xgboost ####################\n",
    "############################################\n",
    "import xgboost as xgb\n",
    "# dsm = xgb.DMatrix(x, label=y)\n",
    "dt = xgb.DMatrix(x_test, label=yc_test,feature_names=feature_name)\n",
    "dx = xgb.DMatrix(x_train, label=yc_train,feature_names=feature_name)\n",
    "                  \n",
    "# dt = xgb.DMatrix(x_ans_, label=yc_ans)\n",
    "# dx = xgb.DMatrix(x_, label=yc)\n",
    "\n",
    "# dsm = xgb.DMatrix(x_sm, label=y_sm)                  \n",
    "evallist = [(dt, 'eval'), (dx, 'train')]\n",
    "\n",
    "num_round = 100\n",
    "\n",
    "param = {\n",
    "  'objective': 'binary:logistic',\n",
    " 'colsample_bytree': 0.9683760122352089,\n",
    " 'gamma':0.8835260600913024,\n",
    "#  'learning_rate': 0.1249426498504554,\n",
    "#  'max_depth': 20,\n",
    " 'min_child_weight': 110.95324500379702,\n",
    " 'n_estimators': 19,\n",
    " 'scale_pos_weight': 1,\n",
    " 'seed': 42,\n",
    " 'eval_metric': 'auc',\n",
    " 'lambda': 70,\n",
    " 'alpha': 20,\n",
    " 'rate_drop':0.950292864879127905,\n",
    " 'tree_method':'exact',\n",
    "#  'normalize_type':'forest',\n",
    " 'subsample': 0.9035691355661921\n",
    "}\n",
    " \n",
    "evals_result = {}\n",
    "bst_c = xgb.train(param, dx, num_round, evallist, evals_result=evals_result,early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014445108587368001 0.0\n",
      "0.888577252593989 0.001\n",
      "0.902899750180085 0.002\n",
      "0.9120725857127531 0.003\n",
      "0.9182950940273116 0.004\n",
      "0.9226707741352093 0.005\n",
      "0.9266403053013932 0.006\n",
      "0.9302343402761812 0.007\n",
      "0.9334222263092556 0.008\n",
      "0.9361656474627186 0.009000000000000001\n",
      "0.9382347081091851 0.01\n",
      "0.9403114319432311 0.011\n",
      "0.9417980903336551 0.012\n",
      "0.9433537174122948 0.013000000000000001\n",
      "0.9448403758027188 0.014\n",
      "0.9461737704415528 0.015\n",
      "0.9474381963921713 0.016\n",
      "0.9485263690284611 0.017\n",
      "0.9496681839778074 0.018000000000000002\n",
      "0.9507257038637792 0.019\n",
      "0.9517142550615354 0.02\n",
      "0.9526261743834965 0.021\n",
      "0.9534767882048216 0.022\n",
      "0.9543274020261467 0.023\n",
      "0.955193342222631 0.024\n",
      "0.9557450917283553 0.025\n",
      "0.9565267368614649 0.026000000000000002\n",
      "0.9571627814305639 0.027\n",
      "0.9577681732493448 0.028\n",
      "0.9583582386929668 0.029\n",
      "0.9588486826980551 0.03\n",
      "0.9594847272671541 0.031\n",
      "0.960074792710776 0.032\n",
      "0.9606265422165003 0.033\n",
      "0.9611016598464297 0.034\n",
      "0.961592103851518 0.035\n",
      "0.9620825478566064 0.036000000000000004\n",
      "0.9624886967983203 0.037\n",
      "0.9627875611139209 0.038\n",
      "0.9631630573053167 0.039\n",
      "0.9635232271215535 0.04\n",
      "0.9638757337502107 0.041\n",
      "0.964274219504345 0.042\n",
      "0.9647033580087974 0.043000000000000003\n",
      "0.9650405382622956 0.044\n",
      "0.9653394025778963 0.045\n",
      "0.9656689196438151 0.046\n",
      "0.9659677839594157 0.047\n",
      "0.966289637837755 0.048\n",
      "0.9665578494030377 0.049\n",
      "0.9667647554676844 0.05\n",
      "0.967048293408126 0.051000000000000004\n",
      "0.9673624840988858 0.052000000000000005\n",
      "0.9676920011648045 0.053\n",
      "0.9679755391052463 0.054\n",
      "0.9682207611077904 0.055\n",
      "0.9684966358606526 0.056\n",
      "0.9687571842383558 0.057\n",
      "0.9690330589912179 0.058\n",
      "0.9691939859303876 0.059000000000000004\n",
      "0.9694085551826137 0.06\n",
      "0.9696461139975784 0.061\n",
      "0.9697917145615891 0.062\n",
      "0.9699756310634972 0.063\n",
      "0.9701978635033028 0.064\n",
      "0.970458411881006 0.065\n",
      "0.9706806443208117 0.066\n",
      "0.9708875503854584 0.067\n",
      "0.971079130074946 0.068\n",
      "0.9712707097644336 0.069\n",
      "0.9714469630787622 0.07\n",
      "0.9716232163930909 0.07100000000000001\n",
      "0.9718761015832146 0.07200000000000001\n",
      "0.9720676812727022 0.073\n",
      "0.9721902922739742 0.074\n",
      "0.9723052400876668 0.075\n",
      "0.972519809339893 0.076\n",
      "0.9726500835287446 0.077\n",
      "0.9729642742195044 0.078\n",
      "0.9731941698468896 0.079\n",
      "0.973293791285423 0.08\n",
      "0.9734777077873312 0.081\n",
      "0.9736539611016598 0.082\n",
      "0.9738148880408295 0.083\n",
      "0.9740371204806352 0.084\n",
      "0.9742133737949638 0.085\n",
      "0.9743359847962358 0.08600000000000001\n",
      "0.9744356062347694 0.08700000000000001\n",
      "0.9745888699863595 0.088\n",
      "0.9747344705503701 0.089\n",
      "0.9749720293653348 0.09\n",
      "0.9751099667417659 0.091\n",
      "0.9752708936809356 0.092\n",
      "0.9754854629331616 0.093\n",
      "0.9755850843716952 0.094\n",
      "0.9756923689978083 0.095\n",
      "0.9758149799990804 0.096\n",
      "0.975960580563091 0.097\n",
      "0.9760985179395221 0.098\n",
      "0.9762594448786918 0.099\n",
      "0.9764280350054408 0.1\n",
      "0.9765276564439744 0.101\n",
      "0.9766196146949285 0.10200000000000001\n",
      "0.9767422256962006 0.10300000000000001\n",
      "0.9768801630726317 0.10400000000000001\n",
      "0.9769951108863243 0.105\n",
      "0.9771177218875964 0.106\n",
      "0.97721734332613 0.107\n",
      "0.9774242493907765 0.108\n",
      "0.9775315340168896 0.109\n",
      "0.9776464818305822 0.11\n",
      "0.9777614296442748 0.111\n",
      "0.9778074087697519 0.112\n",
      "0.9778917038331264 0.113\n",
      "0.9779989884592395 0.114\n",
      "0.9780602939598756 0.115\n",
      "0.9781139362729321 0.116\n",
      "0.9781675785859886 0.117\n",
      "0.9783131791499993 0.11800000000000001\n",
      "0.9784204637761124 0.11900000000000001\n",
      "0.978535411589805 0.12\n",
      "0.978704001716554 0.121\n",
      "0.9787959599675081 0.122\n",
      "0.9788572654681441 0.123\n",
      "0.9789262341563597 0.124\n",
      "0.9789952028445752 0.125\n",
      "0.9790794979079498 0.126\n",
      "0.9791714561589039 0.127\n",
      "0.9792480880346989 0.128\n",
      "0.979309393535335 0.129\n",
      "0.9793860254111301 0.13\n",
      "0.979462657286925 0.131\n",
      "0.9795239627875612 0.132\n",
      "0.9795852682881971 0.133\n",
      "0.9796925529143102 0.134\n",
      "0.9798075007280028 0.135\n",
      "0.9799377749168544 0.136\n",
      "0.9799760908547519 0.137\n",
      "0.9800144067926495 0.138\n",
      "0.980114028231183 0.139\n",
      "0.9801446809815011 0.14\n",
      "0.9802366392324552 0.14100000000000001\n",
      "0.9803285974834092 0.14200000000000002\n",
      "0.9804052293592043 0.14300000000000002\n",
      "0.9804512084846813 0.14400000000000002\n",
      "0.9805508299232149 0.145\n",
      "0.9806581145493279 0.146\n",
      "0.980750072800282 0.147\n",
      "0.9808650206139746 0.148\n",
      "0.9809339893021901 0.149\n",
      "0.9809799684276672 0.15\n",
      "0.9810106211779852 0.151\n",
      "0.9810489371158827 0.152\n",
      "0.9811025794289393 0.153\n",
      "0.9811715481171548 0.154\n",
      "0.9812175272426319 0.155\n",
      "0.9812328536177909 0.156\n",
      "0.9812941591184269 0.157\n",
      "0.981340138243904 0.158\n",
      "0.9813937805569605 0.159\n",
      "0.9814627492451761 0.16\n",
      "0.9814857388079146 0.161\n",
      "0.981539381120971 0.162\n",
      "0.9816466657470841 0.163\n",
      "0.9817386239980382 0.164\n",
      "0.9818229190614128 0.165\n",
      "0.9819148773123668 0.166\n",
      "0.9820221619384799 0.167\n",
      "0.982083467439116 0.168\n",
      "0.9821830888776496 0.169\n",
      "0.9822290680031266 0.17\n",
      "0.9822980366913421 0.171\n",
      "0.9823593421919782 0.17200000000000001\n",
      "0.9824129845050347 0.17300000000000001\n",
      "0.9824359740677733 0.17400000000000002\n",
      "0.9824742900056708 0.17500000000000002\n",
      "0.9825355955063068 0.176\n",
      "0.9826198905696814 0.177\n",
      "0.9827118488206354 0.178\n",
      "0.9827808175088509 0.179\n",
      "0.982826796634328 0.18\n",
      "0.982888102134964 0.181\n",
      "0.9829417444480206 0.182\n",
      "0.9829723971983386 0.183\n",
      "0.9830260395113951 0.184\n",
      "0.9830643554492927 0.185\n",
      "0.9830873450120312 0.186\n",
      "0.9831486505126672 0.187\n",
      "0.9832406087636213 0.188\n",
      "0.9833402302021549 0.189\n",
      "0.9834091988903705 0.19\n",
      "0.983508820328904 0.191\n",
      "0.9835318098916426 0.192\n",
      "0.9835624626419606 0.193\n",
      "0.9835854522046991 0.194\n",
      "0.9836237681425966 0.195\n",
      "0.9836774104556532 0.196\n",
      "0.9837233895811301 0.197\n",
      "0.9837923582693457 0.198\n",
      "0.9838230110196637 0.199\n",
      "0.9838306742072432 0.2\n",
      "0.9838613269575612 0.201\n",
      "0.9838996428954588 0.202\n",
      "0.9839686115836743 0.203\n",
      "0.9840069275215719 0.20400000000000001\n",
      "0.9840452434594694 0.20500000000000002\n",
      "0.9840835593973669 0.20600000000000002\n",
      "0.9841372017104235 0.20700000000000002\n",
      "0.984160191273162 0.20800000000000002\n",
      "0.984206170398639 0.209\n",
      "0.9842828022744341 0.21\n",
      "0.9843211182123316 0.211\n",
      "0.9843364445874906 0.212\n",
      "0.9843824237129677 0.213\n",
      "0.9843900869005472 0.214\n",
      "0.9844513924011832 0.215\n",
      "0.9844897083390807 0.216\n",
      "0.9845280242769783 0.217\n",
      "0.9845356874645578 0.218\n",
      "0.9845586770272963 0.219\n",
      "0.9845740034024553 0.22\n",
      "0.9845969929651938 0.221\n",
      "0.9846429720906709 0.222\n",
      "0.9846659616534094 0.223\n",
      "0.9846812880285684 0.224\n",
      "0.9847196039664659 0.225\n",
      "0.9847425935292045 0.226\n",
      "0.9847579199043635 0.227\n",
      "0.984780909467102 0.228\n",
      "0.984765583091943 0.229\n",
      "0.984780909467102 0.23\n",
      "0.9848038990298404 0.231\n",
      "0.9848345517801584 0.232\n",
      "0.984872867718056 0.233\n",
      "0.984934173218692 0.234\n",
      "0.98496482596901 0.23500000000000001\n",
      "0.9849954787193281 0.23600000000000002\n",
      "0.9850184682820666 0.23700000000000002\n",
      "0.9850261314696461 0.23800000000000002\n",
      "0.9850567842199641 0.23900000000000002\n",
      "0.9850644474075436 0.24\n",
      "0.9850644474075436 0.241\n",
      "0.9850797737827026 0.242\n",
      "0.9850797737827026 0.243\n",
      "0.9851410792833387 0.244\n",
      "0.9851487424709182 0.245\n",
      "0.9851793952212362 0.246\n",
      "0.9852177111591337 0.247\n",
      "0.9851947215963952 0.248\n",
      "0.9852560270970313 0.249\n",
      "0.9852713534721903 0.25\n",
      "0.9853173325976673 0.251\n",
      "0.9853939644734624 0.252\n",
      "0.9854476067865189 0.253\n",
      "0.985493585911996 0.254\n",
      "0.985539565037473 0.255\n",
      "0.985554891412632 0.256\n",
      "0.9855932073505296 0.257\n",
      "0.9855932073505296 0.258\n",
      "0.9856008705381091 0.259\n",
      "0.9855778809753705 0.26\n",
      "0.9856391864760066 0.261\n",
      "0.9856468496635861 0.262\n",
      "0.9856545128511656 0.263\n",
      "0.9856545128511656 0.264\n",
      "0.9856545128511656 0.265\n",
      "0.9856928287890631 0.266\n",
      "0.9857388079145402 0.267\n",
      "0.9857694606648582 0.268\n",
      "0.9858001134151761 0.269\n",
      "0.9858077766027556 0.27\n",
      "0.9858231029779146 0.271\n",
      "0.9858690821033917 0.272\n",
      "0.9858690821033917 0.273\n",
      "0.9858997348537097 0.274\n",
      "0.9859533771667663 0.275\n",
      "0.9859380507916072 0.276\n",
      "0.9859610403543458 0.277\n",
      "0.9859533771667663 0.278\n",
      "0.9859993562922433 0.279\n",
      "0.9859687035419253 0.28\n",
      "0.9859993562922433 0.281\n",
      "0.9860376722301408 0.28200000000000003\n",
      "0.9860529986052998 0.28300000000000003\n",
      "0.9860836513556179 0.28400000000000003\n",
      "0.9861143041059359 0.28500000000000003\n",
      "0.9861526200438334 0.28600000000000003\n",
      "0.9861985991693105 0.28700000000000003\n",
      "0.9861756096065719 0.28800000000000003\n",
      "0.9861985991693105 0.289\n",
      "0.9861832727941514 0.29\n",
      "0.9861832727941514 0.291\n",
      "0.986236915107208 0.292\n",
      "0.986267567857526 0.293\n",
      "0.9862752310451055 0.294\n",
      "0.986298220607844 0.295\n",
      "0.9863058837954235 0.296\n",
      "0.9863595261084801 0.297\n",
      "0.9863901788587981 0.298\n",
      "0.9864284947966956 0.299\n",
      "0.9864668107345932 0.3\n",
      "0.9864514843594342 0.301\n",
      "0.9864284947966956 0.302\n",
      "0.9864438211718547 0.303\n",
      "0.9864744739221727 0.304\n",
      "0.9864821371097522 0.305\n",
      "0.9864744739221727 0.306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9865204530476497 0.307\n",
      "0.9865357794228087 0.308\n",
      "0.9865587689855472 0.309\n",
      "0.9866354008613423 0.31\n",
      "0.9866354008613423 0.311\n",
      "0.9866430640489218 0.312\n",
      "0.9866737167992398 0.313\n",
      "0.9866583904240808 0.314\n",
      "0.9866277376737628 0.315\n",
      "0.9866124112986038 0.316\n",
      "0.9865817585482858 0.317\n",
      "0.9866047481110243 0.318\n",
      "0.9866047481110243 0.319\n",
      "0.9866277376737628 0.32\n",
      "0.9866354008613423 0.321\n",
      "0.9866430640489218 0.322\n",
      "0.9866354008613423 0.323\n",
      "0.9866737167992398 0.324\n",
      "0.9867043695495579 0.325\n",
      "0.9867273591122964 0.326\n",
      "0.9867350222998759 0.327\n",
      "0.9867503486750349 0.328\n",
      "0.9867810014253529 0.329\n",
      "0.9867886646129324 0.33\n",
      "0.9868346437384095 0.331\n",
      "0.9868806228638864 0.332\n",
      "0.9869036124266249 0.333\n",
      "0.9869189388017839 0.334\n",
      "0.9869572547396815 0.335\n",
      "0.98698024430242 0.336\n",
      "0.9870032338651585 0.337\n",
      "0.987026223427897 0.338\n",
      "0.9870492129906356 0.339\n",
      "0.9870492129906356 0.34\n",
      "0.9870568761782151 0.341\n",
      "0.9871181816788511 0.342\n",
      "0.9871181816788511 0.343\n",
      "0.9871335080540101 0.34400000000000003\n",
      "0.9871335080540101 0.34500000000000003\n",
      "0.9871641608043281 0.34600000000000003\n",
      "0.9871564976167486 0.34700000000000003\n",
      "0.9871948135546462 0.34800000000000003\n",
      "0.9872178031173847 0.34900000000000003\n",
      "0.9872714454304412 0.35000000000000003\n",
      "0.9872637822428617 0.35100000000000003\n",
      "0.9872637822428617 0.352\n",
      "0.9872407926801232 0.353\n",
      "0.9872407926801232 0.354\n",
      "0.9872561190552822 0.355\n",
      "0.9872484558677027 0.356\n",
      "0.9872561190552822 0.357\n",
      "0.9872484558677027 0.358\n",
      "0.9872484558677027 0.359\n",
      "0.9872484558677027 0.36\n",
      "0.9872791086180207 0.361\n",
      "0.9872791086180207 0.362\n",
      "0.9873020981807593 0.363\n",
      "0.9873174245559183 0.364\n",
      "0.9873250877434978 0.365\n",
      "0.9873020981807593 0.366\n",
      "0.9873020981807593 0.367\n",
      "0.9873020981807593 0.368\n",
      "0.9873557404938158 0.369\n",
      "0.9873480773062363 0.37\n",
      "0.9873710668689748 0.371\n",
      "0.9873710668689748 0.372\n",
      "0.9874017196192928 0.373\n",
      "0.9874093828068723 0.374\n",
      "0.9874476987447699 0.375\n",
      "0.9874630251199289 0.376\n",
      "0.9874553619323494 0.377\n",
      "0.9874706883075084 0.378\n",
      "0.9874400355571904 0.379\n",
      "0.9874400355571904 0.38\n",
      "0.9874553619323494 0.381\n",
      "0.9874630251199289 0.382\n",
      "0.9874630251199289 0.383\n",
      "0.9874476987447699 0.384\n",
      "0.9874553619323494 0.385\n",
      "0.9874783514950879 0.386\n",
      "0.9874783514950879 0.387\n",
      "0.9874706883075084 0.388\n",
      "0.9874783514950879 0.389\n",
      "0.9875013410578264 0.39\n",
      "0.9875319938081444 0.391\n",
      "0.987539656995724 0.392\n",
      "0.987539656995724 0.393\n",
      "0.9875166674329854 0.394\n",
      "0.9875319938081444 0.395\n",
      "0.9875166674329854 0.396\n",
      "0.9874860146826674 0.397\n",
      "0.9874783514950879 0.398\n",
      "0.9874553619323494 0.399\n",
      "0.9874630251199289 0.4\n",
      "0.9874630251199289 0.401\n",
      "0.9874476987447699 0.402\n",
      "0.9874706883075084 0.403\n",
      "0.9874706883075084 0.404\n",
      "0.9874783514950879 0.405\n",
      "0.9874783514950879 0.406\n",
      "0.9874783514950879 0.40700000000000003\n",
      "0.9875090042454059 0.40800000000000003\n",
      "0.9875243306205649 0.40900000000000003\n",
      "0.987539656995724 0.41000000000000003\n",
      "0.9875626465584625 0.41100000000000003\n",
      "0.9875779729336215 0.41200000000000003\n",
      "0.9875932993087805 0.41300000000000003\n",
      "0.9876086256839395 0.41400000000000003\n",
      "0.987616288871519 0.41500000000000004\n",
      "0.987585636121201 0.41600000000000004\n",
      "0.9875626465584625 0.417\n",
      "0.9875626465584625 0.418\n",
      "0.987554983370883 0.419\n",
      "0.987539656995724 0.42\n",
      "0.987570309746042 0.421\n",
      "0.98760096249636 0.422\n",
      "0.9876086256839395 0.423\n",
      "0.987631615246678 0.424\n",
      "0.987646941621837 0.425\n",
      "0.9876622679969961 0.426\n",
      "0.9876852575597346 0.427\n",
      "0.9877235734976321 0.428\n",
      "0.9877235734976321 0.429\n",
      "0.9877388998727911 0.43\n",
      "0.9877235734976321 0.431\n",
      "0.9876852575597346 0.432\n",
      "0.9877082471224731 0.433\n",
      "0.9876852575597346 0.434\n",
      "0.9877082471224731 0.435\n",
      "0.9877465630603706 0.436\n",
      "0.9877542262479501 0.437\n",
      "0.9877312366852116 0.438\n",
      "0.9877388998727911 0.439\n",
      "0.9877312366852116 0.44\n",
      "0.9877312366852116 0.441\n",
      "0.9877388998727911 0.442\n",
      "0.9877925421858477 0.443\n",
      "0.9878078685610067 0.444\n",
      "0.9878002053734272 0.445\n",
      "0.9878231949361657 0.446\n",
      "0.9878385213113247 0.447\n",
      "0.9878691740616427 0.448\n",
      "0.9878768372492222 0.449\n",
      "0.9879151531871198 0.45\n",
      "0.9879458059374377 0.451\n",
      "0.9879534691250172 0.452\n",
      "0.9879304795622788 0.453\n",
      "0.9879151531871198 0.454\n",
      "0.9879228163746993 0.455\n",
      "0.9879381427498583 0.456\n",
      "0.9879458059374377 0.457\n",
      "0.9879534691250172 0.458\n",
      "0.9879534691250172 0.459\n",
      "0.9879611323125967 0.46\n",
      "0.9879534691250172 0.461\n",
      "0.9879841218753352 0.462\n",
      "0.9879534691250172 0.463\n",
      "0.9879534691250172 0.464\n",
      "0.9879458059374377 0.465\n",
      "0.9879534691250172 0.466\n",
      "0.9879917850629147 0.467\n",
      "0.9879994482504942 0.468\n",
      "0.9879917850629147 0.46900000000000003\n",
      "0.9879917850629147 0.47000000000000003\n",
      "0.9879994482504942 0.47100000000000003\n",
      "0.9879917850629147 0.47200000000000003\n",
      "0.9879764586877557 0.47300000000000003\n",
      "0.9879917850629147 0.47400000000000003\n",
      "0.9879917850629147 0.47500000000000003\n",
      "0.9879917850629147 0.47600000000000003\n",
      "0.9879764586877557 0.47700000000000004\n",
      "0.9879764586877557 0.47800000000000004\n",
      "0.9879687955001762 0.47900000000000004\n",
      "0.9879458059374377 0.48\n",
      "0.9879534691250172 0.481\n",
      "0.9879534691250172 0.482\n",
      "0.9879381427498583 0.483\n",
      "0.9879381427498583 0.484\n",
      "0.9879381427498583 0.485\n",
      "0.9879381427498583 0.486\n",
      "0.9879381427498583 0.487\n",
      "0.9879228163746993 0.488\n",
      "0.9879228163746993 0.489\n",
      "0.9879611323125967 0.49\n",
      "0.9879917850629147 0.491\n",
      "0.9880301010008123 0.492\n",
      "0.9880454273759713 0.493\n",
      "0.9880377641883918 0.494\n",
      "0.9880454273759713 0.495\n",
      "0.9880454273759713 0.496\n",
      "0.9880377641883918 0.497\n",
      "0.9880454273759713 0.498\n",
      "0.9880301010008123 0.499\n",
      "0.9880301010008123 0.5\n",
      "0.9880377641883918 0.501\n",
      "0.9880454273759713 0.502\n",
      "0.9880760801262893 0.503\n",
      "0.9880760801262893 0.504\n",
      "0.9880990696890278 0.505\n",
      "0.9880837433138688 0.506\n",
      "0.9880607537511303 0.507\n",
      "0.9880530905635508 0.508\n",
      "0.9880224378132328 0.509\n",
      "0.9880224378132328 0.51\n",
      "0.9880377641883918 0.511\n",
      "0.9880224378132328 0.512\n",
      "0.9880301010008123 0.513\n",
      "0.9880377641883918 0.514\n",
      "0.9880301010008123 0.515\n",
      "0.9880224378132328 0.516\n",
      "0.9880071114380737 0.517\n",
      "0.9879994482504942 0.518\n",
      "0.9879994482504942 0.519\n",
      "0.9879917850629147 0.52\n",
      "0.9880147746256532 0.521\n",
      "0.9879917850629147 0.522\n",
      "0.9879917850629147 0.523\n",
      "0.9879841218753352 0.524\n",
      "0.9879841218753352 0.525\n",
      "0.9879764586877557 0.526\n",
      "0.9879687955001762 0.527\n",
      "0.9879534691250172 0.528\n",
      "0.9879611323125967 0.529\n",
      "0.9879687955001762 0.53\n",
      "0.9879764586877557 0.531\n",
      "0.9879611323125967 0.532\n",
      "0.9879994482504942 0.533\n",
      "0.9879994482504942 0.534\n",
      "0.9879994482504942 0.535\n",
      "0.9879917850629147 0.536\n",
      "0.9880147746256532 0.537\n",
      "0.9880147746256532 0.538\n",
      "0.9880071114380737 0.539\n",
      "0.9879764586877557 0.54\n",
      "0.9879994482504942 0.541\n",
      "0.9879917850629147 0.542\n",
      "0.9879917850629147 0.543\n",
      "0.9879841218753352 0.544\n",
      "0.9879917850629147 0.545\n",
      "0.9879687955001762 0.546\n",
      "0.9879534691250172 0.547\n",
      "0.9879228163746993 0.548\n",
      "0.9879304795622788 0.549\n",
      "0.9879381427498583 0.55\n",
      "0.9879381427498583 0.551\n",
      "0.9879458059374377 0.552\n",
      "0.9879764586877557 0.553\n",
      "0.9879687955001762 0.554\n",
      "0.9879687955001762 0.555\n",
      "0.9879917850629147 0.556\n",
      "0.9879841218753352 0.557\n",
      "0.9879687955001762 0.558\n",
      "0.9879917850629147 0.559\n",
      "0.9879841218753352 0.56\n",
      "0.9879381427498583 0.561\n",
      "0.9879458059374377 0.562\n",
      "0.9879534691250172 0.5630000000000001\n",
      "0.9879381427498583 0.5640000000000001\n",
      "0.9879304795622788 0.5650000000000001\n",
      "0.9879228163746993 0.5660000000000001\n",
      "0.9879381427498583 0.5670000000000001\n",
      "0.9879304795622788 0.5680000000000001\n",
      "0.9878921636243811 0.5690000000000001\n",
      "0.9878845004368016 0.5700000000000001\n",
      "0.9878998268119608 0.5710000000000001\n",
      "0.9879074899995403 0.5720000000000001\n",
      "0.9878998268119608 0.5730000000000001\n",
      "0.9878998268119608 0.5740000000000001\n",
      "0.9878615108740632 0.5750000000000001\n",
      "0.9878691740616427 0.5760000000000001\n",
      "0.9878538476864837 0.577\n",
      "0.9878231949361657 0.578\n",
      "0.9877772158106887 0.579\n",
      "0.9877618894355296 0.58\n",
      "0.9877542262479501 0.581\n",
      "0.9877465630603706 0.582\n",
      "0.9877159103100526 0.583\n",
      "0.9876775943721551 0.584\n",
      "0.9876546048094165 0.585\n",
      "0.9876546048094165 0.586\n",
      "0.9876546048094165 0.587\n",
      "0.9876392784342575 0.588\n",
      "0.9876392784342575 0.589\n",
      "0.987631615246678 0.59\n",
      "0.987631615246678 0.591\n",
      "0.9876239520590985 0.592\n",
      "0.9876392784342575 0.593\n",
      "0.987631615246678 0.594\n",
      "0.987631615246678 0.595\n",
      "0.987616288871519 0.596\n",
      "0.9876239520590985 0.597\n",
      "0.987616288871519 0.598\n",
      "0.987616288871519 0.599\n",
      "0.987631615246678 0.6\n",
      "0.9876392784342575 0.601\n",
      "0.987646941621837 0.602\n",
      "0.9876546048094165 0.603\n",
      "0.9876546048094165 0.604\n",
      "0.9876392784342575 0.605\n",
      "0.987631615246678 0.606\n",
      "0.9876392784342575 0.607\n",
      "0.9876392784342575 0.608\n",
      "0.987631615246678 0.609\n",
      "0.98760096249636 0.61\n",
      "0.98760096249636 0.611\n",
      "0.9875779729336215 0.612\n",
      "0.9875626465584625 0.613\n",
      "0.987554983370883 0.614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9875319938081444 0.615\n",
      "0.987539656995724 0.616\n",
      "0.9875243306205649 0.617\n",
      "0.9875166674329854 0.618\n",
      "0.9875319938081444 0.619\n",
      "0.9875243306205649 0.62\n",
      "0.9875090042454059 0.621\n",
      "0.987539656995724 0.622\n",
      "0.9875319938081444 0.623\n",
      "0.987539656995724 0.624\n",
      "0.987539656995724 0.625\n",
      "0.9875319938081444 0.626\n",
      "0.9875166674329854 0.627\n",
      "0.9875243306205649 0.628\n",
      "0.9875319938081444 0.629\n",
      "0.9875243306205649 0.63\n",
      "0.9874860146826674 0.631\n",
      "0.9874860146826674 0.632\n",
      "0.9874706883075084 0.633\n",
      "0.9874706883075084 0.634\n",
      "0.9874630251199289 0.635\n",
      "0.9874630251199289 0.636\n",
      "0.9874630251199289 0.637\n",
      "0.9874706883075084 0.638\n",
      "0.9874936778702469 0.639\n",
      "0.9874936778702469 0.64\n",
      "0.9874936778702469 0.641\n",
      "0.9874936778702469 0.642\n",
      "0.9874783514950879 0.643\n",
      "0.9874783514950879 0.644\n",
      "0.9874630251199289 0.645\n",
      "0.9874783514950879 0.646\n",
      "0.9874783514950879 0.647\n",
      "0.9874783514950879 0.648\n",
      "0.9874706883075084 0.649\n",
      "0.9874553619323494 0.65\n",
      "0.9874630251199289 0.651\n",
      "0.9874400355571904 0.652\n",
      "0.9874476987447699 0.653\n",
      "0.9874476987447699 0.654\n",
      "0.9874400355571904 0.655\n",
      "0.9874170459944519 0.656\n",
      "0.9874017196192928 0.657\n",
      "0.9873710668689748 0.658\n",
      "0.9873557404938158 0.659\n",
      "0.9873480773062363 0.66\n",
      "0.9873250877434978 0.661\n",
      "0.9873250877434978 0.662\n",
      "0.9873174245559183 0.663\n",
      "0.9872944349931798 0.664\n",
      "0.9872867718056002 0.665\n",
      "0.9872791086180207 0.666\n",
      "0.9872791086180207 0.667\n",
      "0.9872714454304412 0.668\n",
      "0.9872637822428617 0.669\n",
      "0.9872561190552822 0.67\n",
      "0.9872637822428617 0.671\n",
      "0.9872484558677027 0.672\n",
      "0.9872561190552822 0.673\n",
      "0.9872637822428617 0.674\n",
      "0.9872484558677027 0.675\n",
      "0.9872407926801232 0.676\n",
      "0.9872331294925437 0.677\n",
      "0.9872407926801232 0.678\n",
      "0.9872407926801232 0.679\n",
      "0.9872561190552822 0.68\n",
      "0.9872407926801232 0.681\n",
      "0.9872407926801232 0.682\n",
      "0.9872331294925437 0.683\n",
      "0.9872178031173847 0.684\n",
      "0.9872101399298052 0.685\n",
      "0.9871948135546462 0.686\n",
      "0.9871871503670667 0.687\n",
      "0.9871718239919077 0.6880000000000001\n",
      "0.9871488344291691 0.6890000000000001\n",
      "0.9871335080540101 0.6900000000000001\n",
      "0.9871411712415896 0.6910000000000001\n",
      "0.9871181816788511 0.6920000000000001\n",
      "0.9871411712415896 0.6930000000000001\n",
      "0.9871105184912716 0.6940000000000001\n",
      "0.9871028553036921 0.6950000000000001\n",
      "0.9870875289285331 0.6960000000000001\n",
      "0.9870722025533741 0.6970000000000001\n",
      "0.9870645393657946 0.6980000000000001\n",
      "0.9870492129906356 0.6990000000000001\n",
      "0.9870568761782151 0.7000000000000001\n",
      "0.987041549803056 0.7010000000000001\n",
      "0.987026223427897 0.7020000000000001\n",
      "0.9870338866154765 0.7030000000000001\n",
      "0.987026223427897 0.704\n",
      "0.9869879074899995 0.705\n",
      "0.986964917927261 0.706\n",
      "0.986964917927261 0.707\n",
      "0.9869572547396815 0.708\n",
      "0.9869419283645225 0.709\n",
      "0.9869419283645225 0.71\n",
      "0.9869419283645225 0.711\n",
      "0.9869572547396815 0.712\n",
      "0.986934265176943 0.713\n",
      "0.9869036124266249 0.714\n",
      "0.9869036124266249 0.715\n",
      "0.9869112756142044 0.716\n",
      "0.9869112756142044 0.717\n",
      "0.9869189388017839 0.718\n",
      "0.9868959492390454 0.719\n",
      "0.9868499701135685 0.72\n",
      "0.986842306925989 0.721\n",
      "0.9868346437384095 0.722\n",
      "0.98682698055083 0.723\n",
      "0.9868193173632505 0.724\n",
      "0.9868193173632505 0.725\n",
      "0.986811654175671 0.726\n",
      "0.986811654175671 0.727\n",
      "0.986811654175671 0.728\n",
      "0.9868039909880915 0.729\n",
      "0.9867810014253529 0.73\n",
      "0.9867733382377734 0.731\n",
      "0.9867503486750349 0.732\n",
      "0.9867350222998759 0.733\n",
      "0.9867426854874554 0.734\n",
      "0.9867350222998759 0.735\n",
      "0.9866967063619784 0.736\n",
      "0.9866813799868193 0.737\n",
      "0.9866737167992398 0.738\n",
      "0.9866737167992398 0.739\n",
      "0.9866583904240808 0.74\n",
      "0.9866430640489218 0.741\n",
      "0.9866430640489218 0.742\n",
      "0.9866660536116603 0.743\n",
      "0.9866737167992398 0.744\n",
      "0.9866813799868193 0.745\n",
      "0.9866583904240808 0.746\n",
      "0.9866737167992398 0.747\n",
      "0.9866583904240808 0.748\n",
      "0.9866507272365013 0.749\n",
      "0.9866430640489218 0.75\n",
      "0.9866354008613423 0.751\n",
      "0.9866354008613423 0.752\n",
      "0.9866430640489218 0.753\n",
      "0.9866507272365013 0.754\n",
      "0.9866507272365013 0.755\n",
      "0.9866354008613423 0.756\n",
      "0.9866200744861833 0.757\n",
      "0.9866124112986038 0.758\n",
      "0.9866200744861833 0.759\n",
      "0.9865970849234448 0.76\n",
      "0.9865894217358653 0.761\n",
      "0.9865894217358653 0.762\n",
      "0.9865894217358653 0.763\n",
      "0.9865970849234448 0.764\n",
      "0.9865894217358653 0.765\n",
      "0.9865817585482858 0.766\n",
      "0.9865587689855472 0.767\n",
      "0.9865587689855472 0.768\n",
      "0.9865281162352292 0.769\n",
      "0.9865051266724907 0.77\n",
      "0.9865051266724907 0.771\n",
      "0.9864974634849112 0.772\n",
      "0.9864821371097522 0.773\n",
      "0.9864514843594342 0.774\n",
      "0.9864438211718547 0.775\n",
      "0.9864208316091161 0.776\n",
      "0.9864361579842751 0.777\n",
      "0.9864438211718547 0.778\n",
      "0.9864208316091161 0.779\n",
      "0.9863978420463776 0.78\n",
      "0.9863978420463776 0.781\n",
      "0.9864055052339571 0.782\n",
      "0.9864055052339571 0.783\n",
      "0.9864055052339571 0.784\n",
      "0.9863901788587981 0.785\n",
      "0.9863825156712186 0.786\n",
      "0.9863595261084801 0.787\n",
      "0.9863518629209006 0.788\n",
      "0.9863518629209006 0.789\n",
      "0.9863365365457416 0.79\n",
      "0.9863288733581621 0.791\n",
      "0.9863365365457416 0.792\n",
      "0.9863288733581621 0.793\n",
      "0.9863058837954235 0.794\n",
      "0.986298220607844 0.795\n",
      "0.9862752310451055 0.796\n",
      "0.986252241482367 0.797\n",
      "0.986236915107208 0.798\n",
      "0.9862292519196285 0.799\n",
      "0.9862292519196285 0.8\n",
      "0.9862139255444695 0.801\n",
      "0.9862139255444695 0.802\n",
      "0.98620626235689 0.803\n",
      "0.9861985991693105 0.804\n",
      "0.9861679464189924 0.805\n",
      "0.9861602832314129 0.806\n",
      "0.9861449568562539 0.807\n",
      "0.9861296304810949 0.808\n",
      "0.9861296304810949 0.809\n",
      "0.9861219672935154 0.81\n",
      "0.9861066409183564 0.811\n",
      "0.9860989777307769 0.812\n",
      "0.9860989777307769 0.8130000000000001\n",
      "0.9860913145431974 0.8140000000000001\n",
      "0.9860836513556179 0.8150000000000001\n",
      "0.9860606617928793 0.8160000000000001\n",
      "0.9860683249804588 0.8170000000000001\n",
      "0.9860606617928793 0.8180000000000001\n",
      "0.9860606617928793 0.8190000000000001\n",
      "0.9860606617928793 0.8200000000000001\n",
      "0.9860376722301408 0.8210000000000001\n",
      "0.9860300090425613 0.8220000000000001\n",
      "0.9860376722301408 0.8230000000000001\n",
      "0.9860146826674023 0.8240000000000001\n",
      "0.9860070194798228 0.8250000000000001\n",
      "0.9860070194798228 0.8260000000000001\n",
      "0.9860070194798228 0.8270000000000001\n",
      "0.9860070194798228 0.8280000000000001\n",
      "0.9860070194798228 0.8290000000000001\n",
      "0.9859916931046638 0.8300000000000001\n",
      "0.9859533771667663 0.8310000000000001\n",
      "0.9859533771667663 0.8320000000000001\n",
      "0.9859227244164482 0.833\n",
      "0.9859303876040277 0.834\n",
      "0.9859380507916072 0.835\n",
      "0.9859150612288687 0.836\n",
      "0.9859227244164482 0.837\n",
      "0.9859150612288687 0.838\n",
      "0.9858997348537097 0.839\n",
      "0.9859073980412892 0.84\n",
      "0.9858997348537097 0.841\n",
      "0.9858844084785507 0.842\n",
      "0.9858767452909712 0.843\n",
      "0.9858767452909712 0.844\n",
      "0.9858614189158122 0.845\n",
      "0.9858537557282328 0.846\n",
      "0.9858537557282328 0.847\n",
      "0.9858460925406533 0.848\n",
      "0.9858154397903351 0.849\n",
      "0.9858154397903351 0.85\n",
      "0.9857924502275967 0.851\n",
      "0.9857847870400172 0.852\n",
      "0.9857617974772787 0.853\n",
      "0.9857771238524377 0.854\n",
      "0.9857771238524377 0.855\n",
      "0.9857771238524377 0.856\n",
      "0.9857771238524377 0.857\n",
      "0.9857771238524377 0.858\n",
      "0.9857694606648582 0.859\n",
      "0.9857771238524377 0.86\n",
      "0.9857771238524377 0.861\n",
      "0.9857771238524377 0.862\n",
      "0.9857771238524377 0.863\n",
      "0.9857617974772787 0.864\n",
      "0.9857617974772787 0.865\n",
      "0.9857464711021197 0.866\n",
      "0.9857541342896992 0.867\n",
      "0.9857464711021197 0.868\n",
      "0.9857464711021197 0.869\n",
      "0.9857464711021197 0.87\n",
      "0.9857388079145402 0.871\n",
      "0.9857388079145402 0.872\n",
      "0.9857234815393812 0.873\n",
      "0.985708155164222 0.874\n",
      "0.985708155164222 0.875\n",
      "0.9857004919766426 0.876\n",
      "0.9856928287890631 0.877\n",
      "0.9856928287890631 0.878\n",
      "0.9856928287890631 0.879\n",
      "0.9856851656014836 0.88\n",
      "0.9856851656014836 0.881\n",
      "0.9856851656014836 0.882\n",
      "0.9856851656014836 0.883\n",
      "0.9856851656014836 0.884\n",
      "0.9856851656014836 0.885\n",
      "0.9856851656014836 0.886\n",
      "0.9856851656014836 0.887\n",
      "0.9856775024139041 0.888\n",
      "0.9856545128511656 0.889\n",
      "0.9856545128511656 0.89\n",
      "0.9856468496635861 0.891\n",
      "0.9856468496635861 0.892\n",
      "0.9856391864760066 0.893\n",
      "0.9856315232884271 0.894\n",
      "0.9856238601008476 0.895\n",
      "0.9856238601008476 0.896\n",
      "0.9856161969132681 0.897\n",
      "0.9856085337256886 0.898\n",
      "0.9856085337256886 0.899\n",
      "0.9856085337256886 0.9\n",
      "0.9856085337256886 0.901\n",
      "0.9856008705381091 0.902\n",
      "0.9856008705381091 0.903\n",
      "0.9856008705381091 0.904\n",
      "0.9856008705381091 0.905\n",
      "0.9856008705381091 0.906\n",
      "0.98558554416295 0.907\n",
      "0.98558554416295 0.908\n",
      "0.9855778809753705 0.909\n",
      "0.9855778809753705 0.91\n",
      "0.9855778809753705 0.911\n",
      "0.9855778809753705 0.912\n",
      "0.9855778809753705 0.913\n",
      "0.9855778809753705 0.914\n",
      "0.9855778809753705 0.915\n",
      "0.985570217787791 0.916\n",
      "0.985570217787791 0.917\n",
      "0.985570217787791 0.918\n",
      "0.985570217787791 0.919\n",
      "0.9855625546002115 0.92\n",
      "0.9855625546002115 0.921\n",
      "0.9855625546002115 0.922\n",
      "0.9855625546002115 0.923\n",
      "0.9855625546002115 0.924\n",
      "0.9855625546002115 0.925\n",
      "0.9855625546002115 0.926\n",
      "0.9855625546002115 0.927\n",
      "0.9855625546002115 0.928\n",
      "0.9855625546002115 0.929\n",
      "0.9855625546002115 0.93\n",
      "0.9855625546002115 0.931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9855625546002115 0.932\n",
      "0.985554891412632 0.933\n",
      "0.985554891412632 0.934\n",
      "0.985554891412632 0.935\n",
      "0.985554891412632 0.936\n",
      "0.985554891412632 0.937\n",
      "0.985554891412632 0.9380000000000001\n",
      "0.985554891412632 0.9390000000000001\n",
      "0.985554891412632 0.9400000000000001\n",
      "0.985554891412632 0.9410000000000001\n",
      "0.985554891412632 0.9420000000000001\n",
      "0.985554891412632 0.9430000000000001\n",
      "0.985554891412632 0.9440000000000001\n",
      "0.985554891412632 0.9450000000000001\n",
      "0.985554891412632 0.9460000000000001\n",
      "0.985554891412632 0.9470000000000001\n",
      "0.985554891412632 0.9480000000000001\n",
      "0.985554891412632 0.9490000000000001\n",
      "0.985554891412632 0.9500000000000001\n",
      "0.985554891412632 0.9510000000000001\n",
      "0.985554891412632 0.9520000000000001\n",
      "0.985554891412632 0.9530000000000001\n",
      "0.985554891412632 0.9540000000000001\n",
      "0.985554891412632 0.9550000000000001\n",
      "0.985554891412632 0.9560000000000001\n",
      "0.985554891412632 0.9570000000000001\n",
      "0.985554891412632 0.9580000000000001\n",
      "0.985554891412632 0.9590000000000001\n",
      "0.985554891412632 0.96\n",
      "0.985554891412632 0.961\n",
      "0.985554891412632 0.962\n",
      "0.985554891412632 0.963\n",
      "0.985554891412632 0.964\n",
      "0.985554891412632 0.965\n",
      "0.985554891412632 0.966\n",
      "0.985554891412632 0.967\n",
      "0.985554891412632 0.968\n",
      "0.985554891412632 0.969\n",
      "0.985554891412632 0.97\n",
      "0.985554891412632 0.971\n",
      "0.985554891412632 0.972\n",
      "0.985554891412632 0.973\n",
      "0.985554891412632 0.974\n",
      "0.985554891412632 0.975\n",
      "0.985554891412632 0.976\n",
      "0.985554891412632 0.977\n",
      "0.985554891412632 0.978\n",
      "0.985554891412632 0.979\n",
      "0.985554891412632 0.98\n",
      "0.985554891412632 0.981\n",
      "0.985554891412632 0.982\n",
      "0.985554891412632 0.983\n",
      "0.985554891412632 0.984\n",
      "0.985554891412632 0.985\n",
      "0.985554891412632 0.986\n",
      "0.985554891412632 0.987\n",
      "0.985554891412632 0.988\n",
      "0.985554891412632 0.989\n",
      "0.985554891412632 0.99\n",
      "0.985554891412632 0.991\n",
      "0.985554891412632 0.992\n",
      "0.985554891412632 0.993\n",
      "0.985554891412632 0.994\n",
      "0.985554891412632 0.995\n",
      "0.985554891412632 0.996\n",
      "0.985554891412632 0.997\n",
      "0.985554891412632 0.998\n",
      "0.985554891412632 0.999\n"
     ]
    }
   ],
   "source": [
    "treee = bst_c.best_ntree_limit\n",
    "# treee = 20\n",
    "# dt = xgb.DMatrix(x_test, label=yc_test,feature_names=feature_name)\n",
    "yc_pre = np.array(bst_c.predict(dt, ntree_limit=treee))\n",
    "# yc_pre = bst_c.predict(dt)\n",
    "# yc_pre\n",
    "\n",
    "from sklearn.metrics import precision_score,precision_recall_fscore_support\n",
    "# fpr, tpr, thresholds = roc_curve(yc_test, yc_pre, pos_label=1)\n",
    "thresholds = np.arange(0, 1, 0.001)\n",
    "for i in thresholds:\n",
    "#     print(i)\n",
    "    ycx_pre  = yc_pre > i\n",
    "    ycx_pre = ycx_pre.astype(int)\n",
    "#     print(f1_score(yc_ans, ycx_pre, average='binary',pos_label=1),i)\n",
    "    print(precision_score(yc_test, ycx_pre, average='micro',pos_label=1),i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "ycx_pre  = yc_pre > 0.505\n",
    "ycx_pre = ycx_pre.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9948346062414648"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1- ycx_pre.sum()/len(ycx_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1dd9971b710>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(bst_c,max_num_features =20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, ADASYN  \n",
    "from imblearn.combine import SMOTEENN ,SMOTETomek\n",
    "\n",
    "sm = SMOTEENN(random_state=42)\n",
    "# sm = SMOTETomek(random_state=42)\n",
    "# sm = ADASYN(random_state=42)\n",
    "# sm.fit(x)\n",
    "# x_sm, y_sm = sm.fit_sample(x_train, y_train)\n",
    "x, yc = sm.fit_sample(x, yc)\n",
    "\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3188442703493053"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "param = {\n",
    " 'objective': 'reg:linear',\n",
    " 'colsample_bytree': 0.9683760122352089,\n",
    " 'gamma':0.8835260600913024,\n",
    " 'learning_rate': 0.199426498504554,\n",
    " 'max_depth': 20,\n",
    " 'min_child_weight': 10.95324500379702,\n",
    " 'n_estimators': 25,\n",
    " 'scale_pos_weight': 1,\n",
    " 'seed': 42,\n",
    " 'eval_metric': 'rmse',\n",
    " 'lambda': 150,\n",
    " 'alpha': 100,\n",
    " 'rate_drop':0.50292864879127905,\n",
    " 'tree_method':'exact',\n",
    " 'normalize_type':'forest',\n",
    " 'subsample': 0.9035691355661921\n",
    "  'nthread':-1,\n",
    "}\n",
    "\n",
    "clf = XGBRegressor(**param)\n",
    "clf.fit(x,y)\n",
    "y_clf = clf.predict(x_test)\n",
    "y_clf[y_clf < 0] = 0\n",
    "# y_clf[y_clf > 2952646748] = 2952646748\n",
    "# y_ans = y_ans[:,0]\n",
    "score1(y_test,y_clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[20]\tvalid_0's rmse: 1.7373\n",
      "[40]\tvalid_0's rmse: 1.65721\n",
      "[60]\tvalid_0's rmse: 1.63687\n",
      "[80]\tvalid_0's rmse: 1.62926\n",
      "[100]\tvalid_0's rmse: 1.62868\n",
      "[120]\tvalid_0's rmse: 1.631\n",
      "[140]\tvalid_0's rmse: 1.63541\n",
      "[160]\tvalid_0's rmse: 1.64008\n",
      "[180]\tvalid_0's rmse: 1.64315\n",
      "Early stopping, best iteration is:\n",
      "[88]\tvalid_0's rmse: 1.62743\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.627431954710102"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_clf = model.predict(x_test, num_iteration=model.best_iteration)\n",
    "y_clf[y_clf < 0] = 0\n",
    "# y_clf[y_clf > 2952646748] = 2952646748\n",
    "# y_ans = y_ans[:,0]\n",
    "score1(y_test,y_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_clf[y_clf==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cv = data.drop(['Sum_transactionRevenue'], axis=1).values\n",
    "y_cv = data[['Sum_transactionRevenue']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2608089.33516184, -2077871.22695353, -2612211.73882191,\n",
       "       -2265468.29715745, -2286456.17723482])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# neg_mean_squared_log_error\n",
    "cross_val_score(clf, x_cv, y_cv, cv=5,scoring=\"neg_mean_absolute_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EBM_IT_01\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:6499: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  alternative=\"'density'\", removal=\"3.1\")\n",
      "C:\\Users\\EBM_IT_01\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:6499: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  alternative=\"'density'\", removal=\"3.1\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_plot = y_ans[:,0]-yc_pre\n",
    "x_plot = np.abs(x_plot)\n",
    "sns.distplot(x_plot);\n",
    "sns.distplot(x_plot, bins=20, kde=False, rug=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.0695055e+06, -5.0295868e+02,  5.1632475e+05, ...,\n",
       "        1.3233814e+05,  1.6087338e+05,  1.6798831e+03], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yc_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.66657810e+05, 5.02958679e+02, 1.69351520e+05, ...,\n",
       "       1.32338141e+05, 1.36926045e+05, 6.90217694e+03])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0],\n",
       "       [   4],\n",
       "       [   5],\n",
       "       ...,\n",
       "       [9924],\n",
       "       [9930],\n",
       "       [9940]], dtype=int64)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 300000\n",
    "np.argwhere(x_plot > n).shape\n",
    "np.argwhere(x_plot > n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-ee57cf78fb03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_plot\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_plot\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "x_plot[:,np.argwhere(x_plot < n)[:,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.75606782e-03, 5.07341902e-03, 3.05088435e-03, ...,\n",
       "        9.16666667e-01, 0.00000000e+00, 1.00000000e+00],\n",
       "       [1.67725809e-06, 2.84259875e-06, 0.00000000e+00, ...,\n",
       "        1.00000000e+00, 1.00000000e+00, 0.00000000e+00],\n",
       "       [2.50879982e-04, 3.41430258e-04, 1.94312425e-04, ...,\n",
       "        9.16666667e-01, 0.00000000e+00, 1.00000000e+00],\n",
       "       ...,\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        9.16666667e-01, 0.00000000e+00, 1.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = x[np.argwhere(x_plot < n)[:,0],:]\n",
    "x_ans_ = x_ans[np.argwhere(x_plot < n)[:,0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = y[np.argwhere(x_plot < n)[:,0],:]\n",
    "y_ans_ = y_ans[np.argwhere(x_plot < n)[:,0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8822, 29)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:37:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 3756 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[0]\teval-mae:1.44346e+06\ttrain-mae:139852\n",
      "Multiple eval metrics have been passed: 'train-mae' will be used for early stopping.\n",
      "\n",
      "Will train until train-mae hasn't improved in 50 rounds.\n",
      "[16:37:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4654 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[1]\teval-mae:1.13133e+06\ttrain-mae:183636\n",
      "[16:37:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4482 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 1 trees, weight = 0.769231\n",
      "[2]\teval-mae:1.33743e+06\ttrain-mae:157741\n",
      "[16:37:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4382 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 3 trees, weight = 0.769231\n",
      "[3]\teval-mae:1.02085e+06\ttrain-mae:163688\n",
      "[16:37:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4010 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 2 trees, weight = 0.769231\n",
      "[4]\teval-mae:1.29973e+06\ttrain-mae:146033\n",
      "[16:37:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4352 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 2 trees, weight = 0.769231\n",
      "[5]\teval-mae:1.13557e+06\ttrain-mae:141043\n",
      "[16:37:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 3800 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 2 trees, weight = 0.769231\n",
      "[6]\teval-mae:1.20416e+06\ttrain-mae:156494\n",
      "[16:37:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 3782 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 4 trees, weight = 0.769231\n",
      "[7]\teval-mae:1.30945e+06\ttrain-mae:86970.6\n",
      "[16:37:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4670 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 3 trees, weight = 0.769231\n",
      "[8]\teval-mae:1.09324e+06\ttrain-mae:156163\n",
      "[16:37:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4624 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 6 trees, weight = 0.769231\n",
      "[9]\teval-mae:1.10483e+06\ttrain-mae:103406\n",
      "[16:37:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4902 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 3 trees, weight = 0.769231\n",
      "[10]\teval-mae:1.03786e+06\ttrain-mae:134562\n",
      "[16:37:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4284 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 5 trees, weight = 0.769231\n",
      "[11]\teval-mae:1.02362e+06\ttrain-mae:104254\n",
      "[16:37:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4024 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 5 trees, weight = 0.769231\n",
      "[12]\teval-mae:1.21605e+06\ttrain-mae:152711\n",
      "[16:37:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4678 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 5 trees, weight = 0.769231\n",
      "[13]\teval-mae:1.07746e+06\ttrain-mae:103945\n",
      "[16:37:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4212 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 5 trees, weight = 0.769231\n",
      "[14]\teval-mae:1.24423e+06\ttrain-mae:126972\n",
      "[16:37:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 3882 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 10 trees, weight = 0.769231\n",
      "[15]\teval-mae:1.13589e+06\ttrain-mae:82300.9\n",
      "[16:37:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4272 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 10 trees, weight = 0.769231\n",
      "[16]\teval-mae:1.26637e+06\ttrain-mae:75652.6\n",
      "[16:37:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4052 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 11 trees, weight = 0.769231\n",
      "[17]\teval-mae:1.19923e+06\ttrain-mae:98636.5\n",
      "[16:37:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4466 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 11 trees, weight = 0.769231\n",
      "[18]\teval-mae:1.13813e+06\ttrain-mae:119213\n",
      "[16:37:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4398 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 8 trees, weight = 0.769231\n",
      "[19]\teval-mae:1.18903e+06\ttrain-mae:131811\n",
      "[16:37:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4510 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 6 trees, weight = 0.769231\n",
      "[20]\teval-mae:1.27579e+06\ttrain-mae:109741\n",
      "[16:37:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4528 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 11 trees, weight = 0.769231\n",
      "[21]\teval-mae:1.18525e+06\ttrain-mae:106244\n",
      "[16:37:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4288 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 15 trees, weight = 0.769231\n",
      "[22]\teval-mae:1.10744e+06\ttrain-mae:158813\n",
      "[16:37:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4192 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 9 trees, weight = 0.769231\n",
      "[23]\teval-mae:1.09322e+06\ttrain-mae:122448\n",
      "[16:37:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 3982 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 15 trees, weight = 0.769231\n",
      "[24]\teval-mae:1.1291e+06\ttrain-mae:132211\n",
      "[16:37:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4572 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 14 trees, weight = 0.769231\n",
      "[25]\teval-mae:1.1093e+06\ttrain-mae:82100.4\n",
      "[16:37:32] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4008 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:32] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 10 trees, weight = 0.769231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26]\teval-mae:1.11854e+06\ttrain-mae:101324\n",
      "[16:37:32] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4410 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:32] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 13 trees, weight = 0.769231\n",
      "[27]\teval-mae:1.18443e+06\ttrain-mae:112135\n",
      "[16:37:32] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4314 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:32] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 15 trees, weight = 0.769231\n",
      "[28]\teval-mae:1.17851e+06\ttrain-mae:120984\n",
      "[16:37:32] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4564 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:32] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 10 trees, weight = 0.769231\n",
      "[29]\teval-mae:1.27472e+06\ttrain-mae:143222\n",
      "[16:37:32] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 3622 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:32] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 19 trees, weight = 0.769231\n",
      "[30]\teval-mae:1.21621e+06\ttrain-mae:55821.3\n",
      "[16:37:32] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 3976 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:32] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 18 trees, weight = 0.769231\n",
      "[31]\teval-mae:996668\ttrain-mae:103661\n",
      "[16:37:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4858 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 18 trees, weight = 0.769231\n",
      "[32]\teval-mae:1.15286e+06\ttrain-mae:103771\n",
      "[16:37:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4310 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 21 trees, weight = 0.769231\n",
      "[33]\teval-mae:1.19551e+06\ttrain-mae:108221\n",
      "[16:37:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4398 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 16 trees, weight = 0.769231\n",
      "[34]\teval-mae:1.2127e+06\ttrain-mae:137989\n",
      "[16:37:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4704 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 22 trees, weight = 0.769231\n",
      "[35]\teval-mae:1.22215e+06\ttrain-mae:135463\n",
      "[16:37:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4234 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 21 trees, weight = 0.769231\n",
      "[36]\teval-mae:1.04487e+06\ttrain-mae:140597\n",
      "[16:37:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4614 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 18 trees, weight = 0.769231\n",
      "[37]\teval-mae:1.15993e+06\ttrain-mae:117202\n",
      "[16:37:34] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4726 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:34] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 18 trees, weight = 0.769231\n",
      "[38]\teval-mae:1.04446e+06\ttrain-mae:125174\n",
      "[16:37:34] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4444 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:34] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 19 trees, weight = 0.769231\n",
      "[39]\teval-mae:1.08392e+06\ttrain-mae:115241\n",
      "[16:37:34] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4312 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:34] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 23 trees, weight = 0.769231\n",
      "[40]\teval-mae:1.09652e+06\ttrain-mae:119266\n",
      "[16:37:34] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4574 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:34] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 15 trees, weight = 0.769231\n",
      "[41]\teval-mae:1.25941e+06\ttrain-mae:105803\n",
      "[16:37:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4476 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 21 trees, weight = 0.769231\n",
      "[42]\teval-mae:1.06163e+06\ttrain-mae:88509.3\n",
      "[16:37:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4410 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 19 trees, weight = 0.769231\n",
      "[43]\teval-mae:1.23416e+06\ttrain-mae:101800\n",
      "[16:37:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 3820 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 18 trees, weight = 0.769231\n",
      "[44]\teval-mae:1.0792e+06\ttrain-mae:101525\n",
      "[16:37:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4516 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 22 trees, weight = 0.769231\n",
      "[45]\teval-mae:1.15993e+06\ttrain-mae:73450.4\n",
      "[16:37:36] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4352 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:36] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 23 trees, weight = 0.769231\n",
      "[46]\teval-mae:1.09289e+06\ttrain-mae:97602.3\n",
      "[16:37:36] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4378 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:36] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 25 trees, weight = 0.769231\n",
      "[47]\teval-mae:1.07459e+06\ttrain-mae:107756\n",
      "[16:37:36] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4376 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:36] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 26 trees, weight = 0.769231\n",
      "[48]\teval-mae:1.24816e+06\ttrain-mae:91484.8\n",
      "[16:37:36] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4008 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:36] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 24 trees, weight = 0.769231\n",
      "[49]\teval-mae:1.16754e+06\ttrain-mae:101339\n",
      "[16:37:36] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4638 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:36] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 27 trees, weight = 0.769231\n",
      "[50]\teval-mae:1.16563e+06\ttrain-mae:93673.5\n",
      "[16:37:37] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4536 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:37] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 28 trees, weight = 0.769231\n",
      "[51]\teval-mae:1.1721e+06\ttrain-mae:102353\n",
      "[16:37:37] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4090 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:37] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 22 trees, weight = 0.769231\n",
      "[52]\teval-mae:1.03733e+06\ttrain-mae:87872.4\n",
      "[16:37:37] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4256 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:37] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 27 trees, weight = 0.769231\n",
      "[53]\teval-mae:1.20149e+06\ttrain-mae:120106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:37:38] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4016 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:38] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 29 trees, weight = 0.769231\n",
      "[54]\teval-mae:1.10875e+06\ttrain-mae:103951\n",
      "[16:37:38] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4710 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:38] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 20 trees, weight = 0.769231\n",
      "[55]\teval-mae:1.13232e+06\ttrain-mae:120567\n",
      "[16:37:38] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4360 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:38] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 25 trees, weight = 0.769231\n",
      "[56]\teval-mae:1.15804e+06\ttrain-mae:139301\n",
      "[16:37:38] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4026 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:38] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 28 trees, weight = 0.769231\n",
      "[57]\teval-mae:1.21505e+06\ttrain-mae:134632\n",
      "[16:37:39] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4420 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:39] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 27 trees, weight = 0.769231\n",
      "[58]\teval-mae:1.07056e+06\ttrain-mae:87091.1\n",
      "[16:37:39] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4230 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:39] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 27 trees, weight = 0.769231\n",
      "[59]\teval-mae:1.29618e+06\ttrain-mae:106144\n",
      "[16:37:39] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4416 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:39] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 29 trees, weight = 0.769231\n",
      "[60]\teval-mae:1.16956e+06\ttrain-mae:128503\n",
      "[16:37:40] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4772 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:40] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 37 trees, weight = 0.769231\n",
      "[61]\teval-mae:1.17319e+06\ttrain-mae:131826\n",
      "[16:37:40] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4114 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:40] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 31 trees, weight = 0.769231\n",
      "[62]\teval-mae:1.07702e+06\ttrain-mae:80681.2\n",
      "[16:37:40] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4884 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:40] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 32 trees, weight = 0.769231\n",
      "[63]\teval-mae:1.0487e+06\ttrain-mae:104618\n",
      "[16:37:41] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4154 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:41] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 29 trees, weight = 0.769231\n",
      "[64]\teval-mae:1.01973e+06\ttrain-mae:101850\n",
      "[16:37:41] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4826 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:41] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 29 trees, weight = 0.769231\n",
      "[65]\teval-mae:1.28803e+06\ttrain-mae:90102\n",
      "[16:37:41] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4592 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:41] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 36 trees, weight = 0.769231\n",
      "[66]\teval-mae:1.01999e+06\ttrain-mae:128159\n",
      "[16:37:42] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4624 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:42] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 36 trees, weight = 0.769231\n",
      "[67]\teval-mae:1.25034e+06\ttrain-mae:105978\n",
      "[16:37:42] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4472 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:42] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 32 trees, weight = 0.769231\n",
      "[68]\teval-mae:1.11763e+06\ttrain-mae:93835.6\n",
      "[16:37:42] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 3492 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:42] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 36 trees, weight = 0.769231\n",
      "[69]\teval-mae:1.03259e+06\ttrain-mae:112023\n",
      "[16:37:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 3804 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 36 trees, weight = 0.769231\n",
      "[70]\teval-mae:1.0733e+06\ttrain-mae:126817\n",
      "[16:37:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4750 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 32 trees, weight = 0.769231\n",
      "[71]\teval-mae:1.12132e+06\ttrain-mae:91468.6\n",
      "[16:37:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 3760 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 39 trees, weight = 0.769231\n",
      "[72]\teval-mae:1.18158e+06\ttrain-mae:86555.6\n",
      "[16:37:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4824 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 34 trees, weight = 0.769231\n",
      "[73]\teval-mae:1.22673e+06\ttrain-mae:66575.8\n",
      "[16:37:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4076 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 39 trees, weight = 0.769231\n",
      "[74]\teval-mae:1.06703e+06\ttrain-mae:106033\n",
      "[16:37:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4566 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 36 trees, weight = 0.769231\n",
      "[75]\teval-mae:1.01116e+06\ttrain-mae:121767\n",
      "[16:37:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4752 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 42 trees, weight = 0.769231\n",
      "[76]\teval-mae:1.16004e+06\ttrain-mae:91204\n",
      "[16:37:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4038 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 37 trees, weight = 0.769231\n",
      "[77]\teval-mae:1.11353e+06\ttrain-mae:93661.2\n",
      "[16:37:46] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4336 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:46] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 42 trees, weight = 0.769231\n",
      "[78]\teval-mae:1.12969e+06\ttrain-mae:141735\n",
      "[16:37:46] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4196 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:46] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 29 trees, weight = 0.769231\n",
      "[79]\teval-mae:1.11649e+06\ttrain-mae:139268\n",
      "[16:37:47] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4262 extra nodes, 0 pruned nodes, max_depth=20\n",
      "[16:37:47] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\gbm\\gbtree.cc:494: drop 41 trees, weight = 0.769231\n",
      "[80]\teval-mae:1.1799e+06\ttrain-mae:92087\n",
      "Stopping. Best iteration:\n",
      "[30]\teval-mae:1.21621e+06\ttrain-mae:55821.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "# dsm = xgb.DMatrix(x, label=y)\n",
    "dt = xgb.DMatrix(x_ans, label=y_ans)\n",
    "dx = xgb.DMatrix(x_, label=y_)\n",
    "evallist = [(dt, 'eval'), (dx, 'train')]\n",
    "num_round = 100\n",
    "param = {\n",
    "    'booster':'dart',\n",
    " 'objective': 'reg:linear',\n",
    " 'colsample_bytree': 0.9683760122352089,\n",
    " 'gamma':0.8835260600913024,\n",
    "#  'learning_rate': 0.199426498504554,\n",
    " 'max_depth': 20,\n",
    " 'min_child_weight': 10.95324500379702,\n",
    " 'n_estimators': 25,\n",
    " 'scale_pos_weight': 10,\n",
    " 'seed': 42,\n",
    " 'eval_metric': 'mae',\n",
    " 'lambda': 150,\n",
    " 'alpha': 100,\n",
    " 'rate_drop':0.50292864879127905,\n",
    " 'tree_method':'exact',\n",
    " 'normalize_type':'forest',\n",
    " 'subsample': 0.9035691355661921\n",
    "}\n",
    "evals_result = {}\n",
    "bst = xgb.train(param, dx, num_round, evallist, evals_result=evals_result,early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1497023.198957567"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treee = bst.best_ntree_limit\n",
    "# treee = 20\n",
    "# yc_pre = np.array(bst.predict(dt, ntree_limit=treee))\n",
    "yc_pre = bst.predict(dt)\n",
    "score1(y_ans,yc_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fullVisitorId</th>\n",
       "      <th>PredictedLogRevenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000000259678714014</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000049363351866189</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000053049821714864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000059488412965267</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000085840370633780</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0000091131414287111</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0000117255350596610</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0000118334805178127</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0000130646294093000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0000150005271820273</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0000166374699289385</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0000174453501096099</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>000018122977590134</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>000020731284570628</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0000232022622082281</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0000271086753662651</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0000282648818935742</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00003038793126460</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0000313524203455157</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0000324924635296742</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0000338548677636278</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0000354865008116989</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0000359974620542953</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0000384434116640351</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0000385653946068037</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0000397214032106948</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0000414003317636552</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0000421850492821864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0000436683523507380</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0000443488189725694</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617212</th>\n",
       "      <td>9999496940471624606</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617213</th>\n",
       "      <td>9999518756481096457</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617214</th>\n",
       "      <td>999953571560215806</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617215</th>\n",
       "      <td>9999562285379264463</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617216</th>\n",
       "      <td>9999614340989943927</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617217</th>\n",
       "      <td>9999627287761032935</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617218</th>\n",
       "      <td>9999629963061245228</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617219</th>\n",
       "      <td>9999664171331798815</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617220</th>\n",
       "      <td>9999686314743907991</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617221</th>\n",
       "      <td>9999686339684184799</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617222</th>\n",
       "      <td>9999687906312811598</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617223</th>\n",
       "      <td>9999715835738672145</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617224</th>\n",
       "      <td>9999733949531374274</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617225</th>\n",
       "      <td>9999775084144044428</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617226</th>\n",
       "      <td>999977536649351076</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617227</th>\n",
       "      <td>9999786007255502100</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617228</th>\n",
       "      <td>9999789814107288877</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617229</th>\n",
       "      <td>9999803509476553670</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617230</th>\n",
       "      <td>9999818112872622034</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617231</th>\n",
       "      <td>9999819762744314978</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617232</th>\n",
       "      <td>9999824326944242344</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617233</th>\n",
       "      <td>9999834067265202923</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617234</th>\n",
       "      <td>9999841005270636052</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617235</th>\n",
       "      <td>9999860794386137754</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617236</th>\n",
       "      <td>9999882818693474736</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617237</th>\n",
       "      <td>9999905960465191827</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617238</th>\n",
       "      <td>9999941518946450908</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617239</th>\n",
       "      <td>9999969142283897422</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617240</th>\n",
       "      <td>9999985820452794361</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617241</th>\n",
       "      <td>9999997304197521748</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>617242 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              fullVisitorId  PredictedLogRevenue\n",
       "0       0000000259678714014                  0.0\n",
       "1       0000049363351866189                  0.0\n",
       "2       0000053049821714864                  0.0\n",
       "3       0000059488412965267                  0.0\n",
       "4       0000085840370633780                  0.0\n",
       "5       0000091131414287111                  0.0\n",
       "6       0000117255350596610                  0.0\n",
       "7       0000118334805178127                  0.0\n",
       "8       0000130646294093000                  0.0\n",
       "9       0000150005271820273                  0.0\n",
       "10      0000166374699289385                  0.0\n",
       "11      0000174453501096099                  0.0\n",
       "12       000018122977590134                  0.0\n",
       "13       000020731284570628                  0.0\n",
       "14      0000232022622082281                  0.0\n",
       "15      0000271086753662651                  0.0\n",
       "16      0000282648818935742                  0.0\n",
       "17        00003038793126460                  0.0\n",
       "18      0000313524203455157                  0.0\n",
       "19      0000324924635296742                  0.0\n",
       "20      0000338548677636278                  0.0\n",
       "21      0000354865008116989                  0.0\n",
       "22      0000359974620542953                  0.0\n",
       "23      0000384434116640351                  0.0\n",
       "24      0000385653946068037                  0.0\n",
       "25      0000397214032106948                  0.0\n",
       "26      0000414003317636552                  0.0\n",
       "27      0000421850492821864                  0.0\n",
       "28      0000436683523507380                  0.0\n",
       "29      0000443488189725694                  0.0\n",
       "...                     ...                  ...\n",
       "617212  9999496940471624606                  0.0\n",
       "617213  9999518756481096457                  0.0\n",
       "617214   999953571560215806                  0.0\n",
       "617215  9999562285379264463                  0.0\n",
       "617216  9999614340989943927                  0.0\n",
       "617217  9999627287761032935                  0.0\n",
       "617218  9999629963061245228                  0.0\n",
       "617219  9999664171331798815                  0.0\n",
       "617220  9999686314743907991                  0.0\n",
       "617221  9999686339684184799                  0.0\n",
       "617222  9999687906312811598                  0.0\n",
       "617223  9999715835738672145                  0.0\n",
       "617224  9999733949531374274                  0.0\n",
       "617225  9999775084144044428                  0.0\n",
       "617226   999977536649351076                  0.0\n",
       "617227  9999786007255502100                  0.0\n",
       "617228  9999789814107288877                  0.0\n",
       "617229  9999803509476553670                  0.0\n",
       "617230  9999818112872622034                  0.0\n",
       "617231  9999819762744314978                  0.0\n",
       "617232  9999824326944242344                  0.0\n",
       "617233  9999834067265202923                  0.0\n",
       "617234  9999841005270636052                  0.0\n",
       "617235  9999860794386137754                  0.0\n",
       "617236  9999882818693474736                  0.0\n",
       "617237  9999905960465191827                  0.0\n",
       "617238  9999941518946450908                  0.0\n",
       "617239  9999969142283897422                  0.0\n",
       "617240  9999985820452794361                  0.0\n",
       "617241  9999997304197521748                  0.0\n",
       "\n",
       "[617242 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('google_data/sample_submission.csv', delimiter = ',')\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # it's a operational system library, to set some informations\n",
    "import random # random is to generate random values\n",
    "\n",
    "import pandas as pd # to manipulate data frames \n",
    "import numpy as np # to work with matrix\n",
    "from scipy.stats import kurtosis, skew # it's to explore some statistics of numerical values\n",
    "\n",
    "import matplotlib.pyplot as plt # to graphics plot\n",
    "import seaborn as sns # a good library to graphic plots\n",
    "# import squarify # to better understand proportion of categorys - it's a treemap layout algorithm\n",
    "\n",
    "# Importing librarys to use on interactive graphs\n",
    "# from plotly.offline import init_notebook_mode, iplot, plot \n",
    "# import plotly.graph_objs as go \n",
    "\n",
    "import json # to convert json in df\n",
    "from pandas.io.json import json_normalize # to normalize the json file\n",
    "\n",
    "columns = ['device', 'geoNetwork', 'totals', 'trafficSource'] # Columns that have json format\n",
    "\n",
    "dir_path = \"google_data/\" # you can change to your local \n",
    "\n",
    "# p is a fractional number to skiprows and read just a random sample of the our dataset. \n",
    "p = 0.7 # *** In this case we will use 50% of data set *** #\n",
    "\n",
    "#Code to transform the json format columns in table\n",
    "def json_read(df):\n",
    "    #joining the [ path + df received]\n",
    "    data_frame = dir_path + df\n",
    "    \n",
    "    #Importing the dataset\n",
    "    df = pd.read_csv(data_frame, \n",
    "                     converters={column: json.loads for column in columns}, # loading the json columns properly\n",
    "                     dtype={'fullVisitorId': 'str'}, # transforming this column to string\n",
    "                     skiprows=lambda i: i>0 and random.random() > p)# Number of rows that will be imported randomly\n",
    "    \n",
    "    for column in columns: #loop to finally transform the columns in data frame\n",
    "        #It will normalize and set the json to a table\n",
    "        column_as_df = json_normalize(df[column]) \n",
    "        # here will be set the name using the category and subcategory of json columns\n",
    "        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns] \n",
    "        # after extracting the values, let drop the original columns\n",
    "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "        \n",
    "    # Printing the shape of dataframes that was imported     \n",
    "    print(f\"Loaded {os.path.basename(data_frame)}. Shape: {df.shape}\")\n",
    "    return df # returning the df after importing and transforming to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train.csv. Shape: (632669, 55)\n"
     ]
    }
   ],
   "source": [
    "df_test = json_read(\"train.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(\"google_data/train_json_decode.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['channelGrouping',\n",
       " 'date',\n",
       " 'fullVisitorId',\n",
       " 'sessionId',\n",
       " 'socialEngagementType',\n",
       " 'visitId',\n",
       " 'visitNumber',\n",
       " 'visitStartTime',\n",
       " 'device.browser',\n",
       " 'device.browserSize',\n",
       " 'device.browserVersion',\n",
       " 'device.deviceCategory',\n",
       " 'device.flashVersion',\n",
       " 'device.isMobile',\n",
       " 'device.language',\n",
       " 'device.mobileDeviceBranding',\n",
       " 'device.mobileDeviceInfo',\n",
       " 'device.mobileDeviceMarketingName',\n",
       " 'device.mobileDeviceModel',\n",
       " 'device.mobileInputSelector',\n",
       " 'device.operatingSystem',\n",
       " 'device.operatingSystemVersion',\n",
       " 'device.screenColors',\n",
       " 'device.screenResolution',\n",
       " 'geoNetwork.city',\n",
       " 'geoNetwork.cityId',\n",
       " 'geoNetwork.continent',\n",
       " 'geoNetwork.country',\n",
       " 'geoNetwork.latitude',\n",
       " 'geoNetwork.longitude',\n",
       " 'geoNetwork.metro',\n",
       " 'geoNetwork.networkDomain',\n",
       " 'geoNetwork.networkLocation',\n",
       " 'geoNetwork.region',\n",
       " 'geoNetwork.subContinent',\n",
       " 'totals.bounces',\n",
       " 'totals.hits',\n",
       " 'totals.newVisits',\n",
       " 'totals.pageviews',\n",
       " 'totals.visits',\n",
       " 'trafficSource.adContent',\n",
       " 'trafficSource.adwordsClickInfo.adNetworkType',\n",
       " 'trafficSource.adwordsClickInfo.criteriaParameters',\n",
       " 'trafficSource.adwordsClickInfo.gclId',\n",
       " 'trafficSource.adwordsClickInfo.isVideoAd',\n",
       " 'trafficSource.adwordsClickInfo.page',\n",
       " 'trafficSource.adwordsClickInfo.slot',\n",
       " 'trafficSource.campaign',\n",
       " 'trafficSource.isTrueDirect',\n",
       " 'trafficSource.keyword',\n",
       " 'trafficSource.medium',\n",
       " 'trafficSource.referralPath',\n",
       " 'trafficSource.source']"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data_ans[['fullVisitorId','visitNumber']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visitNumber</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fullVisitorId</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0000000259678714014</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000049363351866189</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000053049821714864</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000059488412965267</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000085840370633780</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000091131414287111</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000117255350596610</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000118334805178127</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000130646294093000</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000150005271820273</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000166374699289385</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000174453501096099</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000018122977590134</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000020731284570628</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000232022622082281</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000271086753662651</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000282648818935742</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00003038793126460</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000313524203455157</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000324924635296742</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000338548677636278</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000354865008116989</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000359974620542953</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000384434116640351</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000385653946068037</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000397214032106948</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000414003317636552</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000421850492821864</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000436683523507380</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000443488189725694</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999496940471624606</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999518756481096457</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999953571560215806</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999562285379264463</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999614340989943927</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999627287761032935</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999629963061245228</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999664171331798815</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999686314743907991</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999686339684184799</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999687906312811598</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999715835738672145</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999733949531374274</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999775084144044428</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999977536649351076</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999786007255502100</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999789814107288877</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999803509476553670</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999818112872622034</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999819762744314978</th>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999824326944242344</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999834067265202923</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999841005270636052</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999860794386137754</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999882818693474736</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999905960465191827</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999941518946450908</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999969142283897422</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999985820452794361</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999997304197521748</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>617242 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     visitNumber\n",
       "fullVisitorId                   \n",
       "0000000259678714014          0.0\n",
       "0000049363351866189          0.0\n",
       "0000053049821714864          0.0\n",
       "0000059488412965267          0.0\n",
       "0000085840370633780          0.0\n",
       "0000091131414287111          0.0\n",
       "0000117255350596610          0.0\n",
       "0000118334805178127          0.0\n",
       "0000130646294093000          0.0\n",
       "0000150005271820273          0.0\n",
       "0000166374699289385          0.0\n",
       "0000174453501096099          0.0\n",
       "000018122977590134           0.0\n",
       "000020731284570628           0.0\n",
       "0000232022622082281          0.0\n",
       "0000271086753662651          0.0\n",
       "0000282648818935742          0.0\n",
       "00003038793126460            0.0\n",
       "0000313524203455157          0.0\n",
       "0000324924635296742          0.0\n",
       "0000338548677636278          0.0\n",
       "0000354865008116989          0.0\n",
       "0000359974620542953          0.0\n",
       "0000384434116640351          0.0\n",
       "0000385653946068037          0.0\n",
       "0000397214032106948          0.0\n",
       "0000414003317636552          0.0\n",
       "0000421850492821864          0.0\n",
       "0000436683523507380          0.0\n",
       "0000443488189725694          0.0\n",
       "...                          ...\n",
       "9999496940471624606          1.0\n",
       "9999518756481096457          1.0\n",
       "999953571560215806           1.0\n",
       "9999562285379264463          1.0\n",
       "9999614340989943927          1.0\n",
       "9999627287761032935          1.0\n",
       "9999629963061245228          1.0\n",
       "9999664171331798815          0.0\n",
       "9999686314743907991          1.0\n",
       "9999686339684184799          0.0\n",
       "9999687906312811598          0.0\n",
       "9999715835738672145          0.0\n",
       "9999733949531374274          0.0\n",
       "9999775084144044428          1.0\n",
       "999977536649351076           1.0\n",
       "9999786007255502100          0.0\n",
       "9999789814107288877          0.0\n",
       "9999803509476553670          0.0\n",
       "9999818112872622034          1.0\n",
       "9999819762744314978         20.0\n",
       "9999824326944242344          3.0\n",
       "9999834067265202923          0.0\n",
       "9999841005270636052          1.0\n",
       "9999860794386137754          0.0\n",
       "9999882818693474736          1.0\n",
       "9999905960465191827          0.0\n",
       "9999941518946450908          1.0\n",
       "9999969142283897422          0.0\n",
       "9999985820452794361          0.0\n",
       "9999997304197521748          1.0\n",
       "\n",
       "[617242 rows x 1 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.join(test.set_index('fullVisitorId'),on='fullVisitorId').groupby(\"fullVisitorId\").agg({\"visitNumber\" : \"sum\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = RandomForestRegressor()\n",
    "params = {\n",
    "    \"max_depth\": [1, 2, 3,5,10, None],\n",
    "    \"max_features\": st.randint(1, 27),\n",
    "#     \"max_features\": [\"log2\"],\n",
    "    \"min_samples_split\": st.randint(2, 10),\n",
    "#     \"min_samples_leaf\": st.randint(1, n_features),\n",
    "#     \"bootstrap\": [True, False],\n",
    "    \"criterion\": [\"mse\"],\n",
    "    'oob_score': [True, False],\n",
    "    'random_state': [42],\n",
    "    'warm_start' : [True],\n",
    "}\n",
    "rnf = RandomizedSearchCV(estimator, params, cv=5,n_jobs=-1, n_iter=10, scoring = 'neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting KFold iterations...\n",
      "Iteration: 1   rmse: 1.6168445813140997   model: lgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EBM_IT_01\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:724: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1   rmse: 1.633474498512408   model: rnf\n",
      "Iteration: 1  (All models)   rmse: 1.6191733549810883\n",
      "Iteration: 2   rmse: 1.6168166837952265   model: lgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EBM_IT_01\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:724: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2   rmse: 1.6412719074489954   model: rnf\n",
      "Iteration: 2  (All models)   rmse: 1.6224415976771276\n",
      "Iteration: 3   rmse: 1.609027513737331   model: lgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EBM_IT_01\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:724: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3   rmse: 1.6693029432615665   model: rnf\n",
      "Iteration: 3  (All models)   rmse: 1.6284659444338523\n",
      "Iteration: 4   rmse: 1.6226411514181984   model: lgb\n",
      "Iteration: 4   rmse: 1.6481481989311957   model: rnf\n",
      "Iteration: 4  (All models)   rmse: 1.6291505385621872\n",
      "Iteration: 5   rmse: 1.6275881423381688   model: lgb\n",
      "Iteration: 5   rmse: 1.6818418479801116   model: rnf\n",
      "Iteration: 5  (All models)   rmse: 1.6453760243835989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EBM_IT_01\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:724: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "k=1\n",
    "splits=5\n",
    "avg_score=0\n",
    "\n",
    "lgb_params = {\"objective\" : \"regression\", \"metric\" : \"rmse\",\n",
    "              \"num_leaves\" : 50, \"learning_rate\" : 0.032, \n",
    "              \"bagging_fraction\" : 0.75, \"feature_fraction\" : 0.8, \"bagging_frequency\" : 9, 'use_best_model':True}\n",
    "\n",
    "y_kf = np.copy(y)\n",
    "y_kf[y_kf != 0] = 1\n",
    "\n",
    "skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=200)\n",
    "skf.get_n_splits(x)\n",
    "print('\\nStarting KFold iterations...')\n",
    "importances = pd.DataFrame()\n",
    "for train_index,test_index in skf.split(x,y_kf):\n",
    "\n",
    "    lgb = LGBMRegressor(**lgb_params)\n",
    "    xgb = XGBRegressor(nthread=-1)\n",
    "    models = [('lgb',lgb),('rnf',rnf)]\n",
    "   \n",
    "    df_X=x[train_index,:]\n",
    "    df_y=y[train_index]\n",
    "    val_X=x[test_index,:]\n",
    "    val_y=y[test_index]\n",
    "    kk = 1\n",
    "    models = [('lgb',lgb),('rnf',rnf)]\n",
    "    for name,model in models:\n",
    "        model.fit(df_X,df_y)\n",
    "        preds_x=pd.Series(model.predict(val_X))\n",
    "        acc= score1(val_y,preds_x)\n",
    "        print('Iteration:',k,'  rmse:',acc,'  model:',name)\n",
    "    \n",
    "        if kk==1:\n",
    "#             score=acc\n",
    "            model1=model\n",
    "            preds_stack=pd.Series(model.predict(val_X))\n",
    "#             sv = preds.copy()\n",
    "\n",
    "        else:\n",
    "            preds1=pd.Series(model.predict(val_X))\n",
    "            preds_stack=preds_stack+preds1\n",
    "        \n",
    "        kk = kk+1\n",
    "    preds_stack=preds_stack/len(models)\n",
    "    acc= score1(val_y,preds_stack)\n",
    "    print('Iteration:',k,' (All models)   rmse:',acc)\n",
    "#     imp_df = pd.DataFrame()\n",
    "#     imp_df['feature'] = np.asarray(feature_name)\n",
    "#     imp_df['gain'] = model.feature_importances_\n",
    "    \n",
    "#     imp_df['fold'] = k\n",
    "#     importances = pd.concat([importances, imp_df], axis=0, sort=False)\n",
    "    \n",
    "#     avg_score=avg_score+acc        \n",
    "    k=k+1\n",
    "# print('\\n Best score:',score,' Avg Score:',avg_score/splits)\n",
    "# preds=preds/splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting KFold iterations...\n",
      "Iteration: 1   rmse: 1.6146913344419118   model: lgb\n",
      "Iteration: 1   rmse: 2.103823415463157   model: xbg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EBM_IT_01\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1   rmse: 1.6376556392016466   model: rnf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EBM_IT_01\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1   rmse: 1.9107490148554325   model: ext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EBM_IT_01\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1   rmse: 2.276413853849632   model: ada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EBM_IT_01\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1   rmse: 1.8143451956450163   model: elas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EBM_IT_01\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  (All models)   rmse: 1.7126322203663067\n",
      "Iteration: 2   rmse: 1.617130730483827   model: lgb\n",
      "Iteration: 2   rmse: 2.1075407893217655   model: xbg\n",
      "Iteration: 2   rmse: 1.6387075174774817   model: rnf\n",
      "Iteration: 2   rmse: 1.645825834088968   model: ext\n",
      "Iteration: 2   rmse: 2.3097790639132323   model: ada\n",
      "Iteration: 2   rmse: 1.832210827558753   model: elas\n",
      "Iteration: 2  (All models)   rmse: 1.6930315729410021\n",
      "Iteration: 3   rmse: 1.6106447912020534   model: lgb\n",
      "Iteration: 3   rmse: 2.1077026442245574   model: xbg\n",
      "Iteration: 3   rmse: 1.671326994510118   model: rnf\n",
      "Iteration: 3   rmse: 1.7493061055319288   model: ext\n",
      "Iteration: 3   rmse: 2.2368272506551743   model: ada\n",
      "Iteration: 3   rmse: 1.8490699931878691   model: elas\n",
      "Iteration: 3  (All models)   rmse: 1.6985736049332991\n",
      "Iteration: 4   rmse: 1.6228058433719683   model: lgb\n",
      "Iteration: 4   rmse: 2.1075867647640645   model: xbg\n",
      "Iteration: 4   rmse: 1.6889989690029623   model: rnf\n",
      "Iteration: 4   rmse: 1.65095205257341   model: ext\n",
      "Iteration: 4   rmse: 2.280549702785664   model: ada\n",
      "Iteration: 4   rmse: 1.8296498168544173   model: elas\n",
      "Iteration: 4  (All models)   rmse: 1.68868034618455\n",
      "Iteration: 5   rmse: 1.6275378096014146   model: lgb\n",
      "Iteration: 5   rmse: 2.1078817670476635   model: xbg\n",
      "Iteration: 5   rmse: 1.6470566434167118   model: rnf\n",
      "Iteration: 5   rmse: 1.655805300282725   model: ext\n",
      "Iteration: 5   rmse: 2.1860222099417896   model: ada\n",
      "Iteration: 5   rmse: 1.8845535335322225   model: elas\n",
      "Iteration: 5  (All models)   rmse: 1.690357394977638\n",
      " Avg Score: [1.6185621  2.10690708 1.65674915 1.72252766 2.25791842 1.84196587]   All models: 1.696655027880559\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm.sklearn import LGBMRegressor\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import scipy.stats as st\n",
    "from sklearn.model_selection import RandomizedSearchCV , GridSearchCV\n",
    "\n",
    "k=1\n",
    "splits=5\n",
    "\n",
    "\n",
    "lgb_params = {\"objective\" : \"regression\", \"metric\" : \"rmse\",\n",
    "              \"num_leaves\" : 50, \"learning_rate\" : 0.032, \n",
    "              \"bagging_fraction\" : 0.75, \"feature_fraction\" : 0.8, \"bagging_frequency\" : 9, 'use_best_model':True}\n",
    "\n",
    "\n",
    "y_kf = np.copy(y)\n",
    "y_kf[y_kf != 0] = 1\n",
    "\n",
    "x_f = x.values\n",
    "skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=200)\n",
    "skf.get_n_splits(x_f)\n",
    "print('\\nStarting KFold iterations...')\n",
    "importances = pd.DataFrame()\n",
    "\n",
    "x_stack = pd.DataFrame({'ini':np.zeros(x.shape[0])})\n",
    "x_ans_stack = pd.DataFrame({'ini':np.zeros(x_ans.shape[0])})\n",
    "\n",
    "avg_score= np.zeros(6)\n",
    "avg_score_all = 0\n",
    "kk = 1\n",
    "for train_index,test_index in skf.split(x_f,y_kf):\n",
    "\n",
    "    lgb = LGBMRegressor(**lgb_params)\n",
    "#     xgb = XGBRegressor(nthread=-1)\n",
    "\n",
    "    \n",
    "    df_X=x_f[train_index,:]\n",
    "    df_y=y[train_index]\n",
    "    val_X=x_f[test_index,:]\n",
    "    val_y=y[test_index]\n",
    "    \n",
    "    modelq = regressor.get_regressor(n_jobs = -1,n_iter = 5,n_iter_nt = 2,n_components = 25,cv = 5,\n",
    "                  seed=42,n_features= 40,is_pca = False,scoring = 'neg_mean_absolute_error')\n",
    "#     models = [('lgb',lgb),('xgb',xgb),('rnf',rnf)]\n",
    "    models = [('lgb',lgb)] + modelq\n",
    "    for n,(name,model) in enumerate(models,0):\n",
    "        model.fit(df_X,df_y)\n",
    "        preds_x=pd.Series(model.predict(val_X))\n",
    "        acc = score1(val_y,preds_x)\n",
    "        print('Iteration:',k,'  rmse:',acc,'  model:',name)\n",
    "        if k==1:\n",
    "            x_stack['y_'+name] = 0\n",
    "            x_ans_stack['y_'+name] = 0\n",
    "            \n",
    "        if kk == 1:\n",
    "            kk = kk+1;\n",
    "            x_stack = x_stack.drop(['ini'],axis=1)\n",
    "            x_ans_stack = x_ans_stack.drop(['ini'],axis=1)\n",
    "        x_stack['y_'+name][test_index] = preds_x \n",
    "        preds1=pd.Series(model.predict(x_ans.values))\n",
    "        \n",
    "        x_ans_stack['y_'+name] += (preds1/splits)\n",
    "        avg_score[n]=avg_score[n]+acc\n",
    "    \n",
    "    val_mean = np.mean(x_stack.values[test_index,:], axis=1)\n",
    "    avg_score_all += score1(val_y,val_mean)/splits\n",
    "    print('Iteration:',k,' (All models)   rmse:',score1(val_y,val_mean))\n",
    "    k=k+1\n",
    "print(' Avg Score:',avg_score/splits,\"  All models:\",avg_score_all)\n",
    "# print('\\n Best score:',score,' Avg Score:',avg_score/splits)\n",
    "# preds=preds/splits\n",
    "# x_ans_stack = x_ans.copy()\n",
    "# x_ans_stack['y_lgb'] = 0\n",
    "# x_ans_stack['y_lgb'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = np.mean(x_ans_stack.values[:,(0,2)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EBM_IT_01\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\EBM_IT_01\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_lgb</th>\n",
       "      <th>y_xbg</th>\n",
       "      <th>y_rnf</th>\n",
       "      <th>y_ext</th>\n",
       "      <th>y_ada</th>\n",
       "      <th>y_elas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.011861</td>\n",
       "      <td>5.960464e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006006</td>\n",
       "      <td>0.571715</td>\n",
       "      <td>0.458841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.134738</td>\n",
       "      <td>6.231666e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.198501</td>\n",
       "      <td>1.755899</td>\n",
       "      <td>0.831707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.044002</td>\n",
       "      <td>5.960464e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006215</td>\n",
       "      <td>0.225099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.011846</td>\n",
       "      <td>1.177400e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004310</td>\n",
       "      <td>-0.115079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.012426</td>\n",
       "      <td>3.400141e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600171</td>\n",
       "      <td>0.003501</td>\n",
       "      <td>0.237957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.017200</td>\n",
       "      <td>1.177400e-03</td>\n",
       "      <td>0.037582</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499057</td>\n",
       "      <td>0.393055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.012305</td>\n",
       "      <td>6.231666e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033419</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>-0.144365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.012426</td>\n",
       "      <td>3.400141e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009112</td>\n",
       "      <td>0.003501</td>\n",
       "      <td>-0.055648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.012159</td>\n",
       "      <td>5.960464e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005345</td>\n",
       "      <td>-0.055564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.012159</td>\n",
       "      <td>5.960464e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005345</td>\n",
       "      <td>-0.093098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.008369</td>\n",
       "      <td>6.231666e-05</td>\n",
       "      <td>1.487930</td>\n",
       "      <td>0.574613</td>\n",
       "      <td>9.065326</td>\n",
       "      <td>4.715489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.113861</td>\n",
       "      <td>6.231666e-05</td>\n",
       "      <td>1.120381</td>\n",
       "      <td>0.999423</td>\n",
       "      <td>6.690865</td>\n",
       "      <td>2.458644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.012305</td>\n",
       "      <td>6.231666e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013183</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>-0.074001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.012159</td>\n",
       "      <td>5.960464e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005345</td>\n",
       "      <td>-0.099237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6.042297</td>\n",
       "      <td>1.177400e-03</td>\n",
       "      <td>8.452021</td>\n",
       "      <td>7.514898</td>\n",
       "      <td>8.314544</td>\n",
       "      <td>1.254815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.012426</td>\n",
       "      <td>3.400141e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001029</td>\n",
       "      <td>0.003501</td>\n",
       "      <td>-0.125984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.013980</td>\n",
       "      <td>6.231666e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039885</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>-0.106012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.012426</td>\n",
       "      <td>3.400141e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018452</td>\n",
       "      <td>0.003501</td>\n",
       "      <td>0.056106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.012169</td>\n",
       "      <td>3.504097e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006434</td>\n",
       "      <td>-0.009107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.012305</td>\n",
       "      <td>6.231666e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013183</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>-0.156594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.012426</td>\n",
       "      <td>3.400141e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003501</td>\n",
       "      <td>-0.063920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.011846</td>\n",
       "      <td>1.177400e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004310</td>\n",
       "      <td>-0.075231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.013213</td>\n",
       "      <td>3.504097e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.325312</td>\n",
       "      <td>0.831137</td>\n",
       "      <td>1.592563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.012426</td>\n",
       "      <td>3.400141e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.003501</td>\n",
       "      <td>0.292111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.012159</td>\n",
       "      <td>5.960464e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005345</td>\n",
       "      <td>0.140004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.012305</td>\n",
       "      <td>6.231666e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021434</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>-0.014458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.012305</td>\n",
       "      <td>6.231666e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013183</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>-0.074001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.013336</td>\n",
       "      <td>3.400141e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123829</td>\n",
       "      <td>0.003501</td>\n",
       "      <td>1.056951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.012426</td>\n",
       "      <td>3.400141e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093560</td>\n",
       "      <td>0.003501</td>\n",
       "      <td>-0.036842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.011846</td>\n",
       "      <td>1.177400e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004310</td>\n",
       "      <td>-0.100617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521946</th>\n",
       "      <td>0.012169</td>\n",
       "      <td>3.504097e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006434</td>\n",
       "      <td>-0.122156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521947</th>\n",
       "      <td>0.013123</td>\n",
       "      <td>3.400141e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194106</td>\n",
       "      <td>0.003501</td>\n",
       "      <td>0.257380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521948</th>\n",
       "      <td>0.374180</td>\n",
       "      <td>5.960464e-08</td>\n",
       "      <td>0.048170</td>\n",
       "      <td>0.445041</td>\n",
       "      <td>7.518204</td>\n",
       "      <td>2.129619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521949</th>\n",
       "      <td>0.012305</td>\n",
       "      <td>6.231666e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153633</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>-0.156998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521950</th>\n",
       "      <td>0.019050</td>\n",
       "      <td>3.504097e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.831137</td>\n",
       "      <td>0.527996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521951</th>\n",
       "      <td>0.012169</td>\n",
       "      <td>3.504097e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006434</td>\n",
       "      <td>-0.009107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521952</th>\n",
       "      <td>0.012426</td>\n",
       "      <td>3.400141e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036647</td>\n",
       "      <td>0.003501</td>\n",
       "      <td>-0.012816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521953</th>\n",
       "      <td>0.217614</td>\n",
       "      <td>3.504097e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.004752</td>\n",
       "      <td>1.289629</td>\n",
       "      <td>0.877773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521954</th>\n",
       "      <td>0.012305</td>\n",
       "      <td>6.231666e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173655</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>-0.093110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521955</th>\n",
       "      <td>0.012169</td>\n",
       "      <td>3.504097e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006434</td>\n",
       "      <td>-0.191908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521956</th>\n",
       "      <td>0.011846</td>\n",
       "      <td>1.177400e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004310</td>\n",
       "      <td>0.494690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521957</th>\n",
       "      <td>0.012305</td>\n",
       "      <td>6.231666e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021434</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>-0.047997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521958</th>\n",
       "      <td>0.012305</td>\n",
       "      <td>6.231666e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072255</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>-0.107106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521959</th>\n",
       "      <td>0.012426</td>\n",
       "      <td>3.400141e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.003501</td>\n",
       "      <td>-0.097806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521960</th>\n",
       "      <td>0.012159</td>\n",
       "      <td>5.960464e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005345</td>\n",
       "      <td>-0.075579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521961</th>\n",
       "      <td>0.013980</td>\n",
       "      <td>6.231666e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039885</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>-0.112141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521962</th>\n",
       "      <td>0.011846</td>\n",
       "      <td>1.177400e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004310</td>\n",
       "      <td>-0.014207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521963</th>\n",
       "      <td>0.012305</td>\n",
       "      <td>6.231666e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157799</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>0.150768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521964</th>\n",
       "      <td>0.012169</td>\n",
       "      <td>3.504097e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006434</td>\n",
       "      <td>-0.141884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521965</th>\n",
       "      <td>0.014636</td>\n",
       "      <td>5.960464e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006215</td>\n",
       "      <td>-0.014156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521966</th>\n",
       "      <td>2.704713</td>\n",
       "      <td>3.400141e-02</td>\n",
       "      <td>1.361542</td>\n",
       "      <td>2.352500</td>\n",
       "      <td>8.706892</td>\n",
       "      <td>1.921414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521967</th>\n",
       "      <td>0.011846</td>\n",
       "      <td>1.177400e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004310</td>\n",
       "      <td>-0.041759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521968</th>\n",
       "      <td>0.012159</td>\n",
       "      <td>5.960464e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005345</td>\n",
       "      <td>-0.147039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521969</th>\n",
       "      <td>0.018080</td>\n",
       "      <td>1.177400e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.769781</td>\n",
       "      <td>0.527641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521970</th>\n",
       "      <td>0.013503</td>\n",
       "      <td>5.960464e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006215</td>\n",
       "      <td>-0.052753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521971</th>\n",
       "      <td>0.012426</td>\n",
       "      <td>3.400141e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003501</td>\n",
       "      <td>-0.116095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521972</th>\n",
       "      <td>0.012169</td>\n",
       "      <td>3.504097e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006434</td>\n",
       "      <td>-0.101764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521973</th>\n",
       "      <td>0.012169</td>\n",
       "      <td>3.504097e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006434</td>\n",
       "      <td>-0.005548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521974</th>\n",
       "      <td>0.012159</td>\n",
       "      <td>5.960464e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005345</td>\n",
       "      <td>0.054637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521975</th>\n",
       "      <td>0.012169</td>\n",
       "      <td>3.504097e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006434</td>\n",
       "      <td>-0.163152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>521976 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           y_lgb         y_xbg     y_rnf     y_ext     y_ada    y_elas\n",
       "0       0.011861  5.960464e-08  0.000000  0.006006  0.571715  0.458841\n",
       "1       0.134738  6.231666e-05  0.000000  0.198501  1.755899  0.831707\n",
       "2       0.044002  5.960464e-08  0.000000  0.000000  0.006215  0.225099\n",
       "3       0.011846  1.177400e-03  0.000000  0.000000  0.004310 -0.115079\n",
       "4       0.012426  3.400141e-02  0.000000  0.600171  0.003501  0.237957\n",
       "5       0.017200  1.177400e-03  0.037582  0.000000  0.499057  0.393055\n",
       "6       0.012305  6.231666e-05  0.000000  0.033419  0.005982 -0.144365\n",
       "7       0.012426  3.400141e-02  0.000000  0.009112  0.003501 -0.055648\n",
       "8       0.012159  5.960464e-08  0.000000  0.000000  0.005345 -0.055564\n",
       "9       0.012159  5.960464e-08  0.000000  0.000000  0.005345 -0.093098\n",
       "10      1.008369  6.231666e-05  1.487930  0.574613  9.065326  4.715489\n",
       "11      1.113861  6.231666e-05  1.120381  0.999423  6.690865  2.458644\n",
       "12      0.012305  6.231666e-05  0.000000  0.013183  0.005982 -0.074001\n",
       "13      0.012159  5.960464e-08  0.000000  0.000000  0.005345 -0.099237\n",
       "14      6.042297  1.177400e-03  8.452021  7.514898  8.314544  1.254815\n",
       "15      0.012426  3.400141e-02  0.000000  0.001029  0.003501 -0.125984\n",
       "16      0.013980  6.231666e-05  0.000000  0.039885  0.005982 -0.106012\n",
       "17      0.012426  3.400141e-02  0.000000  0.018452  0.003501  0.056106\n",
       "18      0.012169  3.504097e-03  0.000000  0.000000  0.006434 -0.009107\n",
       "19      0.012305  6.231666e-05  0.000000  0.013183  0.005982 -0.156594\n",
       "20      0.012426  3.400141e-02  0.000000  0.000000  0.003501 -0.063920\n",
       "21      0.011846  1.177400e-03  0.000000  0.000000  0.004310 -0.075231\n",
       "22      0.013213  3.504097e-03  0.000000  0.325312  0.831137  1.592563\n",
       "23      0.012426  3.400141e-02  0.000000  0.000335  0.003501  0.292111\n",
       "24      0.012159  5.960464e-08  0.000000  0.000000  0.005345  0.140004\n",
       "25      0.012305  6.231666e-05  0.000000  0.021434  0.005982 -0.014458\n",
       "26      0.012305  6.231666e-05  0.000000  0.013183  0.005982 -0.074001\n",
       "27      0.013336  3.400141e-02  0.000000  0.123829  0.003501  1.056951\n",
       "28      0.012426  3.400141e-02  0.000000  0.093560  0.003501 -0.036842\n",
       "29      0.011846  1.177400e-03  0.000000  0.000000  0.004310 -0.100617\n",
       "...          ...           ...       ...       ...       ...       ...\n",
       "521946  0.012169  3.504097e-03  0.000000  0.000000  0.006434 -0.122156\n",
       "521947  0.013123  3.400141e-02  0.000000  0.194106  0.003501  0.257380\n",
       "521948  0.374180  5.960464e-08  0.048170  0.445041  7.518204  2.129619\n",
       "521949  0.012305  6.231666e-05  0.000000  0.153633  0.005982 -0.156998\n",
       "521950  0.019050  3.504097e-03  0.000000  0.000000  0.831137  0.527996\n",
       "521951  0.012169  3.504097e-03  0.000000  0.000000  0.006434 -0.009107\n",
       "521952  0.012426  3.400141e-02  0.000000  0.036647  0.003501 -0.012816\n",
       "521953  0.217614  3.504097e-03  0.000000  1.004752  1.289629  0.877773\n",
       "521954  0.012305  6.231666e-05  0.000000  0.173655  0.005982 -0.093110\n",
       "521955  0.012169  3.504097e-03  0.000000  0.000000  0.006434 -0.191908\n",
       "521956  0.011846  1.177400e-03  0.000000  0.000000  0.004310  0.494690\n",
       "521957  0.012305  6.231666e-05  0.000000  0.021434  0.005982 -0.047997\n",
       "521958  0.012305  6.231666e-05  0.000000  0.072255  0.005982 -0.107106\n",
       "521959  0.012426  3.400141e-02  0.000000  0.000981  0.003501 -0.097806\n",
       "521960  0.012159  5.960464e-08  0.000000  0.000000  0.005345 -0.075579\n",
       "521961  0.013980  6.231666e-05  0.000000  0.039885  0.005982 -0.112141\n",
       "521962  0.011846  1.177400e-03  0.000000  0.000000  0.004310 -0.014207\n",
       "521963  0.012305  6.231666e-05  0.000000  0.157799  0.005982  0.150768\n",
       "521964  0.012169  3.504097e-03  0.000000  0.000000  0.006434 -0.141884\n",
       "521965  0.014636  5.960464e-08  0.000000  0.000000  0.006215 -0.014156\n",
       "521966  2.704713  3.400141e-02  1.361542  2.352500  8.706892  1.921414\n",
       "521967  0.011846  1.177400e-03  0.000000  0.000000  0.004310 -0.041759\n",
       "521968  0.012159  5.960464e-08  0.000000  0.000000  0.005345 -0.147039\n",
       "521969  0.018080  1.177400e-03  0.000000  0.000000  0.769781  0.527641\n",
       "521970  0.013503  5.960464e-08  0.000000  0.000000  0.006215 -0.052753\n",
       "521971  0.012426  3.400141e-02  0.000000  0.000000  0.003501 -0.116095\n",
       "521972  0.012169  3.504097e-03  0.000000  0.000000  0.006434 -0.101764\n",
       "521973  0.012169  3.504097e-03  0.000000  0.000000  0.006434 -0.005548\n",
       "521974  0.012159  5.960464e-08  0.000000  0.000000  0.005345  0.054637\n",
       "521975  0.012169  3.504097e-03  0.000000  0.000000  0.006434 -0.163152\n",
       "\n",
       "[521976 rows x 6 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130495,)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130495,)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-268-8dfa2c181f1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_mean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float64' object is not callable"
     ]
    }
   ],
   "source": [
    "score(val_y,val_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1095431 , 0.01394459, 1.03464971, ..., 0.01394459, 0.01394459,\n",
       "       0.01394459])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.47305186])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114,)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_importances_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ans_stack.to_csv(\"google_submit/x_ans_stack.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_stack.to_csv(\"google_submit/x_stack.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ans_stack = pd.read_csv('google_submit/x_ans_stack.csv', delimiter = ',')\n",
    "x_stack = pd.read_csv('google_submit/x_stack.csv', delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(521976, 6)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sum_visitNumber</th>\n",
       "      <th>Mode_geoNetwork.subContinent</th>\n",
       "      <th>Sum_totals.bounces</th>\n",
       "      <th>Sum_totals.hits</th>\n",
       "      <th>Sum_totals.newVisits</th>\n",
       "      <th>Sum_totals.pageviews</th>\n",
       "      <th>Sum_totals.visits</th>\n",
       "      <th>Mode_Month</th>\n",
       "      <th>Mode_browser2</th>\n",
       "      <th>Mode_city2</th>\n",
       "      <th>...</th>\n",
       "      <th>Mode_SurceMed.cpm</th>\n",
       "      <th>Mode_SurceMed.none</th>\n",
       "      <th>Mode_SurceMed.organic</th>\n",
       "      <th>Mode_SurceMed.referral</th>\n",
       "      <th>y_lgb</th>\n",
       "      <th>y_xbg</th>\n",
       "      <th>y_rnf</th>\n",
       "      <th>y_ext</th>\n",
       "      <th>y_ada</th>\n",
       "      <th>y_elas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.157007</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>0.782911</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060911</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>0.168563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.003155</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.221950</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013040</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>0.274146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.285474</td>\n",
       "      <td>0.125534</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>-0.216986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.618976</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.228975</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>1.235138</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119648</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>0.175382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.221950</td>\n",
       "      <td>0.079788</td>\n",
       "      <td>0.835262</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059574</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>0.072738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007822</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.221950</td>\n",
       "      <td>0.079788</td>\n",
       "      <td>0.161653</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.119367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.157007</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>0.161653</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.566051</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>6.252671</td>\n",
       "      <td>3.503503</td>\n",
       "      <td>8.861120</td>\n",
       "      <td>2.794468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618976</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.178509</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>0.161653</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043564</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.110760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.006740</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.219982</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>0.161653</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013639</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007977</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.165359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.219982</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>0.012260</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>0.151714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.304307</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>1.235138</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081306</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>0.476021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.006740</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.236857</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>0.161653</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002637</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.074981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.285474</td>\n",
       "      <td>0.125534</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>-0.216986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007687</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.178509</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004524</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.058843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007822</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.178509</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>0.053311</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004172</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>0.015059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008623</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.219982</td>\n",
       "      <td>0.016291</td>\n",
       "      <td>0.010740</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004287</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.083927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.285474</td>\n",
       "      <td>0.125534</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>-0.216986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.285474</td>\n",
       "      <td>0.125534</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>-0.216986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.157007</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>0.530707</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041164</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.080749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.003155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.236857</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>0.161653</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011421</td>\n",
       "      <td>0.078885</td>\n",
       "      <td>0.414702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010099</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.268919</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>0.161653</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027240</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.120587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.006740</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.354258</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>0.161653</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002641</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.119367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.618976</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.236857</td>\n",
       "      <td>0.080786</td>\n",
       "      <td>0.161653</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147537</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>0.020469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.178509</td>\n",
       "      <td>0.079788</td>\n",
       "      <td>0.161653</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.104782</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>4.062209</td>\n",
       "      <td>4.154516</td>\n",
       "      <td>8.046339</td>\n",
       "      <td>3.028094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007687</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.228975</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>0.161653</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002673</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>0.075246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008623</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.228975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.161653</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002641</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.101974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.618976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.228975</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>0.835262</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.425144</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>1.773881</td>\n",
       "      <td>3.564616</td>\n",
       "      <td>7.825705</td>\n",
       "      <td>2.489115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007687</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.178509</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>0.161653</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.092374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.285474</td>\n",
       "      <td>0.125534</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>-0.216986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618976</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.219982</td>\n",
       "      <td>0.079788</td>\n",
       "      <td>1.235138</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082877</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>0.218967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617212</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.003155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.268919</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>0.161653</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142480</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041851</td>\n",
       "      <td>1.698171</td>\n",
       "      <td>0.955975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617213</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.219982</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>0.161653</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.168617</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.230307</td>\n",
       "      <td>0.308850</td>\n",
       "      <td>6.045564</td>\n",
       "      <td>0.920707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617214</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618976</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.219982</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>0.737676</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049529</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>0.098659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617215</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618976</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.354258</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>0.026324</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082612</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.072109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617216</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.003155</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.178509</td>\n",
       "      <td>0.079788</td>\n",
       "      <td>0.017133</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004172</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.041012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617217</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.236857</td>\n",
       "      <td>0.079788</td>\n",
       "      <td>0.737676</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080256</td>\n",
       "      <td>0.082736</td>\n",
       "      <td>0.463700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617218</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.354258</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>0.498982</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013439</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.223531</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>0.871664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617219</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.285474</td>\n",
       "      <td>0.125534</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>-0.216986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617220</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.039113</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.219982</td>\n",
       "      <td>0.016291</td>\n",
       "      <td>0.161653</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013639</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028859</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.334707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617221</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.285474</td>\n",
       "      <td>0.125534</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>-0.216986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617222</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.285474</td>\n",
       "      <td>0.125534</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>-0.216986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617223</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007822</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.219982</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.161653</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002641</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.119367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617224</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.285474</td>\n",
       "      <td>0.125534</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>-0.216986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617225</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.221950</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>0.161653</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000554</td>\n",
       "      <td>0.038874</td>\n",
       "      <td>0.078354</td>\n",
       "      <td>0.271478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617226</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.157007</td>\n",
       "      <td>0.016291</td>\n",
       "      <td>0.161653</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.058427</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.016071</td>\n",
       "      <td>0.228340</td>\n",
       "      <td>1.838440</td>\n",
       "      <td>0.571394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617227</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.285474</td>\n",
       "      <td>0.125534</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>-0.216986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617228</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.285474</td>\n",
       "      <td>0.125534</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>-0.216986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617229</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618976</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.178509</td>\n",
       "      <td>0.079788</td>\n",
       "      <td>0.161653</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040651</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.110760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617230</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.039113</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.236857</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>0.015296</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.306887</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.789625</td>\n",
       "      <td>0.648050</td>\n",
       "      <td>2.495084</td>\n",
       "      <td>1.960297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617231</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.003155</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.157007</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>0.005716</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016018</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060871</td>\n",
       "      <td>0.496674</td>\n",
       "      <td>0.207794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617232</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.618976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.354258</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>0.835262</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.325690</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>3.392883</td>\n",
       "      <td>2.620832</td>\n",
       "      <td>8.781960</td>\n",
       "      <td>2.137790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617233</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618976</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.178509</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>0.835262</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044500</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>0.028026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617234</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618976</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.354258</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>0.386472</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099540</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>0.002094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617235</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.285474</td>\n",
       "      <td>0.125534</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>-0.216986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617236</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.268919</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>1.355217</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.032344</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>10.099645</td>\n",
       "      <td>8.131013</td>\n",
       "      <td>9.418634</td>\n",
       "      <td>3.968983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617237</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.285474</td>\n",
       "      <td>0.125534</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>-0.216986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617238</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007687</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.219982</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>0.161653</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002637</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.092374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617239</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>0.248172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.285474</td>\n",
       "      <td>0.125534</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>-0.216986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617240</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.268919</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>1.355217</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068703</td>\n",
       "      <td>0.082736</td>\n",
       "      <td>0.544385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617241</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.178509</td>\n",
       "      <td>0.079788</td>\n",
       "      <td>0.161653</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002637</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.058783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>617242 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sum_visitNumber  Mode_geoNetwork.subContinent  Sum_totals.bounces  \\\n",
       "0                   1.0                      0.618976                 0.0   \n",
       "1                   6.0                      0.003155                 3.0   \n",
       "2                   0.0                      0.248172                 0.0   \n",
       "3                   3.0                      0.618976                 1.0   \n",
       "4                   1.0                      0.618976                 0.0   \n",
       "5                   1.0                      0.007822                 1.0   \n",
       "6                   1.0                      0.618976                 0.0   \n",
       "7                   1.0                      0.618976                 1.0   \n",
       "8                   1.0                      0.006740                 0.0   \n",
       "9                   1.0                      0.007822                 0.0   \n",
       "10                  1.0                      0.618976                 0.0   \n",
       "11                  1.0                      0.006740                 1.0   \n",
       "12                  0.0                      0.248172                 0.0   \n",
       "13                  1.0                      0.007687                 1.0   \n",
       "14                  1.0                      0.007822                 1.0   \n",
       "15                  1.0                      0.008623                 1.0   \n",
       "16                  0.0                      0.248172                 0.0   \n",
       "17                  0.0                      0.248172                 0.0   \n",
       "18                  1.0                      0.618976                 0.0   \n",
       "19                  1.0                      0.003155                 0.0   \n",
       "20                  1.0                      0.010099                 0.0   \n",
       "21                  1.0                      0.006740                 1.0   \n",
       "22                  3.0                      0.618976                 1.0   \n",
       "23                  1.0                      0.618976                 0.0   \n",
       "24                  1.0                      0.007687                 0.0   \n",
       "25                  1.0                      0.008623                 1.0   \n",
       "26                  6.0                      0.618976                 0.0   \n",
       "27                  1.0                      0.007687                 1.0   \n",
       "28                  0.0                      0.248172                 0.0   \n",
       "29                  1.0                      0.618976                 1.0   \n",
       "...                 ...                           ...                 ...   \n",
       "617212              1.0                      0.003155                 0.0   \n",
       "617213              1.0                      0.618976                 0.0   \n",
       "617214              1.0                      0.618976                 1.0   \n",
       "617215              1.0                      0.618976                 1.0   \n",
       "617216              1.0                      0.003155                 1.0   \n",
       "617217              1.0                      0.618976                 0.0   \n",
       "617218              1.0                      0.618976                 0.0   \n",
       "617219              0.0                      0.248172                 0.0   \n",
       "617220              1.0                      0.039113                 0.0   \n",
       "617221              0.0                      0.248172                 0.0   \n",
       "617222              0.0                      0.248172                 0.0   \n",
       "617223              1.0                      0.007822                 1.0   \n",
       "617224              0.0                      0.248172                 0.0   \n",
       "617225              1.0                      0.618976                 0.0   \n",
       "617226              1.0                      0.618976                 0.0   \n",
       "617227              0.0                      0.248172                 0.0   \n",
       "617228              0.0                      0.248172                 0.0   \n",
       "617229              1.0                      0.618976                 1.0   \n",
       "617230              1.0                      0.039113                 0.0   \n",
       "617231             20.0                      0.003155                 2.0   \n",
       "617232              3.0                      0.618976                 0.0   \n",
       "617233              1.0                      0.618976                 1.0   \n",
       "617234              1.0                      0.618976                 1.0   \n",
       "617235              0.0                      0.248172                 0.0   \n",
       "617236              1.0                      0.618976                 0.0   \n",
       "617237              0.0                      0.248172                 0.0   \n",
       "617238              1.0                      0.007687                 1.0   \n",
       "617239              0.0                      0.248172                 0.0   \n",
       "617240              1.0                      0.618976                 0.0   \n",
       "617241              1.0                      0.007822                 0.0   \n",
       "\n",
       "        Sum_totals.hits  Sum_totals.newVisits  Sum_totals.pageviews  \\\n",
       "0                   3.0                   1.0                   3.0   \n",
       "1                   3.0                   1.0                   3.0   \n",
       "2                   0.0                   0.0                   0.0   \n",
       "3                   1.0                   0.0                   1.0   \n",
       "4                   2.0                   1.0                   2.0   \n",
       "5                   1.0                   1.0                   1.0   \n",
       "6                  44.0                   1.0                  33.0   \n",
       "7                   1.0                   1.0                   1.0   \n",
       "8                   3.0                   1.0                   2.0   \n",
       "9                   5.0                   1.0                   5.0   \n",
       "10                  5.0                   1.0                   5.0   \n",
       "11                  1.0                   1.0                   1.0   \n",
       "12                  0.0                   0.0                   0.0   \n",
       "13                  1.0                   1.0                   1.0   \n",
       "14                  1.0                   1.0                   1.0   \n",
       "15                  1.0                   1.0                   1.0   \n",
       "16                  0.0                   0.0                   0.0   \n",
       "17                  0.0                   0.0                   0.0   \n",
       "18                  2.0                   1.0                   2.0   \n",
       "19                  7.0                   1.0                   7.0   \n",
       "20                  2.0                   1.0                   2.0   \n",
       "21                  1.0                   1.0                   1.0   \n",
       "22                  3.0                   1.0                   3.0   \n",
       "23                 49.0                   1.0                  36.0   \n",
       "24                  4.0                   1.0                   4.0   \n",
       "25                  1.0                   1.0                   1.0   \n",
       "26                 52.0                   1.0                  34.0   \n",
       "27                  1.0                   1.0                   1.0   \n",
       "28                  0.0                   0.0                   0.0   \n",
       "29                  1.0                   1.0                   1.0   \n",
       "...                 ...                   ...                   ...   \n",
       "617212             15.0                   1.0                  13.0   \n",
       "617213             14.0                   1.0                  12.0   \n",
       "617214              1.0                   1.0                   1.0   \n",
       "617215              1.0                   1.0                   1.0   \n",
       "617216              1.0                   1.0                   1.0   \n",
       "617217              6.0                   1.0                   6.0   \n",
       "617218              3.0                   1.0                   3.0   \n",
       "617219              0.0                   0.0                   0.0   \n",
       "617220              3.0                   1.0                   1.0   \n",
       "617221              0.0                   0.0                   0.0   \n",
       "617222              0.0                   0.0                   0.0   \n",
       "617223              1.0                   1.0                   1.0   \n",
       "617224              0.0                   0.0                   0.0   \n",
       "617225              6.0                   1.0                   6.0   \n",
       "617226             13.0                   1.0                  10.0   \n",
       "617227              0.0                   0.0                   0.0   \n",
       "617228              0.0                   0.0                   0.0   \n",
       "617229              1.0                   1.0                   1.0   \n",
       "617230             48.0                   1.0                  29.0   \n",
       "617231              8.0                   0.0                   8.0   \n",
       "617232             21.0                   1.0                  17.0   \n",
       "617233              1.0                   1.0                   1.0   \n",
       "617234              1.0                   1.0                   1.0   \n",
       "617235              0.0                   0.0                   0.0   \n",
       "617236             61.0                   1.0                  39.0   \n",
       "617237              0.0                   0.0                   0.0   \n",
       "617238              1.0                   1.0                   1.0   \n",
       "617239              0.0                   0.0                   0.0   \n",
       "617240              6.0                   1.0                   6.0   \n",
       "617241              3.0                   1.0                   3.0   \n",
       "\n",
       "        Sum_totals.visits  Mode_Month  Mode_browser2  Mode_city2    ...     \\\n",
       "0                     1.0    0.157007       0.338363    0.782911    ...      \n",
       "1                     3.0    0.221950       0.338363    0.001474    ...      \n",
       "2                     0.0    0.248172       0.248172    0.248172    ...      \n",
       "3                     1.0    0.228975       0.338363    1.235138    ...      \n",
       "4                     1.0    0.221950       0.079788    0.835262    ...      \n",
       "5                     1.0    0.221950       0.079788    0.161653    ...      \n",
       "6                     1.0    0.157007       0.338363    0.161653    ...      \n",
       "7                     1.0    0.178509       0.338363    0.161653    ...      \n",
       "8                     1.0    0.219982       0.338363    0.161653    ...      \n",
       "9                     1.0    0.219982       0.338363    0.012260    ...      \n",
       "10                    1.0    0.304307       0.338363    1.235138    ...      \n",
       "11                    1.0    0.236857       0.338363    0.161653    ...      \n",
       "12                    0.0    0.248172       0.248172    0.248172    ...      \n",
       "13                    1.0    0.178509       0.338363    0.001474    ...      \n",
       "14                    1.0    0.178509       0.338363    0.053311    ...      \n",
       "15                    1.0    0.219982       0.016291    0.010740    ...      \n",
       "16                    0.0    0.248172       0.248172    0.248172    ...      \n",
       "17                    0.0    0.248172       0.248172    0.248172    ...      \n",
       "18                    1.0    0.157007       0.338363    0.530707    ...      \n",
       "19                    1.0    0.236857       0.338363    0.161653    ...      \n",
       "20                    1.0    0.268919       0.338363    0.161653    ...      \n",
       "21                    1.0    0.354258       0.338363    0.161653    ...      \n",
       "22                    2.0    0.236857       0.080786    0.161653    ...      \n",
       "23                    1.0    0.178509       0.079788    0.161653    ...      \n",
       "24                    1.0    0.228975       0.338363    0.161653    ...      \n",
       "25                    1.0    0.228975       0.000000    0.161653    ...      \n",
       "26                    3.0    0.228975       0.338363    0.835262    ...      \n",
       "27                    1.0    0.178509       0.338363    0.161653    ...      \n",
       "28                    0.0    0.248172       0.248172    0.248172    ...      \n",
       "29                    1.0    0.219982       0.079788    1.235138    ...      \n",
       "...                   ...         ...            ...         ...    ...      \n",
       "617212                1.0    0.268919       0.338363    0.161653    ...      \n",
       "617213                1.0    0.219982       0.338363    0.161653    ...      \n",
       "617214                1.0    0.219982       0.338363    0.737676    ...      \n",
       "617215                1.0    0.354258       0.338363    0.026324    ...      \n",
       "617216                1.0    0.178509       0.079788    0.017133    ...      \n",
       "617217                1.0    0.236857       0.079788    0.737676    ...      \n",
       "617218                1.0    0.354258       0.338363    0.498982    ...      \n",
       "617219                0.0    0.248172       0.248172    0.248172    ...      \n",
       "617220                1.0    0.219982       0.016291    0.161653    ...      \n",
       "617221                0.0    0.248172       0.248172    0.248172    ...      \n",
       "617222                0.0    0.248172       0.248172    0.248172    ...      \n",
       "617223                1.0    0.219982       0.000000    0.161653    ...      \n",
       "617224                0.0    0.248172       0.248172    0.248172    ...      \n",
       "617225                1.0    0.221950       0.338363    0.161653    ...      \n",
       "617226                1.0    0.157007       0.016291    0.161653    ...      \n",
       "617227                0.0    0.248172       0.248172    0.248172    ...      \n",
       "617228                0.0    0.248172       0.248172    0.248172    ...      \n",
       "617229                1.0    0.178509       0.079788    0.161653    ...      \n",
       "617230                1.0    0.236857       0.338363    0.015296    ...      \n",
       "617231                5.0    0.157007       0.338363    0.005716    ...      \n",
       "617232                2.0    0.354258       0.338363    0.835262    ...      \n",
       "617233                1.0    0.178509       0.338363    0.835262    ...      \n",
       "617234                1.0    0.354258       0.338363    0.386472    ...      \n",
       "617235                0.0    0.248172       0.248172    0.248172    ...      \n",
       "617236                1.0    0.268919       0.338363    1.355217    ...      \n",
       "617237                0.0    0.248172       0.248172    0.248172    ...      \n",
       "617238                1.0    0.219982       0.338363    0.161653    ...      \n",
       "617239                0.0    0.248172       0.248172    0.248172    ...      \n",
       "617240                1.0    0.268919       0.338363    1.355217    ...      \n",
       "617241                1.0    0.178509       0.079788    0.161653    ...      \n",
       "\n",
       "        Mode_SurceMed.cpm  Mode_SurceMed.none  Mode_SurceMed.organic  \\\n",
       "0                       0                   0                      1   \n",
       "1                       0                   0                      0   \n",
       "2                       0                   1                      0   \n",
       "3                       0                   0                      0   \n",
       "4                       0                   0                      1   \n",
       "5                       0                   0                      1   \n",
       "6                       0                   0                      0   \n",
       "7                       0                   0                      0   \n",
       "8                       0                   0                      0   \n",
       "9                       0                   0                      1   \n",
       "10                      0                   0                      1   \n",
       "11                      0                   0                      0   \n",
       "12                      0                   1                      0   \n",
       "13                      0                   0                      0   \n",
       "14                      0                   0                      0   \n",
       "15                      0                   0                      0   \n",
       "16                      0                   1                      0   \n",
       "17                      0                   1                      0   \n",
       "18                      0                   0                      1   \n",
       "19                      0                   0                      0   \n",
       "20                      0                   0                      0   \n",
       "21                      0                   0                      1   \n",
       "22                      0                   0                      0   \n",
       "23                      0                   0                      1   \n",
       "24                      0                   0                      1   \n",
       "25                      0                   0                      0   \n",
       "26                      0                   0                      1   \n",
       "27                      0                   0                      1   \n",
       "28                      0                   1                      0   \n",
       "29                      0                   0                      0   \n",
       "...                   ...                 ...                    ...   \n",
       "617212                  0                   0                      1   \n",
       "617213                  0                   0                      1   \n",
       "617214                  0                   0                      1   \n",
       "617215                  0                   0                      0   \n",
       "617216                  0                   0                      0   \n",
       "617217                  0                   0                      0   \n",
       "617218                  0                   0                      0   \n",
       "617219                  0                   1                      0   \n",
       "617220                  0                   0                      0   \n",
       "617221                  0                   1                      0   \n",
       "617222                  0                   1                      0   \n",
       "617223                  0                   0                      1   \n",
       "617224                  0                   1                      0   \n",
       "617225                  0                   0                      1   \n",
       "617226                  0                   0                      1   \n",
       "617227                  0                   1                      0   \n",
       "617228                  0                   1                      0   \n",
       "617229                  0                   0                      0   \n",
       "617230                  0                   0                      0   \n",
       "617231                  0                   0                      0   \n",
       "617232                  0                   0                      0   \n",
       "617233                  0                   0                      0   \n",
       "617234                  0                   0                      0   \n",
       "617235                  0                   1                      0   \n",
       "617236                  0                   0                      0   \n",
       "617237                  0                   1                      0   \n",
       "617238                  0                   0                      1   \n",
       "617239                  0                   1                      0   \n",
       "617240                  0                   0                      1   \n",
       "617241                  0                   0                      1   \n",
       "\n",
       "        Mode_SurceMed.referral      y_lgb     y_xbg      y_rnf     y_ext  \\\n",
       "0                            0   0.012181  0.007749   0.000000  0.060911   \n",
       "1                            0   0.012400  0.007749   0.000000  0.013040   \n",
       "2                            0   0.064950  0.007749   0.285474  0.125534   \n",
       "3                            0   0.012181  0.007749   0.000000  0.119648   \n",
       "4                            0   0.012181  0.007749   0.000000  0.059574   \n",
       "5                            0   0.012181  0.007749   0.000000  0.003470   \n",
       "6                            0   4.566051  0.007749   6.252671  3.503503   \n",
       "7                            0   0.012181  0.007749   0.000000  0.043564   \n",
       "8                            1   0.013639  0.007749   0.000000  0.007977   \n",
       "9                            0   0.012181  0.007749   0.000000  0.002705   \n",
       "10                           0   0.012181  0.007749   0.000000  0.081306   \n",
       "11                           1   0.012181  0.007749   0.000000  0.002637   \n",
       "12                           0   0.064950  0.007749   0.285474  0.125534   \n",
       "13                           0   0.012181  0.007749   0.000000  0.004524   \n",
       "14                           1   0.012181  0.007749   0.000000  0.004172   \n",
       "15                           0   0.012181  0.007749   0.000000  0.004287   \n",
       "16                           0   0.064950  0.007749   0.285474  0.125534   \n",
       "17                           0   0.064950  0.007749   0.285474  0.125534   \n",
       "18                           0   0.012181  0.007749   0.000000  0.041164   \n",
       "19                           1   0.012181  0.007749   0.000000  0.011421   \n",
       "20                           1   0.012181  0.007749   0.000000  0.027240   \n",
       "21                           0   0.012181  0.007749   0.000000  0.002641   \n",
       "22                           0   0.012181  0.007749   0.000000  0.147537   \n",
       "23                           0   4.104782  0.007749   4.062209  4.154516   \n",
       "24                           0   0.012181  0.007749   0.000000  0.002673   \n",
       "25                           1   0.012181  0.007749   0.000000  0.002641   \n",
       "26                           0   2.425144  0.007749   1.773881  3.564616   \n",
       "27                           0   0.012181  0.007749   0.000000  0.002669   \n",
       "28                           0   0.064950  0.007749   0.285474  0.125534   \n",
       "29                           1   0.012181  0.007749   0.000000  0.082877   \n",
       "...                        ...        ...       ...        ...       ...   \n",
       "617212                       0   0.142480  0.007749   0.000000  0.041851   \n",
       "617213                       0   0.168617  0.007749   0.230307  0.308850   \n",
       "617214                       0   0.012181  0.007749   0.000000  0.049529   \n",
       "617215                       0   0.012181  0.007749   0.000000  0.082612   \n",
       "617216                       1   0.012181  0.007749   0.000000  0.004172   \n",
       "617217                       1   0.012181  0.007749   0.000000  0.080256   \n",
       "617218                       1   0.013439  0.007749   0.000052  0.223531   \n",
       "617219                       0   0.064950  0.007749   0.285474  0.125534   \n",
       "617220                       1   0.013639  0.007749   0.000000  0.028859   \n",
       "617221                       0   0.064950  0.007749   0.285474  0.125534   \n",
       "617222                       0   0.064950  0.007749   0.285474  0.125534   \n",
       "617223                       0   0.012181  0.007749   0.000000  0.002641   \n",
       "617224                       0   0.064950  0.007749   0.285474  0.125534   \n",
       "617225                       0   0.012181  0.007749   0.000554  0.038874   \n",
       "617226                       0   0.058427  0.007749   0.016071  0.228340   \n",
       "617227                       0   0.064950  0.007749   0.285474  0.125534   \n",
       "617228                       0   0.064950  0.007749   0.285474  0.125534   \n",
       "617229                       0   0.012181  0.007749   0.000000  0.040651   \n",
       "617230                       0   0.306887  0.007749   0.789625  0.648050   \n",
       "617231                       1   0.016018  0.007749   0.000000  0.060871   \n",
       "617232                       1   3.325690  0.007749   3.392883  2.620832   \n",
       "617233                       0   0.012181  0.007749   0.000000  0.044500   \n",
       "617234                       0   0.012181  0.007749   0.000000  0.099540   \n",
       "617235                       0   0.064950  0.007749   0.285474  0.125534   \n",
       "617236                       1  10.032344  0.007749  10.099645  8.131013   \n",
       "617237                       0   0.064950  0.007749   0.285474  0.125534   \n",
       "617238                       0   0.012181  0.007749   0.000000  0.002637   \n",
       "617239                       0   0.064950  0.007749   0.285474  0.125534   \n",
       "617240                       0   0.012181  0.007749   0.000000  0.068703   \n",
       "617241                       0   0.012181  0.007749   0.000000  0.002637   \n",
       "\n",
       "           y_ada    y_elas  \n",
       "0       0.005114  0.168563  \n",
       "1       0.005114  0.274146  \n",
       "2       0.005288 -0.216986  \n",
       "3       0.005114  0.175382  \n",
       "4       0.005114  0.072738  \n",
       "5       0.005114 -0.119367  \n",
       "6       8.861120  2.794468  \n",
       "7       0.005114 -0.110760  \n",
       "8       0.005114 -0.165359  \n",
       "9       0.005288  0.151714  \n",
       "10      0.005288  0.476021  \n",
       "11      0.005114 -0.074981  \n",
       "12      0.005288 -0.216986  \n",
       "13      0.005114 -0.058843  \n",
       "14      0.005114  0.015059  \n",
       "15      0.005114 -0.083927  \n",
       "16      0.005288 -0.216986  \n",
       "17      0.005288 -0.216986  \n",
       "18      0.005114 -0.080749  \n",
       "19      0.078885  0.414702  \n",
       "20      0.005114 -0.120587  \n",
       "21      0.005114 -0.119367  \n",
       "22      0.005114  0.020469  \n",
       "23      8.046339  3.028094  \n",
       "24      0.005288  0.075246  \n",
       "25      0.005114 -0.101974  \n",
       "26      7.825705  2.489115  \n",
       "27      0.005114 -0.092374  \n",
       "28      0.005288 -0.216986  \n",
       "29      0.005114  0.218967  \n",
       "...          ...       ...  \n",
       "617212  1.698171  0.955975  \n",
       "617213  6.045564  0.920707  \n",
       "617214  0.005114  0.098659  \n",
       "617215  0.005114 -0.072109  \n",
       "617216  0.005114 -0.041012  \n",
       "617217  0.082736  0.463700  \n",
       "617218  0.005114  0.871664  \n",
       "617219  0.005288 -0.216986  \n",
       "617220  0.005114 -0.334707  \n",
       "617221  0.005288 -0.216986  \n",
       "617222  0.005288 -0.216986  \n",
       "617223  0.005114 -0.119367  \n",
       "617224  0.005288 -0.216986  \n",
       "617225  0.078354  0.271478  \n",
       "617226  1.838440  0.571394  \n",
       "617227  0.005288 -0.216986  \n",
       "617228  0.005288 -0.216986  \n",
       "617229  0.005114 -0.110760  \n",
       "617230  2.495084  1.960297  \n",
       "617231  0.496674  0.207794  \n",
       "617232  8.781960  2.137790  \n",
       "617233  0.005114  0.028026  \n",
       "617234  0.005114  0.002094  \n",
       "617235  0.005288 -0.216986  \n",
       "617236  9.418634  3.968983  \n",
       "617237  0.005288 -0.216986  \n",
       "617238  0.005114 -0.092374  \n",
       "617239  0.005288 -0.216986  \n",
       "617240  0.082736  0.544385  \n",
       "617241  0.005114 -0.058783  \n",
       "\n",
       "[617242 rows x 75 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  pd.concat([x_ans, x_ans_stack], axis=1, sort=False)\n",
    "x_ans.reset_index(drop=True).join(x_ans_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_lgb</th>\n",
       "      <th>y_xbg</th>\n",
       "      <th>y_rnf</th>\n",
       "      <th>y_ext</th>\n",
       "      <th>y_ada</th>\n",
       "      <th>y_elas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060911</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>0.168563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.012400</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013040</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>0.274146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.285474</td>\n",
       "      <td>0.125534</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>-0.216986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119648</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>0.175382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059574</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>0.072738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.119367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.566051</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>6.252671</td>\n",
       "      <td>3.503503</td>\n",
       "      <td>8.861120</td>\n",
       "      <td>2.794468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043564</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.110760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.013639</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007977</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.165359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>0.151714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081306</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>0.476021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002637</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.074981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.285474</td>\n",
       "      <td>0.125534</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>-0.216986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004524</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.058843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004172</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>0.015059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004287</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.083927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.285474</td>\n",
       "      <td>0.125534</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>-0.216986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.285474</td>\n",
       "      <td>0.125534</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>-0.216986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041164</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.080749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011421</td>\n",
       "      <td>0.078885</td>\n",
       "      <td>0.414702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027240</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.120587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002641</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.119367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147537</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>0.020469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.104782</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>4.062209</td>\n",
       "      <td>4.154516</td>\n",
       "      <td>8.046339</td>\n",
       "      <td>3.028094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002673</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>0.075246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002641</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.101974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.425144</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>1.773881</td>\n",
       "      <td>3.564616</td>\n",
       "      <td>7.825705</td>\n",
       "      <td>2.489115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.092374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.285474</td>\n",
       "      <td>0.125534</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>-0.216986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082877</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>0.218967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617212</th>\n",
       "      <td>0.142480</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041851</td>\n",
       "      <td>1.698171</td>\n",
       "      <td>0.955975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617213</th>\n",
       "      <td>0.168617</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.230307</td>\n",
       "      <td>0.308850</td>\n",
       "      <td>6.045564</td>\n",
       "      <td>0.920707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617214</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049529</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>0.098659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617215</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082612</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.072109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617216</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004172</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.041012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617217</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080256</td>\n",
       "      <td>0.082736</td>\n",
       "      <td>0.463700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617218</th>\n",
       "      <td>0.013439</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.223531</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>0.871664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617219</th>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.285474</td>\n",
       "      <td>0.125534</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>-0.216986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617220</th>\n",
       "      <td>0.013639</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028859</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.334707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617221</th>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.285474</td>\n",
       "      <td>0.125534</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>-0.216986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617222</th>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.285474</td>\n",
       "      <td>0.125534</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>-0.216986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617223</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002641</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.119367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617224</th>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.285474</td>\n",
       "      <td>0.125534</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>-0.216986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617225</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000554</td>\n",
       "      <td>0.038874</td>\n",
       "      <td>0.078354</td>\n",
       "      <td>0.271478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617226</th>\n",
       "      <td>0.058427</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.016071</td>\n",
       "      <td>0.228340</td>\n",
       "      <td>1.838440</td>\n",
       "      <td>0.571394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617227</th>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.285474</td>\n",
       "      <td>0.125534</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>-0.216986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617228</th>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.285474</td>\n",
       "      <td>0.125534</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>-0.216986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617229</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040651</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.110760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617230</th>\n",
       "      <td>0.306887</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.789625</td>\n",
       "      <td>0.648050</td>\n",
       "      <td>2.495084</td>\n",
       "      <td>1.960297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617231</th>\n",
       "      <td>0.016018</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060871</td>\n",
       "      <td>0.496674</td>\n",
       "      <td>0.207794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617232</th>\n",
       "      <td>3.325690</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>3.392883</td>\n",
       "      <td>2.620832</td>\n",
       "      <td>8.781960</td>\n",
       "      <td>2.137790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617233</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044500</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>0.028026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617234</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099540</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>0.002094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617235</th>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.285474</td>\n",
       "      <td>0.125534</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>-0.216986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617236</th>\n",
       "      <td>10.032344</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>10.099645</td>\n",
       "      <td>8.131013</td>\n",
       "      <td>9.418634</td>\n",
       "      <td>3.968983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617237</th>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.285474</td>\n",
       "      <td>0.125534</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>-0.216986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617238</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002637</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.092374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617239</th>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.285474</td>\n",
       "      <td>0.125534</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>-0.216986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617240</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068703</td>\n",
       "      <td>0.082736</td>\n",
       "      <td>0.544385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617241</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002637</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.058783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>617242 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_lgb     y_xbg      y_rnf     y_ext     y_ada    y_elas\n",
       "0        0.012181  0.007749   0.000000  0.060911  0.005114  0.168563\n",
       "1        0.012400  0.007749   0.000000  0.013040  0.005114  0.274146\n",
       "2        0.064950  0.007749   0.285474  0.125534  0.005288 -0.216986\n",
       "3        0.012181  0.007749   0.000000  0.119648  0.005114  0.175382\n",
       "4        0.012181  0.007749   0.000000  0.059574  0.005114  0.072738\n",
       "5        0.012181  0.007749   0.000000  0.003470  0.005114 -0.119367\n",
       "6        4.566051  0.007749   6.252671  3.503503  8.861120  2.794468\n",
       "7        0.012181  0.007749   0.000000  0.043564  0.005114 -0.110760\n",
       "8        0.013639  0.007749   0.000000  0.007977  0.005114 -0.165359\n",
       "9        0.012181  0.007749   0.000000  0.002705  0.005288  0.151714\n",
       "10       0.012181  0.007749   0.000000  0.081306  0.005288  0.476021\n",
       "11       0.012181  0.007749   0.000000  0.002637  0.005114 -0.074981\n",
       "12       0.064950  0.007749   0.285474  0.125534  0.005288 -0.216986\n",
       "13       0.012181  0.007749   0.000000  0.004524  0.005114 -0.058843\n",
       "14       0.012181  0.007749   0.000000  0.004172  0.005114  0.015059\n",
       "15       0.012181  0.007749   0.000000  0.004287  0.005114 -0.083927\n",
       "16       0.064950  0.007749   0.285474  0.125534  0.005288 -0.216986\n",
       "17       0.064950  0.007749   0.285474  0.125534  0.005288 -0.216986\n",
       "18       0.012181  0.007749   0.000000  0.041164  0.005114 -0.080749\n",
       "19       0.012181  0.007749   0.000000  0.011421  0.078885  0.414702\n",
       "20       0.012181  0.007749   0.000000  0.027240  0.005114 -0.120587\n",
       "21       0.012181  0.007749   0.000000  0.002641  0.005114 -0.119367\n",
       "22       0.012181  0.007749   0.000000  0.147537  0.005114  0.020469\n",
       "23       4.104782  0.007749   4.062209  4.154516  8.046339  3.028094\n",
       "24       0.012181  0.007749   0.000000  0.002673  0.005288  0.075246\n",
       "25       0.012181  0.007749   0.000000  0.002641  0.005114 -0.101974\n",
       "26       2.425144  0.007749   1.773881  3.564616  7.825705  2.489115\n",
       "27       0.012181  0.007749   0.000000  0.002669  0.005114 -0.092374\n",
       "28       0.064950  0.007749   0.285474  0.125534  0.005288 -0.216986\n",
       "29       0.012181  0.007749   0.000000  0.082877  0.005114  0.218967\n",
       "...           ...       ...        ...       ...       ...       ...\n",
       "617212   0.142480  0.007749   0.000000  0.041851  1.698171  0.955975\n",
       "617213   0.168617  0.007749   0.230307  0.308850  6.045564  0.920707\n",
       "617214   0.012181  0.007749   0.000000  0.049529  0.005114  0.098659\n",
       "617215   0.012181  0.007749   0.000000  0.082612  0.005114 -0.072109\n",
       "617216   0.012181  0.007749   0.000000  0.004172  0.005114 -0.041012\n",
       "617217   0.012181  0.007749   0.000000  0.080256  0.082736  0.463700\n",
       "617218   0.013439  0.007749   0.000052  0.223531  0.005114  0.871664\n",
       "617219   0.064950  0.007749   0.285474  0.125534  0.005288 -0.216986\n",
       "617220   0.013639  0.007749   0.000000  0.028859  0.005114 -0.334707\n",
       "617221   0.064950  0.007749   0.285474  0.125534  0.005288 -0.216986\n",
       "617222   0.064950  0.007749   0.285474  0.125534  0.005288 -0.216986\n",
       "617223   0.012181  0.007749   0.000000  0.002641  0.005114 -0.119367\n",
       "617224   0.064950  0.007749   0.285474  0.125534  0.005288 -0.216986\n",
       "617225   0.012181  0.007749   0.000554  0.038874  0.078354  0.271478\n",
       "617226   0.058427  0.007749   0.016071  0.228340  1.838440  0.571394\n",
       "617227   0.064950  0.007749   0.285474  0.125534  0.005288 -0.216986\n",
       "617228   0.064950  0.007749   0.285474  0.125534  0.005288 -0.216986\n",
       "617229   0.012181  0.007749   0.000000  0.040651  0.005114 -0.110760\n",
       "617230   0.306887  0.007749   0.789625  0.648050  2.495084  1.960297\n",
       "617231   0.016018  0.007749   0.000000  0.060871  0.496674  0.207794\n",
       "617232   3.325690  0.007749   3.392883  2.620832  8.781960  2.137790\n",
       "617233   0.012181  0.007749   0.000000  0.044500  0.005114  0.028026\n",
       "617234   0.012181  0.007749   0.000000  0.099540  0.005114  0.002094\n",
       "617235   0.064950  0.007749   0.285474  0.125534  0.005288 -0.216986\n",
       "617236  10.032344  0.007749  10.099645  8.131013  9.418634  3.968983\n",
       "617237   0.064950  0.007749   0.285474  0.125534  0.005288 -0.216986\n",
       "617238   0.012181  0.007749   0.000000  0.002637  0.005114 -0.092374\n",
       "617239   0.064950  0.007749   0.285474  0.125534  0.005288 -0.216986\n",
       "617240   0.012181  0.007749   0.000000  0.068703  0.082736  0.544385\n",
       "617241   0.012181  0.007749   0.000000  0.002637  0.005114 -0.058783\n",
       "\n",
       "[617242 rows x 6 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ans_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0.012181\n",
       "1          0.012400\n",
       "2          0.064950\n",
       "3          0.012181\n",
       "4          0.012181\n",
       "5          0.012181\n",
       "6          4.566051\n",
       "7          0.012181\n",
       "8          0.013639\n",
       "9          0.012181\n",
       "10         0.012181\n",
       "11         0.012181\n",
       "12         0.064950\n",
       "13         0.012181\n",
       "14         0.012181\n",
       "15         0.012181\n",
       "16         0.064950\n",
       "17         0.064950\n",
       "18         0.012181\n",
       "19         0.012181\n",
       "20         0.012181\n",
       "21         0.012181\n",
       "22         0.012181\n",
       "23         4.104782\n",
       "24         0.012181\n",
       "25         0.012181\n",
       "26         2.425144\n",
       "27         0.012181\n",
       "28         0.064950\n",
       "29         0.012181\n",
       "            ...    \n",
       "617212     0.142480\n",
       "617213     0.168617\n",
       "617214     0.012181\n",
       "617215     0.012181\n",
       "617216     0.012181\n",
       "617217     0.012181\n",
       "617218     0.013439\n",
       "617219     0.064950\n",
       "617220     0.013639\n",
       "617221     0.064950\n",
       "617222     0.064950\n",
       "617223     0.012181\n",
       "617224     0.064950\n",
       "617225     0.012181\n",
       "617226     0.058427\n",
       "617227     0.064950\n",
       "617228     0.064950\n",
       "617229     0.012181\n",
       "617230     0.306887\n",
       "617231     0.016018\n",
       "617232     3.325690\n",
       "617233     0.012181\n",
       "617234     0.012181\n",
       "617235     0.064950\n",
       "617236    10.032344\n",
       "617237     0.064950\n",
       "617238     0.012181\n",
       "617239     0.064950\n",
       "617240     0.012181\n",
       "617241     0.012181\n",
       "Length: 617242, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0.014950\n",
       "1         0.014950\n",
       "2         0.014950\n",
       "3         0.014950\n",
       "4         0.014950\n",
       "5         0.014950\n",
       "6         0.048240\n",
       "7         0.039105\n",
       "8         0.014950\n",
       "9         0.014950\n",
       "10        0.434285\n",
       "11        0.014950\n",
       "12        0.014950\n",
       "13        0.014950\n",
       "14        0.014950\n",
       "15        0.014950\n",
       "16        0.014950\n",
       "17        0.014950\n",
       "18        0.014950\n",
       "19        0.039105\n",
       "20        0.039105\n",
       "21        0.014950\n",
       "22        0.014950\n",
       "23        0.024085\n",
       "24        0.014950\n",
       "25        0.014950\n",
       "26        0.048240\n",
       "27        0.014950\n",
       "28        0.014950\n",
       "29        0.014950\n",
       "            ...   \n",
       "617212    0.017985\n",
       "617213    0.015936\n",
       "617214    0.014950\n",
       "617215    0.014950\n",
       "617216    0.014950\n",
       "617217    0.014950\n",
       "617218    0.014950\n",
       "617219    0.014950\n",
       "617220    0.014950\n",
       "617221    0.014950\n",
       "617222    0.014950\n",
       "617223    0.014950\n",
       "617224    0.014950\n",
       "617225    0.014950\n",
       "617226    0.015435\n",
       "617227    0.014950\n",
       "617228    0.014950\n",
       "617229    0.014950\n",
       "617230    0.017985\n",
       "617231    0.014950\n",
       "617232    0.024085\n",
       "617233    0.014950\n",
       "617234    0.014950\n",
       "617235    0.014950\n",
       "617236    0.024085\n",
       "617237    0.014950\n",
       "617238    0.014950\n",
       "617239    0.014950\n",
       "617240    0.014950\n",
       "617241    0.014950\n",
       "Length: 617242, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
